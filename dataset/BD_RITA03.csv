doc;authors;title;abstract;keywords;institution;language;status_article;pages;year;date
5683;Rodrigo Gasparoni Santos;TVX - Time and Versions in XML;The proposed work explores the application of temporal and versioning management techniques to the content of XML documents. Recently, many methods have been proposed to address this problem. These methods differ in many aspects, such as which XML objects should be temporalized, thus possessing each one a set of advantages and limitations. We present the TVX model, whose goal is to combine these techniques into a single approach its united use should supply great flexibility in the temporal treatment of XML documents. This will be accomplished through a high-level data model which can be easily implemented in XML format, along with XQuery-like query and data manipulation languages to handle the temporal XML documents. We also present a case study considering a historic query module over the text of the Brazilian Constitution, that demonstrates the employment of the TVX model in a real-life application.;;Instituto de Informática - UFRGS;pt_BR;Published;18;2008;2008-08-28 17:40:19
5683;Nina Edelweiss;TVX - Time and Versions in XML;The proposed work explores the application of temporal and versioning management techniques to the content of XML documents. Recently, many methods have been proposed to address this problem. These methods differ in many aspects, such as which XML objects should be temporalized, thus possessing each one a set of advantages and limitations. We present the TVX model, whose goal is to combine these techniques into a single approach its united use should supply great flexibility in the temporal treatment of XML documents. This will be accomplished through a high-level data model which can be easily implemented in XML format, along with XQuery-like query and data manipulation languages to handle the temporal XML documents. We also present a case study considering a historic query module over the text of the Brazilian Constitution, that demonstrates the employment of the TVX model in a real-life application.;;Instituto de Informática - UFRGS;pt_BR;Published;18;2008;2008-08-28 17:40:19
5683;Renata de Matos Galante;TVX - Time and Versions in XML;The proposed work explores the application of temporal and versioning management techniques to the content of XML documents. Recently, many methods have been proposed to address this problem. These methods differ in many aspects, such as which XML objects should be temporalized, thus possessing each one a set of advantages and limitations. We present the TVX model, whose goal is to combine these techniques into a single approach its united use should supply great flexibility in the temporal treatment of XML documents. This will be accomplished through a high-level data model which can be easily implemented in XML format, along with XQuery-like query and data manipulation languages to handle the temporal XML documents. We also present a case study considering a historic query module over the text of the Brazilian Constitution, that demonstrates the employment of the TVX model in a real-life application.;;Instituto de Informática - UFRGS;pt_BR;Published;18;2008;2008-08-28 17:40:19
5685;Dárlinton B. F. Carvalho;Uma Proposta de Framework para Desenvolvimento de Aplicações Paralelas com Mobilidade;This work proposes an object-oriented framework for the development of grid-distributed applications with mobility support. The development is based on the paradigm of multi-agent systems, using the JADE platform. Communication between applications is based on the MPI standard. The purpose of this work is to use platforms for developing agent-oriented systems without the need to program them directly. This makes it possible to facilitate the development of applications distributed in grids with support for the migration of processes between machines.;;Departamento de Informática, Pontifícia Universidade Católica do Rio de Janeiro;pt_BR;Published;13;2008;2008-08-29 11:43:20
5685;Carlos José P. de Lucena;Uma Proposta de Framework para Desenvolvimento de Aplicações Paralelas com Mobilidade;This work proposes an object-oriented framework for the development of grid-distributed applications with mobility support. The development is based on the paradigm of multi-agent systems, using the JADE platform. Communication between applications is based on the MPI standard. The purpose of this work is to use platforms for developing agent-oriented systems without the need to program them directly. This makes it possible to facilitate the development of applications distributed in grids with support for the migration of processes between machines.;;Departamento de Informática, Pontifícia Universidade Católica do Rio de Janeiro;pt_BR;Published;13;2008;2008-08-29 11:43:20
5685;Celso Carneiro Ribeiro;Uma Proposta de Framework para Desenvolvimento de Aplicações Paralelas com Mobilidade;This work proposes an object-oriented framework for the development of grid-distributed applications with mobility support. The development is based on the paradigm of multi-agent systems, using the JADE platform. Communication between applications is based on the MPI standard. The purpose of this work is to use platforms for developing agent-oriented systems without the need to program them directly. This makes it possible to facilitate the development of applications distributed in grids with support for the migration of processes between machines.;;Instituto de Computação, Universidade Federal Fluminense;pt_BR;Published;13;2008;2008-08-29 11:43:20
5690;Ana Carolina Lorena;Uma Introdução às Support Vector Machines;This paper presents an introduction to the Support Vector Machines (SVMs), a Machine Learning technique that has received increasing attention in the last years. The SVMs have been applied to several pattern recognition tasks, obtaining results superior to those of other learning techniques in various applications.;;Centro de Matemática, Computação e Cognição, Universidade Federal do ABC;pt_BR;Published;24;2008;2008-08-31 12:17:31
5690;André C. P. L. F. de Carvalho;Uma Introdução às Support Vector Machines;This paper presents an introduction to the Support Vector Machines (SVMs), a Machine Learning technique that has received increasing attention in the last years. The SVMs have been applied to several pattern recognition tasks, obtaining results superior to those of other learning techniques in various applications.;;Departamento de Ciências de Computação, Instituto de Ciências Matemáticas e de Computação, USP;pt_BR;Published;24;2008;2008-08-31 12:17:31
5691;Lucas Brusamarello;Técnicas probabilísticas para análise de yield em nível elétrico usando propagação de erros e derivadas numéricas;In nanometric technologies, variations in CMOS parameters are a challenge for designing circuits with appropriate yield. In this work we propose an efficient and accurate methodology for the statistical modeling of circuits. Error propagation and numerical techniques are applied for electrical-level modeling of random and systematic variations during the manufacturing process. The model considers covariances between parameters and spatial correlation, and has as output statistical estimators that can be used in higher-level tools, such as statistical delay analysis (SSTA) tools. Furthermore, we developed a methodology for the quantitative analysis of the contribution of each parameter to the variance of the circuit response. As case studies, we model the yield of a SRAM memory and a dynamic precharge NOR gate. In the first, we consider the impact of channel length and threshold voltage on SRAM memory cell access time. We develop a probabilistic model for the delay of a dynamic NOR with static keeper, considering variations in channel width and threshold voltage. We compared the results calculated by the proposed methodology with statistical data obtained from Monte Carlo simulations. We report a performance gain of 70×, with an error of less than 1%.;;Instituto de Informática, UFRGS;pt_BR;Published;20;2008;2008-08-31 14:42:32
5691;Roberto da Silva;Técnicas probabilísticas para análise de yield em nível elétrico usando propagação de erros e derivadas numéricas;In nanometric technologies, variations in CMOS parameters are a challenge for designing circuits with appropriate yield. In this work we propose an efficient and accurate methodology for the statistical modeling of circuits. Error propagation and numerical techniques are applied for electrical-level modeling of random and systematic variations during the manufacturing process. The model considers covariances between parameters and spatial correlation, and has as output statistical estimators that can be used in higher-level tools, such as statistical delay analysis (SSTA) tools. Furthermore, we developed a methodology for the quantitative analysis of the contribution of each parameter to the variance of the circuit response. As case studies, we model the yield of a SRAM memory and a dynamic precharge NOR gate. In the first, we consider the impact of channel length and threshold voltage on SRAM memory cell access time. We develop a probabilistic model for the delay of a dynamic NOR with static keeper, considering variations in channel width and threshold voltage. We compared the results calculated by the proposed methodology with statistical data obtained from Monte Carlo simulations. We report a performance gain of 70×, with an error of less than 1%.;;Instituto de Informática, UFRGS;pt_BR;Published;20;2008;2008-08-31 14:42:32
5691;Gilson I. Wirth;Técnicas probabilísticas para análise de yield em nível elétrico usando propagação de erros e derivadas numéricas;In nanometric technologies, variations in CMOS parameters are a challenge for designing circuits with appropriate yield. In this work we propose an efficient and accurate methodology for the statistical modeling of circuits. Error propagation and numerical techniques are applied for electrical-level modeling of random and systematic variations during the manufacturing process. The model considers covariances between parameters and spatial correlation, and has as output statistical estimators that can be used in higher-level tools, such as statistical delay analysis (SSTA) tools. Furthermore, we developed a methodology for the quantitative analysis of the contribution of each parameter to the variance of the circuit response. As case studies, we model the yield of a SRAM memory and a dynamic precharge NOR gate. In the first, we consider the impact of channel length and threshold voltage on SRAM memory cell access time. We develop a probabilistic model for the delay of a dynamic NOR with static keeper, considering variations in channel width and threshold voltage. We compared the results calculated by the proposed methodology with statistical data obtained from Monte Carlo simulations. We report a performance gain of 70×, with an error of less than 1%.;;Universidade Estadual do Rio Grande do Sul - UERGS;pt_BR;Published;20;2008;2008-08-31 14:42:32
5691;Ricardo da Luz Reis;Técnicas probabilísticas para análise de yield em nível elétrico usando propagação de erros e derivadas numéricas;In nanometric technologies, variations in CMOS parameters are a challenge for designing circuits with appropriate yield. In this work we propose an efficient and accurate methodology for the statistical modeling of circuits. Error propagation and numerical techniques are applied for electrical-level modeling of random and systematic variations during the manufacturing process. The model considers covariances between parameters and spatial correlation, and has as output statistical estimators that can be used in higher-level tools, such as statistical delay analysis (SSTA) tools. Furthermore, we developed a methodology for the quantitative analysis of the contribution of each parameter to the variance of the circuit response. As case studies, we model the yield of a SRAM memory and a dynamic precharge NOR gate. In the first, we consider the impact of channel length and threshold voltage on SRAM memory cell access time. We develop a probabilistic model for the delay of a dynamic NOR with static keeper, considering variations in channel width and threshold voltage. We compared the results calculated by the proposed methodology with statistical data obtained from Monte Carlo simulations. We report a performance gain of 70×, with an error of less than 1%.;;Instituto de Informática, UFRGS;pt_BR;Published;20;2008;2008-08-31 14:42:32
5692;Felipe Mancini;Aplicação de Redes Neurais Artificiais na Classificação de Padrões Posturais em Crianças Respiradoras Bucais e Nasais;Breathing is the first vital function developed at birth, establishing itself as the main function of the organism. Chronic mouth breathing can cause postural changes, in addition to causing less effort on the diaphragm muscle. This article aims to present results on the application of an unsupervised artificial neural network model, specifically the self-organizing map (SOM), to assist in the diagnosis and evaluation of the clinical evolution of children's posture. mouth and nasal breathers. We presented as standard input to the SOM the variables of posture and distance of the diaphragm muscle excursion of 30 mouth-breathing children and 22 nasal-breathing children. The SOM had a 95% accuracy rate in diagnosing mouth and nasal breathing children. From the resulting topology it was possible to define categorizations of the patients' posture.;;Departamento de Informática em Saúde (DIS), Universidade Federal de São Paulo (UNIFESP);pt_BR;Published;16;2008;2008-08-31 14:57:56
5692;Liu Chiao Yi;Aplicação de Redes Neurais Artificiais na Classificação de Padrões Posturais em Crianças Respiradoras Bucais e Nasais;Breathing is the first vital function developed at birth, establishing itself as the main function of the organism. Chronic mouth breathing can cause postural changes, in addition to causing less effort on the diaphragm muscle. This article aims to present results on the application of an unsupervised artificial neural network model, specifically the self-organizing map (SOM), to assist in the diagnosis and evaluation of the clinical evolution of children's posture. mouth and nasal breathers. We presented as standard input to the SOM the variables of posture and distance of the diaphragm muscle excursion of 30 mouth-breathing children and 22 nasal-breathing children. The SOM had a 95% accuracy rate in diagnosing mouth and nasal breathing children. From the resulting topology it was possible to define categorizations of the patients' posture.;;Departamento de Otorrinolaringologia Pediátrica e Cirurgia de Cabeça e Pescoço, Universidade Federal de São Paulo (UNIFESP);pt_BR;Published;16;2008;2008-08-31 14:57:56
5692;Shirley Shizue Nagata Pignatari;Aplicação de Redes Neurais Artificiais na Classificação de Padrões Posturais em Crianças Respiradoras Bucais e Nasais;Breathing is the first vital function developed at birth, establishing itself as the main function of the organism. Chronic mouth breathing can cause postural changes, in addition to causing less effort on the diaphragm muscle. This article aims to present results on the application of an unsupervised artificial neural network model, specifically the self-organizing map (SOM), to assist in the diagnosis and evaluation of the clinical evolution of children's posture. mouth and nasal breathers. We presented as standard input to the SOM the variables of posture and distance of the diaphragm muscle excursion of 30 mouth-breathing children and 22 nasal-breathing children. The SOM had a 95% accuracy rate in diagnosing mouth and nasal breathing children. From the resulting topology it was possible to define categorizations of the patients' posture.;;Departamento de Otorrinolaringologia Pediátrica e Cirurgia de Cabeça e Pescoço, Universidade Federal de São Paulo (UNIFESP);pt_BR;Published;16;2008;2008-08-31 14:57:56
5692;Antonio Carlos Roque;Aplicação de Redes Neurais Artificiais na Classificação de Padrões Posturais em Crianças Respiradoras Bucais e Nasais;Breathing is the first vital function developed at birth, establishing itself as the main function of the organism. Chronic mouth breathing can cause postural changes, in addition to causing less effort on the diaphragm muscle. This article aims to present results on the application of an unsupervised artificial neural network model, specifically the self-organizing map (SOM), to assist in the diagnosis and evaluation of the clinical evolution of children's posture. mouth and nasal breathers. We presented as standard input to the SOM the variables of posture and distance of the diaphragm muscle excursion of 30 mouth-breathing children and 22 nasal-breathing children. The SOM had a 95% accuracy rate in diagnosing mouth and nasal breathing children. From the resulting topology it was possible to define categorizations of the patients' posture.;;Departamento de Física e Matemática (DFM), Faculdade de Filosofia, Ciências e Letras de Ribeirão Preto (FFCLRP), Universidade de São Paulo (USP);pt_BR;Published;16;2008;2008-08-31 14:57:56
5692;Ivan Torres Pisa;Aplicação de Redes Neurais Artificiais na Classificação de Padrões Posturais em Crianças Respiradoras Bucais e Nasais;Breathing is the first vital function developed at birth, establishing itself as the main function of the organism. Chronic mouth breathing can cause postural changes, in addition to causing less effort on the diaphragm muscle. This article aims to present results on the application of an unsupervised artificial neural network model, specifically the self-organizing map (SOM), to assist in the diagnosis and evaluation of the clinical evolution of children's posture. mouth and nasal breathers. We presented as standard input to the SOM the variables of posture and distance of the diaphragm muscle excursion of 30 mouth-breathing children and 22 nasal-breathing children. The SOM had a 95% accuracy rate in diagnosing mouth and nasal breathing children. From the resulting topology it was possible to define categorizations of the patients' posture.;;Departamento de Informática em Saúde (DIS), Universidade Federal de São Paulo (UNIFESP);pt_BR;Published;16;2008;2008-08-31 14:57:56
5693;Roberta A. A. Fagundes;Performance Evaluation of CORBA Concurrency Control Service Using Stochastic Petri Nets;The interest in performance evaluation of middleware systems is increasing. Measurement techniques are still predominant among those used to carry out performance evaluation. However, performance models are currently being defined due to their flexibility, precision and facilities to carry out capacity planning activities. This paper presents stochastic Petri net models for performance evaluation of the CORBA Concurrency Control Service (CCS), which mediates concurrent access to objects. In order to validate the proposed models, CCS performance results obtained using those models are then compared against ones obtained through actual measurements.;;Universidade Federal de Pernambuco, Centro de Informática;pt_BR;Published;23;2008;2008-08-31 15:08:27
5693;Paulo R. M. Maciel;Performance Evaluation of CORBA Concurrency Control Service Using Stochastic Petri Nets;The interest in performance evaluation of middleware systems is increasing. Measurement techniques are still predominant among those used to carry out performance evaluation. However, performance models are currently being defined due to their flexibility, precision and facilities to carry out capacity planning activities. This paper presents stochastic Petri net models for performance evaluation of the CORBA Concurrency Control Service (CCS), which mediates concurrent access to objects. In order to validate the proposed models, CCS performance results obtained using those models are then compared against ones obtained through actual measurements.;;Universidade Federal de Pernambuco, Centro de Informática;pt_BR;Published;23;2008;2008-08-31 15:08:27
5693;Nelson S. Rosa;Performance Evaluation of CORBA Concurrency Control Service Using Stochastic Petri Nets;The interest in performance evaluation of middleware systems is increasing. Measurement techniques are still predominant among those used to carry out performance evaluation. However, performance models are currently being defined due to their flexibility, precision and facilities to carry out capacity planning activities. This paper presents stochastic Petri net models for performance evaluation of the CORBA Concurrency Control Service (CCS), which mediates concurrent access to objects. In order to validate the proposed models, CCS performance results obtained using those models are then compared against ones obtained through actual measurements.;;Universidade Federal de Pernambuco, Centro de Informática;pt_BR;Published;23;2008;2008-08-31 15:08:27
5694;Jerônimo Pellegrini;Processos de Decisão de Markov: um tutorial;There are situations in which decisions must be made in sequence, and the result of each decision is not clear to the decision maker. These situations can be mathematically formulated as Markov decision processes, and given the probabilities of the values ​​resulting from the decisions, it is possible to determine a policy that maximizes the expected value of the sequence of decisions. This tutorial describes Markov decision processes (both the fully observable and partially observable case) and briefly discusses some methods for their solution. Semi-Markov processes are not discussed.;;Instituto de Computação, Unicamp;pt_BR;Published;46;2008;2008-08-31 15:18:00
5694;Jacques Wainer;Processos de Decisão de Markov: um tutorial;There are situations in which decisions must be made in sequence, and the result of each decision is not clear to the decision maker. These situations can be mathematically formulated as Markov decision processes, and given the probabilities of the values ​​resulting from the decisions, it is possible to determine a policy that maximizes the expected value of the sequence of decisions. This tutorial describes Markov decision processes (both the fully observable and partially observable case) and briefly discusses some methods for their solution. Semi-Markov processes are not discussed.;;Instituto de Computação, Unicamp;pt_BR;Published;46;2008;2008-08-31 15:18:00
5695;João Porto de Albuquerque;Aspectos sociotécnicos da computação: contextualizando o desenvolvimento de sistemas de computação com o modelo Mikropolis;This paper discusses the need of considering sociotechnical aspectsin the scientific and professional practice of computing. In the pursuit of this goal, the paper firstly places the research on sociotechnical aspects of computing in a historical context and outlines the current research scenario in the area. Subsequently, the Mikropolis Model is explained—a model developed to provide orientation to the practical activities of the computing professional in respect to the social aspects of these activities. Furthermore, the paper discusses how to bring the model closer to the particular Brazilian context, in order to achieve analytical and didactic instruments especially suited to the reality in Brazil.;;Department Informatics, Universität Hamburg;pt_BR;Published;16;2008;2008-08-31 15:24:27
5696;Henrique Luiz Cukierman;Um Olhar Sociotécnico sobre a Engenharia de Software;New technologies modify the form and the substance of socialcontrol, participation and cohesion. However, as they modify, they are also modified by social practices in such a way it is possible to argue that social and technical dimensions constitute a process of mutual construction, only apprehended through an approach simultaneously social and technical, trough a sociotechnical frame. This article presents some of this frame’s main features, as well as its challenges to software engineering.;;PESC/COPPE/UFRJ;pt_BR;Published;20;2008;2008-08-31 15:31:37
5696;Cássio Teixeira;Um Olhar Sociotécnico sobre a Engenharia de Software;New technologies modify the form and the substance of socialcontrol, participation and cohesion. However, as they modify, they are also modified by social practices in such a way it is possible to argue that social and technical dimensions constitute a process of mutual construction, only apprehended through an approach simultaneously social and technical, trough a sociotechnical frame. This article presents some of this frame’s main features, as well as its challenges to software engineering.;;PESC/COPPE/UFRJ;pt_BR;Published;20;2008;2008-08-31 15:31:37
5696;Rafael Prikladnicki;Um Olhar Sociotécnico sobre a Engenharia de Software;New technologies modify the form and the substance of socialcontrol, participation and cohesion. However, as they modify, they are also modified by social practices in such a way it is possible to argue that social and technical dimensions constitute a process of mutual construction, only apprehended through an approach simultaneously social and technical, trough a sociotechnical frame. This article presents some of this frame’s main features, as well as its challenges to software engineering.;;PESC/COPPE/UFRJ e FACIN/PUCRS;pt_BR;Published;20;2008;2008-08-31 15:31:37
5746;Luciano Silva;Computer Vision and Graphics for Heritage Preservation and Digital Archaeology;The goal of this work is to provide attendees with a survey of topics related to Heritage Preservation and Digital Archeology, which are challenging and motivating subjects to both computer vision and graphics community. These issues have been gaining increasing attention and priority within the scientific scenario and among funding agencies and development organizations over the last years. Motivations to this work are the recent efforts in the digital preservation of cultural heritage objects and sites before degradation or damage caused by environmental factors or  human development. One of the main focuses of these researches is the development of new techniques for realistic 3D model building from images, preserving as much information as possible. We intend to introduce and discuss several emerging topics in computer vision and graphics related to the proposed theme while highlighting the major contributions and advances in these fields.;;Departamento de Informática, UFPR;pt_BR;Published;22;2008;2008-09-03 20:16:12
5746;Olga R. P. Bellon;Computer Vision and Graphics for Heritage Preservation and Digital Archaeology;The goal of this work is to provide attendees with a survey of topics related to Heritage Preservation and Digital Archeology, which are challenging and motivating subjects to both computer vision and graphics community. These issues have been gaining increasing attention and priority within the scientific scenario and among funding agencies and development organizations over the last years. Motivations to this work are the recent efforts in the digital preservation of cultural heritage objects and sites before degradation or damage caused by environmental factors or  human development. One of the main focuses of these researches is the development of new techniques for realistic 3D model building from images, preserving as much information as possible. We intend to introduce and discuss several emerging topics in computer vision and graphics related to the proposed theme while highlighting the major contributions and advances in these fields.;;Departamento de Informática, UFPR;pt_BR;Published;22;2008;2008-09-03 20:16:12
5746;Kim L. Boyer;Computer Vision and Graphics for Heritage Preservation and Digital Archaeology;The goal of this work is to provide attendees with a survey of topics related to Heritage Preservation and Digital Archeology, which are challenging and motivating subjects to both computer vision and graphics community. These issues have been gaining increasing attention and priority within the scientific scenario and among funding agencies and development organizations over the last years. Motivations to this work are the recent efforts in the digital preservation of cultural heritage objects and sites before degradation or damage caused by environmental factors or  human development. One of the main focuses of these researches is the development of new techniques for realistic 3D model building from images, preserving as much information as possible. We intend to introduce and discuss several emerging topics in computer vision and graphics related to the proposed theme while highlighting the major contributions and advances in these fields.;;2Department of Electrical and Computer Engineering, OSU;pt_BR;Published;22;2008;2008-09-03 20:16:12
5759;Luis C. Lamb;Editorial;The existence of high-quality publications is one of the indicators of the maturity of a research area. The UFRGS Institute of Informatics has, therefore, been working hard to gain a space of national recognition for the Journal of Theoretical and Applied Informatics (RITA). Our objective is to continue on the path of improvement and qualification of this publication. Currently, we have been publishing the Journal uninterruptedly for 18 years, since 1989. This is a case of success in Brazilian university publications, on this path of quality and wide dissemination of knowledge we are launching the Electronic Edition of the Journal of Theoretical and Applied Informatics , this being the first issue with paper and electronic edition.;;Instituto de Informática, UFRGS;pt_BR;Published;1;2008;2008-09-04 22:10:19
5759;José Palazzo Moreira de Oliveira;Editorial;The existence of high-quality publications is one of the indicators of the maturity of a research area. The UFRGS Institute of Informatics has, therefore, been working hard to gain a space of national recognition for the Journal of Theoretical and Applied Informatics (RITA). Our objective is to continue on the path of improvement and qualification of this publication. Currently, we have been publishing the Journal uninterruptedly for 18 years, since 1989. This is a case of success in Brazilian university publications, on this path of quality and wide dissemination of knowledge we are launching the Electronic Edition of the Journal of Theoretical and Applied Informatics , this being the first issue with paper and electronic edition.;;Instituto de Informática, UFRGS;pt_BR;Published;1;2008;2008-09-04 22:10:19
5759;Lizandro Z. Granville;Editorial;The existence of high-quality publications is one of the indicators of the maturity of a research area. The UFRGS Institute of Informatics has, therefore, been working hard to gain a space of national recognition for the Journal of Theoretical and Applied Informatics (RITA). Our objective is to continue on the path of improvement and qualification of this publication. Currently, we have been publishing the Journal uninterruptedly for 18 years, since 1989. This is a case of success in Brazilian university publications, on this path of quality and wide dissemination of knowledge we are launching the Electronic Edition of the Journal of Theoretical and Applied Informatics , this being the first issue with paper and electronic edition.;;Instituto de Informática, UFRGS;pt_BR;Published;1;2008;2008-09-04 22:10:19
5956;José Palazzo Moreira de Oliveira;Revisores da RITA no JEMS;The editors of the Journal of Theoretical and Applied Informatics would like to thank the editorial board and the following reviewers, from the period 2005-2007, for their efforts in maintaining the high standard of evaluation of the journal's articles.;;Instituto de Informática, UFRGS;pt_BR;Published;3;2008;2008-09-17 18:07:26
5961;Gladimir V. G. Baranoski;An Introduction to Light Interaction with Human Skin;Despite the notable progress in physically-based rendering, there is still a long way to go before one can automatically generate predictable images of organic materials such as human skin. In this tutorial, the main physical and biological aspects involved in the processes of propagation and absorption of light by skin tissues are examined. These processes affect not only skin appearance, but also its health. For this reason, they have also been the object of study in biomedical research. The models of light interaction with human skin developed by the biomedical community are mainly aimed at the simulation of skin spectral properties which are used to determine the concentration and distribution of various substances. In computer graphics, the focus has been on the simulation of light scattering properties that affect skin appearance. Computer models used to simulate these spectral and scattering properties are described in this tutorial, and their strengths and limitations discussed.Keywords: natural phenomena, biologically and physically-based rendering.;;Natural Phenomena Simulation Group, School of Computer Science, University of Waterloo;pt_BR;Published;29;2008;2008-09-18 18:04:15
5961;Aravind Krishnaswamy;An Introduction to Light Interaction with Human Skin;Despite the notable progress in physically-based rendering, there is still a long way to go before one can automatically generate predictable images of organic materials such as human skin. In this tutorial, the main physical and biological aspects involved in the processes of propagation and absorption of light by skin tissues are examined. These processes affect not only skin appearance, but also its health. For this reason, they have also been the object of study in biomedical research. The models of light interaction with human skin developed by the biomedical community are mainly aimed at the simulation of skin spectral properties which are used to determine the concentration and distribution of various substances. In computer graphics, the focus has been on the simulation of light scattering properties that affect skin appearance. Computer models used to simulate these spectral and scattering properties are described in this tutorial, and their strengths and limitations discussed.Keywords: natural phenomena, biologically and physically-based rendering.;;Natural Phenomena Simulation Group, School of Computer Science, University of Waterloo;pt_BR;Published;29;2008;2008-09-18 18:04:15
5962;Siome Klein Goldenstein;A Gentle Introduction to Predictive Filters;Predictive filters are essential tools in modern science. They perform state prediction and parameter estimation in fields such as robotics, computer vision, and computer graphics. Sometimes also called Bayesian filters, they apply the Bayesian rule of conditional probability to combine a predicted behavior with some corrupted indirect observation. When we study and solve a problem, we first need its proper mathematical formulation. Finding the essential parameters that best describe the system is hard modeling their behaviors over time is even more challenging. Usually, we also have an inspection mechanism that provides us with indirect measurements, the observations, of the hidden underlying parameters. We also need to deal with the concept of uncertainty, and use random variables to represent both the state and the observations. Predictive filters are a family of estimation techniques. They combine the uncertain prediction from the system’s dynamics and the corrupted observation. There are many different predictive filters, each dealing with different types of mathematical representations for random variables and system dynamics. Here, the reader will find a dense introduction to predictive filters. After a general introduction, we discuss briefly discussion about mathematical modeling of systems: state representation, dynamics, and observation. Then, we expose some basic issues related to random variables and uncertainty modeling, and discuss four implementations of predictive filters, in order of complexity: the Kalman filter, the extended Kalman filter, the particle filter, and the unscented Kalman filter.Keywords: Predictive Filters, Density Estimators, Kalman Filter, Particle Filter, Unscented Kalman Filter.;;1Instituto de Computação, UNICAMP;pt_BR;Published;29;2008;2008-09-18 23:30:33
5963;Rafael Santos;Java Advanced Imaging API: A Tutorial;This tutorial shows how the Java language and its Java Advanced Imaging (JAI) Application Program Interface (API) can be used to create applications for image representation, processing and visualization. The Java language advantages are its low cost, licensing independence and inter-platform portability. The JAI API advantages are its flexibility and variety of image processing operators. The purpose of this tutorial is to present the basic concepts of the JAI API, including several complete and verified code samples which implements simple image processing and visualization operations. At the end of the tutorial the reader should be able to implement his/her own algorithms using the Java language and the JAI API.Keywords: Image processing, Algorithms, Java, Java Advanced Imaging.;;Divisão de Sensoriamento Remoto – Instituto de Estudos Avançados – Centro Técnico Aeroespacial;pt_BR;Published;31;2008;2008-09-18 23:35:17
5964;Jörn Loviscach;Shader Programming: An Introduction Using the Effect Framework;Current commodity graphics cards offer programmability through vertex shaders and pixel shaders to create special effects by deformation, lighting, texturing, etc. The Effect framework introduced by Microsoft allows to store shader program code, settings, and a limited graphical user interface within a single .fx text file. This supports a division of labor between programmers writing the code and designers using the GUI elements to control settings. Furthermore, the Effect framework proves to be ideal for experimenting with shader programming — be it for learning purposes or for rapid prototyping. In this tutorial, we employ the Effect framework for an exploratory, hands-on approach, introducing first principles only as needed, not in advance. Simple shader programs are used to review basic 3D techniques such as homogeneous coordinates and the Phong shading model. Then we turn to basic deformation effects employing vertex shaders and the use of texture maps as decals or reflected environments inside pixel shaders. To create bump mapping and related effects, tangent space coordinates and normal maps are introduced. Finally, we treat more complex effects such as anisotropic specular highlights.Keywords: Pixel shader, Vertex shader, HLSL, Effect framework;;1Fachbereich Elektrotechnik und Informatik, Hochschule Bremen, BRemen;pt_BR;Published;39;2008;2008-09-18 23:40:07
5978;José Palazzo Moreira de Oliveira;A Revista de Informática Teórica e Aplicada e as Tecnologias de Informação;This article presents a brief overview of the Journal's history in relation to Information Technologies, describes the actions that are being taken to increase its participation in the Computer Science academic community using the possibilities of the Web, and describes the next features that will be developed . Keywords: RITA, Digital Libraries, Information Technology;;Instituto de Informática, UFRGS;pt_BR;Published;3;2008;2008-09-20 20:32:02
5978;Luis C. Lamb;A Revista de Informática Teórica e Aplicada e as Tecnologias de Informação;This article presents a brief overview of the Journal's history in relation to Information Technologies, describes the actions that are being taken to increase its participation in the Computer Science academic community using the possibilities of the Web, and describes the next features that will be developed . Keywords: RITA, Digital Libraries, Information Technology;;Instituto de Informática, UFRGS;pt_BR;Published;3;2008;2008-09-20 20:32:02
5979;André Dantas Rocha;Uso de Aspectos para Verificar Regras de Instanciação de Frameworks;Instantiation of frameworks is usually a time-consuming, error-prone process as there are specific rule types that must be followed to produce an application that meets its requirements. Some of these rules refer to framework specific implementation policies and cannot be verified in compilation time, such as nomenclature rules and default constructors. A relevant problem is that faults generated when these rules are not applied can only be detected later, during the system usage. A mechanism to enforce that certain rules for a persistency framework be strictly followed, is shown.Keyword: AOP, frameworks, instantiation rules;;Instituto de Ciências Matemáticas e de Computação – ICMC/USP;pt_BR;Published;11;2008;2008-09-20 20:44:41
5979;Valter Vieira de Camargo;Uso de Aspectos para Verificar Regras de Instanciação de Frameworks;Instantiation of frameworks is usually a time-consuming, error-prone process as there are specific rule types that must be followed to produce an application that meets its requirements. Some of these rules refer to framework specific implementation policies and cannot be verified in compilation time, such as nomenclature rules and default constructors. A relevant problem is that faults generated when these rules are not applied can only be detected later, during the system usage. A mechanism to enforce that certain rules for a persistency framework be strictly followed, is shown.Keyword: AOP, frameworks, instantiation rules;;Instituto de Ciências Matemáticas e de Computação – ICMC/USP;pt_BR;Published;11;2008;2008-09-20 20:44:41
5979;Paulo César Masiero;Uso de Aspectos para Verificar Regras de Instanciação de Frameworks;Instantiation of frameworks is usually a time-consuming, error-prone process as there are specific rule types that must be followed to produce an application that meets its requirements. Some of these rules refer to framework specific implementation policies and cannot be verified in compilation time, such as nomenclature rules and default constructors. A relevant problem is that faults generated when these rules are not applied can only be detected later, during the system usage. A mechanism to enforce that certain rules for a persistency framework be strictly followed, is shown.Keyword: AOP, frameworks, instantiation rules;;Instituto de Ciências Matemáticas e de Computação – ICMC/USP;pt_BR;Published;11;2008;2008-09-20 20:44:41
5980;Carlos A. R. Andrade;AspectH: Uma Extensão Orientada a Aspectos de Haskel;This paper presents an extension of the Haskell programming language with the objective of improving modularization of functional programs. This extension, AspectH, extends Haskell with aspect oriented concepts. AspectH implements Aspect-Oriented Programming (AOP) through pointcuts and advice, like in AspectJ, and was designed to be used in Haskell programs that use monads.Keywords: Aspect-oriented programming, functional programming, Haskell, monads;;Centro de Informática, UFPE;pt_BR;Published;11;2008;2008-09-20 20:52:03
5980;André L. M. Santos;AspectH: Uma Extensão Orientada a Aspectos de Haskel;This paper presents an extension of the Haskell programming language with the objective of improving modularization of functional programs. This extension, AspectH, extends Haskell with aspect oriented concepts. AspectH implements Aspect-Oriented Programming (AOP) through pointcuts and advice, like in AspectJ, and was designed to be used in Haskell programs that use monads.Keywords: Aspect-oriented programming, functional programming, Haskell, monads;;Centro de Informática, UFPE;pt_BR;Published;11;2008;2008-09-20 20:52:03
5980;Paulo H. M. Borba;AspectH: Uma Extensão Orientada a Aspectos de Haskel;This paper presents an extension of the Haskell programming language with the objective of improving modularization of functional programs. This extension, AspectH, extends Haskell with aspect oriented concepts. AspectH implements Aspect-Oriented Programming (AOP) through pointcuts and advice, like in AspectJ, and was designed to be used in Haskell programs that use monads.Keywords: Aspect-oriented programming, functional programming, Haskell, monads;;Centro de Informática, UFPE;pt_BR;Published;11;2008;2008-09-20 20:52:03
5981;Silvio Antonio Carro;Making Medical Visual Information Available on the WEB;This paper presents a new metadata model to describe and retrieve medical visual information, such images and their diagnoses, using the Web. The classes of this model allow describing medical images of different medical areas, including their properties, components and relationships. This model supports the international classification of diseases and related health problems (i.e. ICD-10) [1]. The MedISeek (Medical Image Seek) prototype presented here proposes a medical image sharing system based on web services, that allows authorized users to describe, store and retrieve medical images and their associated diagnostic information,based on the proposed metadata model. Thus, this paper proposes to include the image description, converted to RDF syntax, into a JPEG image and a persistent structure for relational databases to storage and retrieve this metadata, providing fast indexing and querying. A description of the prototype structure also is provided.;;Faculdade de Informática, UNOESTE, Presidente Prudente/SP, Brasil;pt_BR;Published;15;2008;2008-09-20 21:00:09
5981;Jacob Scharcanski;Making Medical Visual Information Available on the WEB;This paper presents a new metadata model to describe and retrieve medical visual information, such images and their diagnoses, using the Web. The classes of this model allow describing medical images of different medical areas, including their properties, components and relationships. This model supports the international classification of diseases and related health problems (i.e. ICD-10) [1]. The MedISeek (Medical Image Seek) prototype presented here proposes a medical image sharing system based on web services, that allows authorized users to describe, store and retrieve medical images and their associated diagnostic information,based on the proposed metadata model. Thus, this paper proposes to include the image description, converted to RDF syntax, into a JPEG image and a persistent structure for relational databases to storage and retrieve this metadata, providing fast indexing and querying. A description of the prototype structure also is provided.;;Instituto de Informática, UFRGS, Porto Alegre, RS, Brasil;pt_BR;Published;15;2008;2008-09-20 21:00:09
5982;Leandro Soares Indrusiak;Um Framework de Apoio à Colaboração no Projeto Distribuído de Sistemas Integrados;The work described in this paper aims to allow the flexibledistribution of resources and tools supporting the design of integrated systems and considers specifically the need for collaborative interaction among designers. Particular emphasis was given to issues which were only marginally considered in previous approaches, such as the abstraction of the distribution of design automation resources over the network, the consistency control on both synchronous and asynchronous interaction among designers and the support for extensible design data models.;;Instituto de Informática, UFRGS Microelectronic Systems Institute, TU Darmstadt, Karlstr. 15, 64283 Darmstadt;pt_BR;Published;25;2008;2008-09-20 21:08:07
5982;Ricardo A. L. Reis;Um Framework de Apoio à Colaboração no Projeto Distribuído de Sistemas Integrados;The work described in this paper aims to allow the flexibledistribution of resources and tools supporting the design of integrated systems and considers specifically the need for collaborative interaction among designers. Particular emphasis was given to issues which were only marginally considered in previous approaches, such as the abstraction of the distribution of design automation resources over the network, the consistency control on both synchronous and asynchronous interaction among designers and the support for extensible design data models.;;Instituto de Informática, UFRGS, Porto Alegre, RS;pt_BR;Published;25;2008;2008-09-20 21:08:07
5982;Manfred Glesner;Um Framework de Apoio à Colaboração no Projeto Distribuído de Sistemas Integrados;The work described in this paper aims to allow the flexibledistribution of resources and tools supporting the design of integrated systems and considers specifically the need for collaborative interaction among designers. Particular emphasis was given to issues which were only marginally considered in previous approaches, such as the abstraction of the distribution of design automation resources over the network, the consistency control on both synchronous and asynchronous interaction among designers and the support for extensible design data models.;;Microelectronic Systems Institute, TU Darmstadt, Karlstr. 15, 64283 Darmstadt;pt_BR;Published;25;2008;2008-09-20 21:08:07
5983;Carlos A. P. Campani;Teorias da Aleatoriedade;This work is a survey about the definition of “random sequence”. We emphasize the definition of Martin-Löf and the definition based on incompressibility (Kolmogorov complexity). Kolmogorov complexity is a profound and sofisticated theory of information and randomness based on Turing machines. These two definitions solve all the problems of the other approaches, satisfying our intuitive concept of randomness, and both are mathematically correct. Furthermore, we show the Schnorr’s approach, that includes a requisite of effectiveness (computability) in his definition. We show the relations between all definitions in a critical way.Keywords: randomness, Kolmogorov complexity, Turing machine, computability,probability.;;Instituto de Física e Matemática, UFPel, Pelotas, RS e Instituto de Informática, UFRGS;pt_BR;Published;23;2008;2008-09-20 21:15:56
5983;Paulo Blauth Menezes;Teorias da Aleatoriedade;This work is a survey about the definition of “random sequence”. We emphasize the definition of Martin-Löf and the definition based on incompressibility (Kolmogorov complexity). Kolmogorov complexity is a profound and sofisticated theory of information and randomness based on Turing machines. These two definitions solve all the problems of the other approaches, satisfying our intuitive concept of randomness, and both are mathematically correct. Furthermore, we show the Schnorr’s approach, that includes a requisite of effectiveness (computability) in his definition. We show the relations between all definitions in a critical way.Keywords: randomness, Kolmogorov complexity, Turing machine, computability,probability.;;Instituto de Informática, UFRGS, Porto Alegre, RS;pt_BR;Published;23;2008;2008-09-20 21:15:56
6011;Thomas Lewiner;3D Compression: from A to Zip a first complete example;Images invaded most of contemporary publications and communications. This expansion has accelerated with the development of efficient schemes dedicated to image compression. Nowadays, the image creation process relies on multidimensional objects generated from computer aided design, physical simulations, data representation or optimisation problem solutions. This variety of sources motivates the design of compression schemes adapted to specific class of models. The recent launch of Google Sketch’up and its 3D models warehouse has accelerated the shift from two-dimensional images to three-dimensional ones. However, these kind of systems require fast access to eventually huge models, which is possible only through the use of efficient compression schemes. This work is part of a tutorial given at the XXth Brazilian Symposium on Computer Graphics and Image Processing (Sibgrapi 2007).;;Departamento de Matemática, PUC-Rio de Janeiro;pt_BR;Published;29;2008;2008-09-24 10:19:17
6012;Bruno Rabello;Introdução ao XNA;This tutorial explore the basic characteristics for the game development plataform developed by Microsoft, called XNA (XNA´s Not Acronymed). XNA allows the creation of PC games, for Windows plataform and XBOX 360, for a console plataform. XNA aims to substitute the DirectX Manager, a version of DirectX for a .NET plataform. All the applications made in XNA are compiled in a managed code. This code is executed at the Common Language Runtime (CLR), which is the virtual machine of the .NET plataform.;;Instituto de Computação, Universidade Federal Fluminense;pt_BR;Published;43;2008;2008-09-24 10:35:17
6012;Edson Mattos;Introdução ao XNA;This tutorial explore the basic characteristics for the game development plataform developed by Microsoft, called XNA (XNA´s Not Acronymed). XNA allows the creation of PC games, for Windows plataform and XBOX 360, for a console plataform. XNA aims to substitute the DirectX Manager, a version of DirectX for a .NET plataform. All the applications made in XNA are compiled in a managed code. This code is executed at the Common Language Runtime (CLR), which is the virtual machine of the .NET plataform.;;Instituto de Computação, Universidade Federal Fluminense;pt_BR;Published;43;2008;2008-09-24 10:35:17
6012;Bruno Evangelista;Introdução ao XNA;This tutorial explore the basic characteristics for the game development plataform developed by Microsoft, called XNA (XNA´s Not Acronymed). XNA allows the creation of PC games, for Windows plataform and XBOX 360, for a console plataform. XNA aims to substitute the DirectX Manager, a version of DirectX for a .NET plataform. All the applications made in XNA are compiled in a managed code. This code is executed at the Common Language Runtime (CLR), which is the virtual machine of the .NET plataform.;;Instituto de Informática, Universidade Federal de Minas Gerais;pt_BR;Published;43;2008;2008-09-24 10:35:17
6012;Esteban Clua;Introdução ao XNA;This tutorial explore the basic characteristics for the game development plataform developed by Microsoft, called XNA (XNA´s Not Acronymed). XNA allows the creation of PC games, for Windows plataform and XBOX 360, for a console plataform. XNA aims to substitute the DirectX Manager, a version of DirectX for a .NET plataform. All the applications made in XNA are compiled in a managed code. This code is executed at the Common Language Runtime (CLR), which is the virtual machine of the .NET plataform.;;Instituto de Computação, Universidade Federal Fluminense;pt_BR;Published;43;2008;2008-09-24 10:35:17
6014;Anderson Rocha;Steganography and Steganalysis in Digital Multimedia: Hype or Hallelujah?;In this tutorial, we introduce the basic theory behind Steganography and Steganalysis, and present some recent algorithms and developments of these fields. We show how the existing techniques used nowadays are related to Image Processing and Computer Vision, point out several trendy applications of Steganography and Steganalysis, and list a few great research opportunities just waiting to be addressed.;;1Institute of Computing, University of Campinas (Unicamp);pt_BR;Published;27;2008;2008-09-24 10:41:11
6014;Siome Goldenstein;Steganography and Steganalysis in Digital Multimedia: Hype or Hallelujah?;In this tutorial, we introduce the basic theory behind Steganography and Steganalysis, and present some recent algorithms and developments of these fields. We show how the existing techniques used nowadays are related to Image Processing and Computer Vision, point out several trendy applications of Steganography and Steganalysis, and list a few great research opportunities just waiting to be addressed.;;1Institute of Computing, University of Campinas (Unicamp);pt_BR;Published;27;2008;2008-09-24 10:41:11
6015;Andre Suslik Spritzer;Navigation and Interaction in Graph Visualizations;Graphs are widely utilized in many fields and several applications require their visualization. Graph visualization is based on techniques for graph drawing, interaction and navigation in such a way that helps the user in finding and manipulating information efficiently. These techniques, which can be two or three-dimensional, depending on the spatial metaphor used to represent the graph, can be combined in many different ways in order to fit a particular application's needs. This paper presents an overview of the field of graph visualization.;;Instituto de Informática, UFRGS;pt_BR;Published;25;2008;2008-09-24 10:55:14
6015;Carla M. D. S. Freitas;Navigation and Interaction in Graph Visualizations;Graphs are widely utilized in many fields and several applications require their visualization. Graph visualization is based on techniques for graph drawing, interaction and navigation in such a way that helps the user in finding and manipulating information efficiently. These techniques, which can be two or three-dimensional, depending on the spatial metaphor used to represent the graph, can be combined in many different ways in order to fit a particular application's needs. This paper presents an overview of the field of graph visualization.;;Instituto de Informática, UFRGS;pt_BR;Published;25;2008;2008-09-24 10:55:14
6016;Gilson A. Giraldi;Dimensionality Reduction, Classification and Reconstruction Problems in Statistical Learning Approaches;Statistical learning theory explores ways of estimating functional dependency from a given collection of data. The specific sub-area of supervised statistical learning covers important models like Perceptron, Support Vector Machines (SVM) and LinearDiscriminant Analysis (LDA). In this paper we review the theory of such models and compare their separating hypersurfaces for extracting group-differences between samples. Classification and reconstruction are the main goals of this comparison. We show recent advances in this topic of research illustrating their application on face and medical image databases.;;Department of Computer Science, LNCC, Petrópolis, Rio de Janeiro;pt_BR;Published;32;2008;2008-09-24 11:04:29
6016;Paulo S. Rodrigues;Dimensionality Reduction, Classification and Reconstruction Problems in Statistical Learning Approaches;Statistical learning theory explores ways of estimating functional dependency from a given collection of data. The specific sub-area of supervised statistical learning covers important models like Perceptron, Support Vector Machines (SVM) and LinearDiscriminant Analysis (LDA). In this paper we review the theory of such models and compare their separating hypersurfaces for extracting group-differences between samples. Classification and reconstruction are the main goals of this comparison. We show recent advances in this topic of research illustrating their application on face and medical image databases.;;Department of Computer Science, FEI, São Bernardo do Campo, São Paulo;pt_BR;Published;32;2008;2008-09-24 11:04:29
6016;Edson C. Kitani;Dimensionality Reduction, Classification and Reconstruction Problems in Statistical Learning Approaches;Statistical learning theory explores ways of estimating functional dependency from a given collection of data. The specific sub-area of supervised statistical learning covers important models like Perceptron, Support Vector Machines (SVM) and LinearDiscriminant Analysis (LDA). In this paper we review the theory of such models and compare their separating hypersurfaces for extracting group-differences between samples. Classification and reconstruction are the main goals of this comparison. We show recent advances in this topic of research illustrating their application on face and medical image databases.;;3Department of Electrical Engineering, USP, São Paulo, São Paulo;pt_BR;Published;32;2008;2008-09-24 11:04:29
6016;Carlos E. Thomaz;Dimensionality Reduction, Classification and Reconstruction Problems in Statistical Learning Approaches;Statistical learning theory explores ways of estimating functional dependency from a given collection of data. The specific sub-area of supervised statistical learning covers important models like Perceptron, Support Vector Machines (SVM) and LinearDiscriminant Analysis (LDA). In this paper we review the theory of such models and compare their separating hypersurfaces for extracting group-differences between samples. Classification and reconstruction are the main goals of this comparison. We show recent advances in this topic of research illustrating their application on face and medical image databases.;;4Department of Electrical Engineering, FEI, São Bernardo do Campo, São Paulo;pt_BR;Published;32;2008;2008-09-24 11:04:29
6025;Agma Traina;Editorial convidado;This special issue of RITA contains the papers of the Tutorials presented at the Brazilian Symposium on Computer Graphics and Image Processing, SIBGRAPI 2007. It was a pleasure to receive 17 submissions, two of them from foreign countries. The majority of them have a very high standard but, because of time and space constraints, only six of them could be accepted. Herein texts associated to five of them are presented. These papers address a number of current research issues as well as conceptual information seek by the students and researchers of the field, as summarized as follows.;;ICMC-USP, São Carlos, SP;pt_BR;Published;1;2008;2008-09-24 17:10:38
6025;Marcelo Dreux;Editorial convidado;This special issue of RITA contains the papers of the Tutorials presented at the Brazilian Symposium on Computer Graphics and Image Processing, SIBGRAPI 2007. It was a pleasure to receive 17 submissions, two of them from foreign countries. The majority of them have a very high standard but, because of time and space constraints, only six of them could be accepted. Herein texts associated to five of them are presented. These papers address a number of current research issues as well as conceptual information seek by the students and researchers of the field, as summarized as follows.;;PUC-RIO, Rio de Janeiro, RJ;pt_BR;Published;1;2008;2008-09-24 17:10:38
6586;Kathia Marçal de Oliveira;Avaliação Pró-ativa da Deterioração de Sistemas de Informação por meio de Medidas de Gestão;Issues related to the decision on software modernization have received more attention and importance. Many methods have been proposed to support an organization’s choice of the best alternative for a system modernization however, these methods do not search to supply necessary information to the precocious identification of system deterioration. This article presents an approach based on measurement that aims to continuously monitor the system deterioration, allowing managers to perform adjustment actions as soon as possible, aiming to extend its useful life. Practical applications in different systems of a large company show the viability of collecting these measures and indicate the systems in deterioration.;;Universidade Catolica de Brasilia;pt_BR;Published;23;2008;2008-11-06 12:56:02
6867;Alberto Raposo;Guest Editors;The Brazilian Symposium on Virtual and Augmented Reality (SVR) celebrated its 10th edition in 2008, in the city of João Pessoa. During these last 10 years, SVR has earned its space and magnitude as a truly consolidated event within the Brazilian research community. The papers in this special issue of RITA were selectively chosen among the 24 full papers presented in SVR 2008. They are extended and revised versions of those originally presented in the symposium.The material in this special issue is representative of the wide scope of the areas covered by the symposium. The first paper, the only one not presented at the symposium, was written by the editors jointly with Luciano Soares and Veronica Teichrieb. It provides a rich panorama of current Brazilian research in Virtual Reality (VR) and related areas, based on an analysis of the 124 full papers presented over the last four SVR editions. The second paper, by Silvano Malfatti, Selan dos Santos, Luciane Fraga, Claudia Justel, Paulo Rosa, and Jauvane Oliveira, describes some interesting VR work. Its authors present an engine (EnCIMA) aimed at the quick development of VR applications. The third paper, by Fábio Miranda, Romero Tori, Cláudio Bueno, and Lucas Trias, presents research results from the Augmented Reality (AR) area. This paper, which received the best paper award in the symposium, presents a projection-based AR X-Ray tool to allow visual exploration of internal details of walls, as an illustration. The fourth paper, by Leandro Fernandes, Vitor Pamplona, João Prauchner, Luciana Nedel, and Manuel Oliveira, explores 3D user interaction. It presents the implementation of a data glove that uses a camera to track visual markers at finger tips. Finally, the last paper, by Ednaldo Pizzolato, Diego Duarte, and Marcio Fernandes, explores speech recognition as a form of interaction, introducing a software framework for application developers.	Alberto RaposoPontifical Catholic University of Rio de Janeiro - PUC-Rio - Brazil								Judith KelnerFederal University of Pernambuco – UFPE – Recife- Pernambuco – Brazil;;Pontifical Catholic University of Rio de Janeiro - PUC-Rio;pt_BR;Published;0;2008;2008-11-18 15:08:37
6867;Judith Kelner;Guest Editors;The Brazilian Symposium on Virtual and Augmented Reality (SVR) celebrated its 10th edition in 2008, in the city of João Pessoa. During these last 10 years, SVR has earned its space and magnitude as a truly consolidated event within the Brazilian research community. The papers in this special issue of RITA were selectively chosen among the 24 full papers presented in SVR 2008. They are extended and revised versions of those originally presented in the symposium.The material in this special issue is representative of the wide scope of the areas covered by the symposium. The first paper, the only one not presented at the symposium, was written by the editors jointly with Luciano Soares and Veronica Teichrieb. It provides a rich panorama of current Brazilian research in Virtual Reality (VR) and related areas, based on an analysis of the 124 full papers presented over the last four SVR editions. The second paper, by Silvano Malfatti, Selan dos Santos, Luciane Fraga, Claudia Justel, Paulo Rosa, and Jauvane Oliveira, describes some interesting VR work. Its authors present an engine (EnCIMA) aimed at the quick development of VR applications. The third paper, by Fábio Miranda, Romero Tori, Cláudio Bueno, and Lucas Trias, presents research results from the Augmented Reality (AR) area. This paper, which received the best paper award in the symposium, presents a projection-based AR X-Ray tool to allow visual exploration of internal details of walls, as an illustration. The fourth paper, by Leandro Fernandes, Vitor Pamplona, João Prauchner, Luciana Nedel, and Manuel Oliveira, explores 3D user interaction. It presents the implementation of a data glove that uses a camera to track visual markers at finger tips. Finally, the last paper, by Ednaldo Pizzolato, Diego Duarte, and Marcio Fernandes, explores speech recognition as a form of interaction, introducing a software framework for application developers.	Alberto RaposoPontifical Catholic University of Rio de Janeiro - PUC-Rio - Brazil								Judith KelnerFederal University of Pernambuco – UFPE – Recife- Pernambuco – Brazil;;Federal University of Pernambuco – UFPE – Recife;pt_BR;Published;0;2008;2008-11-18 15:08:37
6998;Luiz Eduardo Galvão Martins;FORUM: Modelo e Linguagem para Especificação  de Regras em Ambientes Colaborativos;Collaborative environments development has significantly increased in the last years, as well as the complexity of such systems and the search for better quality. On the collaborative environment development, the definition of precise rules set is important to allow correct interactions among the actors. The goal of this work is to present Forum, a model and a language to specify rules for collaborative environments, helping software designers to make rules specification to support a high level of control in such environments.;;Universidade Metodista de Piracicaba, UNIMEP, FACEN;pt_BR;Published;17;2008;2008-11-28 8:34:21
6998;Luiz Camolesi Júnior;FORUM: Modelo e Linguagem para Especificação  de Regras em Ambientes Colaborativos;Collaborative environments development has significantly increased in the last years, as well as the complexity of such systems and the search for better quality. On the collaborative environment development, the definition of precise rules set is important to allow correct interactions among the actors. The goal of this work is to present Forum, a model and a language to specify rules for collaborative environments, helping software designers to make rules specification to support a high level of control in such environments.;;Universidade Estadual de Campinas, UNICAMP, CESET;pt_BR;Published;17;2008;2008-11-28 8:34:21
6999;Denis Altieri de Oliveira Moraes;Estabilidade de Classificadores de Decisão em Árvore Binária para Dados Imagem em Alta Dimensão;This paper deals with the problem of classifying high-dimensional image data image data using a multiple stage classifier structured as a binary tree. The aim here consists in finding the optimal structure for the binary tree in the sense of achieving a stable accuracy. The advantage presented by a multiple stage classifier lies on the fact that only a sub-set of classes is considered at each stage, allowing a better selection of the features to be used at each node.  The binary tree is a particular case of a tree structured classifier, on which only two classes are considered at each node. This peculiarity makes possible the direct use of statistical distances for feature reduction (selection or extraction). In this study the criterion used for feature reduction at each node consists in optimizing the Bhattacharyya distance separating both classes in the node. The optimization of Bhattacharyya distance was based on the covariance matrices. Once the final set of features is obtained at each particular node, the classification is performed using the Gaussian Maximum Likelihood decision rule. Tests were performed using high-dimensional image data collected by the sensor system AVIRIS covering a test area. The criteria to evaluate the performance of the classifiers are: the final accuracy yielded by the classifier, its stability, and the required computer time.;;Centro Estadual de Pesquisas em Sensoriamento Remoto e Metereologia UFRGS;pt_BR;Published;15;2008;2008-12-02 8:24:34
6999;Victor Haertel;Estabilidade de Classificadores de Decisão em Árvore Binária para Dados Imagem em Alta Dimensão;This paper deals with the problem of classifying high-dimensional image data image data using a multiple stage classifier structured as a binary tree. The aim here consists in finding the optimal structure for the binary tree in the sense of achieving a stable accuracy. The advantage presented by a multiple stage classifier lies on the fact that only a sub-set of classes is considered at each stage, allowing a better selection of the features to be used at each node.  The binary tree is a particular case of a tree structured classifier, on which only two classes are considered at each node. This peculiarity makes possible the direct use of statistical distances for feature reduction (selection or extraction). In this study the criterion used for feature reduction at each node consists in optimizing the Bhattacharyya distance separating both classes in the node. The optimization of Bhattacharyya distance was based on the covariance matrices. Once the final set of features is obtained at each particular node, the classification is performed using the Gaussian Maximum Likelihood decision rule. Tests were performed using high-dimensional image data collected by the sensor system AVIRIS covering a test area. The criteria to evaluate the performance of the classifiers are: the final accuracy yielded by the classifier, its stability, and the required computer time.;;Centro Estadual de Pesquisas em Sensoriamento Remoto e Metereologia UFRGS;pt_BR;Published;15;2008;2008-12-02 8:24:34
7015;Thiago Alexandre Salgueiro Pardo;On the Development and Evaluation of a Brazilian Portuguese Discourse Parser;We present in this paper the development process and the evaluation procedure of a Brazilian Portuguese discourse parser called DiZer. Based on Rhetorical Structure Theory, DiZer is a symbolic cue phrase-based analyzer that makes use of discourse templates learned from a corpus of scientific texts to identify and build the discourse structure of texts. DiZer evaluation shows satisfactory results for scientific and news texts, even tough it was not designed for the latter, which demonstrates DiZer portability.;;Núcleo Interinstitucional de Lingüística Computacional (NILC). Instituto de Ciências Matemáticas e de Computação (ICMC), Universidade de São Paulo;pt_BR;Published;21;2008;2008-12-02 8:40:37
7015;Maria das Graças Volpe Nunes;On the Development and Evaluation of a Brazilian Portuguese Discourse Parser;We present in this paper the development process and the evaluation procedure of a Brazilian Portuguese discourse parser called DiZer. Based on Rhetorical Structure Theory, DiZer is a symbolic cue phrase-based analyzer that makes use of discourse templates learned from a corpus of scientific texts to identify and build the discourse structure of texts. DiZer evaluation shows satisfactory results for scientific and news texts, even tough it was not designed for the latter, which demonstrates DiZer portability.;;Núcleo Interinstitucional de Lingüística Computacional (NILC). Instituto de Ciências Matemáticas e de Computação (ICMC), Universidade de São Paulo;pt_BR;Published;21;2008;2008-12-02 8:40:37
7016;Ana Carolina Lorena;Estratégias para a Combinação de Classificadores Binários em Soluções Multiclasses;Several problems involve the classification of data into categories, also called classes. Given a dataset containing data whose classes are known, Machine Learning algorithms can be employed for the induction of a classifier able to predictthe class of new data from the same domain, performing the desired discrimination. Some learning techniques are originally conceived for the solution of problems with only two classes, also named binary problems. However, several problems requirethe discrimination of examples into more than two categories or classes. This paper surveys strategies for the generalization of binary classifiers to problems with more than two classes, known as multiclass problems. The focus is on strategies that decompose the original multiclass problem into multiple binary subtasks, whose outputs are combined to obtain the final classification.;;1Centro de Matemática, Computação e Cognição, Universidade Federal do ABC, Santo Andre, SP;pt_BR;Published;21;2008;2008-12-02 9:00:11
7016;André C. P. L. F. de Carvalho;Estratégias para a Combinação de Classificadores Binários em Soluções Multiclasses;Several problems involve the classification of data into categories, also called classes. Given a dataset containing data whose classes are known, Machine Learning algorithms can be employed for the induction of a classifier able to predictthe class of new data from the same domain, performing the desired discrimination. Some learning techniques are originally conceived for the solution of problems with only two classes, also named binary problems. However, several problems requirethe discrimination of examples into more than two categories or classes. This paper surveys strategies for the generalization of binary classifiers to problems with more than two classes, known as multiclass problems. The focus is on strategies that decompose the original multiclass problem into multiple binary subtasks, whose outputs are combined to obtain the final classification.;;Departamento de Ciências de Computação, Instituto de Ciências Matemáticas e de Computação, Universidade de São Paulo, SP;pt_BR;Published;21;2008;2008-12-02 9:00:11
7023;Fernanda Rispoli Quartieri;Representação e Classificação de Texturas da Íris Baseada na Transformada Ótima de Gabor;This paper proposes to investigate the application of the method developed by Manjunah and Ma [12], namely, the Optimal Gabor Wavelet Transform (OGWT), in the context of iris texture representation. The proposed method was tested in a widely known database of 1205 eye images [16]. In each one of these images, the iris region was segmented, and then represented in multiple scales using the OGWT the íris texture patterns were represented by their statistics in the Wavelet domain, and compared using similarity metrics. The experimental results indicate that the proposed method obtains a correct iris recognition rate of 94,68%, even considering out of focus images and iris occlusions a correct iris recognition rate of 100% is obtained excluding problematic images. The proposed method is flexible, and allows to fine tune the iris recognition criterion according to the accuracy level required by the application.;;Programa de Pós-Graduação em Engenharia Elétrica – PPGEE, Universidade Federal do Rio Grande do Sul –  UFRGS;pt_BR;Published;15;2008;2008-12-03 9:11:00
7023;Jacob Scharcanski;Representação e Classificação de Texturas da Íris Baseada na Transformada Ótima de Gabor;This paper proposes to investigate the application of the method developed by Manjunah and Ma [12], namely, the Optimal Gabor Wavelet Transform (OGWT), in the context of iris texture representation. The proposed method was tested in a widely known database of 1205 eye images [16]. In each one of these images, the iris region was segmented, and then represented in multiple scales using the OGWT the íris texture patterns were represented by their statistics in the Wavelet domain, and compared using similarity metrics. The experimental results indicate that the proposed method obtains a correct iris recognition rate of 94,68%, even considering out of focus images and iris occlusions a correct iris recognition rate of 100% is obtained excluding problematic images. The proposed method is flexible, and allows to fine tune the iris recognition criterion according to the accuracy level required by the application.;;Programa de Pós-Graduação em Engenharia Elétrica – PPGEE, Universidade Federal do Rio Grande do Sul –  UFRGS;pt_BR;Published;15;2008;2008-12-03 9:11:00
7023;Letícia Vieira Guimarães;Representação e Classificação de Texturas da Íris Baseada na Transformada Ótima de Gabor;This paper proposes to investigate the application of the method developed by Manjunah and Ma [12], namely, the Optimal Gabor Wavelet Transform (OGWT), in the context of iris texture representation. The proposed method was tested in a widely known database of 1205 eye images [16]. In each one of these images, the iris region was segmented, and then represented in multiple scales using the OGWT the íris texture patterns were represented by their statistics in the Wavelet domain, and compared using similarity metrics. The experimental results indicate that the proposed method obtains a correct iris recognition rate of 94,68%, even considering out of focus images and iris occlusions a correct iris recognition rate of 100% is obtained excluding problematic images. The proposed method is flexible, and allows to fine tune the iris recognition criterion according to the accuracy level required by the application.;;Programa de Pós-Graduação em Engenharia Elétrica – PPGEE, Universidade Federal do Rio Grande do Sul –  UFRGS;pt_BR;Published;15;2008;2008-12-03 9:11:00
7023;Adalberto Schuck Junior;Representação e Classificação de Texturas da Íris Baseada na Transformada Ótima de Gabor;This paper proposes to investigate the application of the method developed by Manjunah and Ma [12], namely, the Optimal Gabor Wavelet Transform (OGWT), in the context of iris texture representation. The proposed method was tested in a widely known database of 1205 eye images [16]. In each one of these images, the iris region was segmented, and then represented in multiple scales using the OGWT the íris texture patterns were represented by their statistics in the Wavelet domain, and compared using similarity metrics. The experimental results indicate that the proposed method obtains a correct iris recognition rate of 94,68%, even considering out of focus images and iris occlusions a correct iris recognition rate of 100% is obtained excluding problematic images. The proposed method is flexible, and allows to fine tune the iris recognition criterion according to the accuracy level required by the application.;;Programa de Pós-Graduação em Engenharia Elétrica – PPGEE, Universidade Federal do Rio Grande do Sul –  UFRGS;pt_BR;Published;15;2008;2008-12-03 9:11:00
7024;Dionísio Doering;A Novel Method for Generating Scale Space Kernels Based on Wavelet Theory;The linear scale-space kernel is a Gaussian or Poisson function. These functions were chosen based on several axioms. This representation creates a good base for visualization when there is no information (in advanced) about which scales are more important. These kernels have some deficiencies, as an example, its support region goes from minus to plus infinite. In order to solve these issues several others scale-space kernels have been proposed. In this paper we present a novel method to create scale-space kernels from one-dimensional wavelet functions. In order to do so, we show the scale-space and wavelet fundamental equations and then the relationship between them. We also describe three different methods to generate two-dimensional functions from one-dimensional functions. Then we show results got from scale-space blob detector using the original and two new scale-space bases (Haar and Bi-ortogonal 4.4), and a comparison between the edges detected using the Gaussian kernel and Haar kernel for a noisy image. Finally we show a comparison between the scale space Haar edge detector and the Canny edge detector for an image with one known square in it, for that case we show the Mean Square Error (MSE) of the edges detected with both algorithms.;;Lawrence Berkeley National Laboratory. One Cyclotron Road, MS50A6134, Berkeley, CA, USA;pt_BR;Published;17;2008;2008-12-03 14:04:02
7024;Adalberto Schuck Junior;A Novel Method for Generating Scale Space Kernels Based on Wavelet Theory;The linear scale-space kernel is a Gaussian or Poisson function. These functions were chosen based on several axioms. This representation creates a good base for visualization when there is no information (in advanced) about which scales are more important. These kernels have some deficiencies, as an example, its support region goes from minus to plus infinite. In order to solve these issues several others scale-space kernels have been proposed. In this paper we present a novel method to create scale-space kernels from one-dimensional wavelet functions. In order to do so, we show the scale-space and wavelet fundamental equations and then the relationship between them. We also describe three different methods to generate two-dimensional functions from one-dimensional functions. Then we show results got from scale-space blob detector using the original and two new scale-space bases (Haar and Bi-ortogonal 4.4), and a comparison between the edges detected using the Gaussian kernel and Haar kernel for a noisy image. Finally we show a comparison between the scale space Haar edge detector and the Canny edge detector for an image with one known square in it, for that case we show the Mean Square Error (MSE) of the edges detected with both algorithms.;;DELET – EE – UFRGS;pt_BR;Published;17;2008;2008-12-03 14:04:02
7029;Fabiane Fabiane Barreto Vavassori Benitti;Proposta de Utilização do Diagrama de Seqüência para Definição de Casos de Teste de Unidade;The step of software testing has been increasingly relevant in the software development process, directly impacting on the quality of the product. Among the tests applied in the software, it appears that the unit test does not present an artifact aligned with the standard adopted for specification of software (UML). In this sense, it presents a proposal for using the sequence diagram of how to define artifact of test cases, specifically geared to unit test, making the planning of tests regardless of programming language, expanding the possibilities for using the artifact. The suitability of the proposed diagram is demonstrated by using a tool that allows the generation of code to run on JUnit framework.;;Centro de Educação de Ciências Tecnológicas da Terra e do Mar, UNIVALI, Itajaí, SC;pt_BR;Published;15;2008;2008-12-03 14:12:16
7032;Elias Canhadas Genvigir;Uma Proposta de Modelagem para a Generalização de Elos de Rastreabilidade;Several models proposed traceability links that provide pre–definedgroups of links for requirements traceability. These models are limited to pre–defined links without the ability to add new attributes to the existing links. This work proposes a model for requirements traceability that generalizes the types of links already establishedin the literature and enables addition of new standards allowing the inclusion of attributes to the links that will be used in a specific traceability process.;;Universidade Tecnológica Federal do Paraná – UTFPR;pt_BR;Published;21;2008;2008-12-03 14:27:32
7032;Nandamudi Lankalapalli Vijaykumar;Uma Proposta de Modelagem para a Generalização de Elos de Rastreabilidade;Several models proposed traceability links that provide pre–definedgroups of links for requirements traceability. These models are limited to pre–defined links without the ability to add new attributes to the existing links. This work proposes a model for requirements traceability that generalizes the types of links already establishedin the literature and enables addition of new standards allowing the inclusion of attributes to the links that will be used in a specific traceability process.;;Instituto Nacional de Pesquisas Espaciais – INPE. Sao Jose dos Campos - SP;pt_BR;Published;21;2008;2008-12-03 14:27:32
7033;Luiz E. S. Oliveira;Inspeção Automática de Defeitos em Madeiras de Pinus usando Visão Computacional;This paper addresses the issue of detecting defects in Pine wood using features extracted from grayscale images. The feature set proposed here is based on the concept of texture and it is computed from the co-occurrence matrices. The features provide measures of properties such as smoothness, coarseness, and regularity. Comparative experiments using a color image based feature set extracted from percentile histograms are carried to demonstrate the efficiency of the proposed feature set. Two different learning paradigms, neural networks and support vector machines, and a feature selection algorithm based on multi-objective genetic algorithms were considered in our experiments. The experimental results show that after feature selection, the grayscale image based feature set achieves very competitive performance for the problem of wood defect detection relative to the color image based features.;;Pontifícia Universidade Católica do Paraná, Programa de Pós-Graduação em Informática Aplicada (PPGIa), Curitiba (PR);pt_BR;Published;15;2008;2008-12-03 14:45:17
7033;Paulo R. Cavalin;Inspeção Automática de Defeitos em Madeiras de Pinus usando Visão Computacional;This paper addresses the issue of detecting defects in Pine wood using features extracted from grayscale images. The feature set proposed here is based on the concept of texture and it is computed from the co-occurrence matrices. The features provide measures of properties such as smoothness, coarseness, and regularity. Comparative experiments using a color image based feature set extracted from percentile histograms are carried to demonstrate the efficiency of the proposed feature set. Two different learning paradigms, neural networks and support vector machines, and a feature selection algorithm based on multi-objective genetic algorithms were considered in our experiments. The experimental results show that after feature selection, the grayscale image based feature set achieves very competitive performance for the problem of wood defect detection relative to the color image based features.;;École de Technologie Supérieure (ÉTS), Montreal.;pt_BR;Published;15;2008;2008-12-03 14:45:17
7033;Alceu S. Britto Jr;Inspeção Automática de Defeitos em Madeiras de Pinus usando Visão Computacional;This paper addresses the issue of detecting defects in Pine wood using features extracted from grayscale images. The feature set proposed here is based on the concept of texture and it is computed from the co-occurrence matrices. The features provide measures of properties such as smoothness, coarseness, and regularity. Comparative experiments using a color image based feature set extracted from percentile histograms are carried to demonstrate the efficiency of the proposed feature set. Two different learning paradigms, neural networks and support vector machines, and a feature selection algorithm based on multi-objective genetic algorithms were considered in our experiments. The experimental results show that after feature selection, the grayscale image based feature set achieves very competitive performance for the problem of wood defect detection relative to the color image based features.;;Pontifícia Universidade Católica do Paraná, Programa de Pós-Graduação em Informática Aplicada (PPGIa), Curitiba (PR);pt_BR;Published;15;2008;2008-12-03 14:45:17
7033;Alessandro L. Koerich;Inspeção Automática de Defeitos em Madeiras de Pinus usando Visão Computacional;This paper addresses the issue of detecting defects in Pine wood using features extracted from grayscale images. The feature set proposed here is based on the concept of texture and it is computed from the co-occurrence matrices. The features provide measures of properties such as smoothness, coarseness, and regularity. Comparative experiments using a color image based feature set extracted from percentile histograms are carried to demonstrate the efficiency of the proposed feature set. Two different learning paradigms, neural networks and support vector machines, and a feature selection algorithm based on multi-objective genetic algorithms were considered in our experiments. The experimental results show that after feature selection, the grayscale image based feature set achieves very competitive performance for the problem of wood defect detection relative to the color image based features.;;Pontifícia Universidade Católica do Paraná, Programa de Pós-Graduação em Informática Aplicada (PPGIa), Curitiba (PR);pt_BR;Published;15;2008;2008-12-03 14:45:17
7146;José Palazzo Moreira de Oliveira;Revisores;The editors of the Journal of Theoretical and Applied Informatics would like to thank the editorial board and the following reviewers in 2008 for their efforts in maintaining the high standard of evaluation of the journal's articles.;;Instituto de Informática, UFRGS, Porto Alegre, RS;pt_BR;Published;1;2008;2008-12-12 11:04:59
7147;Flávio Henrique Teles Vieira;Modelagem de Tráfego de Redes Utilizando Cascata Multifractal Generalizada;In this paper we propose a multifractal traffic model that is based on a multiplicative cascade presenting specific multiplier distributions in each cascade stage. In the proposed model, the multipliers are obtained through the estimate of their probability densities found in real network traffic by using Kernel and Acceptance/Rejection methods. Statistical analysis and queueing behavior study were carried out for the model validation. Furthermore, we verify the model performance in capturing the traffic trace characteristics in comparison to other multifractal models.;;1Escola de Engenharia Elétrica e de Computação, Universidade Federal de Goiás, Av. Universitária, n.1488, Qd. 86, Bloco A, 74605-010, Goiânia, GO;pt_BR;Published;17;2008;2008-12-12 11:12:23
7147;Lee Luan Ling;Modelagem de Tráfego de Redes Utilizando Cascata Multifractal Generalizada;In this paper we propose a multifractal traffic model that is based on a multiplicative cascade presenting specific multiplier distributions in each cascade stage. In the proposed model, the multipliers are obtained through the estimate of their probability densities found in real network traffic by using Kernel and Acceptance/Rejection methods. Statistical analysis and queueing behavior study were carried out for the model validation. Furthermore, we verify the model performance in capturing the traffic trace characteristics in comparison to other multifractal models.;;2Departamento de Comunicações (DECOM), Faculdade de Engenharia Elétrica e de Computação, Universidade Estadual de Campinas Caixa Postal 6101, 13.083-852 SP, Brasil;pt_BR;Published;17;2008;2008-12-12 11:12:23
7857;Leyza Baldo Dorini;Teoria espaço-escala: conceitos e abordagens;Multi-scale approaches have been widely used in various signal analysis and processing applications, being essential in cases where there is no preliminary information about the appropriate observation scale. The basic idea is to create a family of derived signals, thus allowing the analysis of different levels of representation to choose those that exhibit the characteristics of interest. Space-scale theory is one of these approaches. From there, the necessary conditions can be established to define transformations that enable the manipulation of characteristics present at different levels in a consistent manner. This work provides a brief review of the main space-scale approaches, as well as their fundamental properties.;;Universidade Tecnológica Federal do Paraná;pt_BR;Published;21;2009;2009-03-04 23:48:49
7857;Neucimar Jerônimo Leite;Teoria espaço-escala: conceitos e abordagens;Multi-scale approaches have been widely used in various signal analysis and processing applications, being essential in cases where there is no preliminary information about the appropriate observation scale. The basic idea is to create a family of derived signals, thus allowing the analysis of different levels of representation to choose those that exhibit the characteristics of interest. Space-scale theory is one of these approaches. From there, the necessary conditions can be established to define transformations that enable the manipulation of characteristics present at different levels in a consistent manner. This work provides a brief review of the main space-scale approaches, as well as their fundamental properties.;;Universidade Estadual de Campinas;pt_BR;Published;21;2009;2009-03-04 23:48:49
8046;Leandro Ferreira;Desenvolvimento de uma rede neuro-fuzzy para predição da temperatura retal de frangos de corte;The objective of this work was to develop and validate a neuro-fuzzy network, based on the LOLIMOT algorithm, for predicting the rectal temperature of broiler chickens. The neuro-fuzzy network was developed based on three input variables: air temperature (T), relative humidity (RH) and air speed (V), with rectal temperature (TR) as the output variable. To create the network, Gaussian membership functions (weight functions) with a standard deviation of 0.33 were used. The number of partitions of the input space adopted by the model was equal to 10. The neuro-fuzzy network was developed in the SCILAB 4.1 computational environment. Experimental results showed that the average standard deviation between the simulated and measured values ​​was 0.11 °C. The neuro-fuzzy network presents itself as a satisfactory hybrid intelligent system for predicting the rectal temperature of broiler chickens, adding features of fuzzy logic based on fuzzy set theory to artificial neural networks.;;Universidade Federal de Lavras;pt_BR;Published;12;2009;2009-03-20 8:58:10
8046;Tadayuki Yanagi Junior;Desenvolvimento de uma rede neuro-fuzzy para predição da temperatura retal de frangos de corte;The objective of this work was to develop and validate a neuro-fuzzy network, based on the LOLIMOT algorithm, for predicting the rectal temperature of broiler chickens. The neuro-fuzzy network was developed based on three input variables: air temperature (T), relative humidity (RH) and air speed (V), with rectal temperature (TR) as the output variable. To create the network, Gaussian membership functions (weight functions) with a standard deviation of 0.33 were used. The number of partitions of the input space adopted by the model was equal to 10. The neuro-fuzzy network was developed in the SCILAB 4.1 computational environment. Experimental results showed that the average standard deviation between the simulated and measured values ​​was 0.11 °C. The neuro-fuzzy network presents itself as a satisfactory hybrid intelligent system for predicting the rectal temperature of broiler chickens, adding features of fuzzy logic based on fuzzy set theory to artificial neural networks.;;Departamento de Engenharia, Universidade Federal de Lavras (UFLA), Caixa Postal 3037,Lavras/MG, Brasil;pt_BR;Published;12;2009;2009-03-20 8:58:10
8046;Alison Zille Lopes;Desenvolvimento de uma rede neuro-fuzzy para predição da temperatura retal de frangos de corte;The objective of this work was to develop and validate a neuro-fuzzy network, based on the LOLIMOT algorithm, for predicting the rectal temperature of broiler chickens. The neuro-fuzzy network was developed based on three input variables: air temperature (T), relative humidity (RH) and air speed (V), with rectal temperature (TR) as the output variable. To create the network, Gaussian membership functions (weight functions) with a standard deviation of 0.33 were used. The number of partitions of the input space adopted by the model was equal to 10. The neuro-fuzzy network was developed in the SCILAB 4.1 computational environment. Experimental results showed that the average standard deviation between the simulated and measured values ​​was 0.11 °C. The neuro-fuzzy network presents itself as a satisfactory hybrid intelligent system for predicting the rectal temperature of broiler chickens, adding features of fuzzy logic based on fuzzy set theory to artificial neural networks.;;Departamento de Engenharia, Universidade Federal de Lavras (UFLA), Caixa Postal 3037,Lavras/MG, Brasil;pt_BR;Published;12;2009;2009-03-20 8:58:10
8046;Wilian Soares Lacerda;Desenvolvimento de uma rede neuro-fuzzy para predição da temperatura retal de frangos de corte;The objective of this work was to develop and validate a neuro-fuzzy network, based on the LOLIMOT algorithm, for predicting the rectal temperature of broiler chickens. The neuro-fuzzy network was developed based on three input variables: air temperature (T), relative humidity (RH) and air speed (V), with rectal temperature (TR) as the output variable. To create the network, Gaussian membership functions (weight functions) with a standard deviation of 0.33 were used. The number of partitions of the input space adopted by the model was equal to 10. The neuro-fuzzy network was developed in the SCILAB 4.1 computational environment. Experimental results showed that the average standard deviation between the simulated and measured values ​​was 0.11 °C. The neuro-fuzzy network presents itself as a satisfactory hybrid intelligent system for predicting the rectal temperature of broiler chickens, adding features of fuzzy logic based on fuzzy set theory to artificial neural networks.;;Departamento de Engenharia, Universidade Federal de Lavras (UFLA), Caixa Postal 3037,Lavras/MG, Brasil;pt_BR;Published;12;2009;2009-03-20 8:58:10
8168;Cristina Nader Vasconcelos;Introdução à Programação de Propósito Geral em Hardware Gráfico;The Graphics Processing Unit (GPU) was initially developed as hardware intended to increase the efficiency and power of graphics processing for rendering tasks. Today, the GPU presents itself as a versatile and of high computing power. It has become a real possibility in the search for solutions for processing large volumes of data, either as a complement or as an alternative to the use of multicore CPUs or distributed systems. The use of the GPU in general purpose computing is of special interest, since for several applications, there are still no sequential formulations that are sufficiently fast to be computed. This tutorial aims to allow the reader to identify algorithms and applications that are candidates for parallel approaches without GPU. generic purpose using graphics hardware without a priori knowledge of 3D graphics systems or parallel systems being essential for the reader.;;PUC-Rio;pt_BR;Published;26;2009;2009-04-04 21:06:12
8168;Paulo Cezar Carvalho;Introdução à Programação de Propósito Geral em Hardware Gráfico;The Graphics Processing Unit (GPU) was initially developed as hardware intended to increase the efficiency and power of graphics processing for rendering tasks. Today, the GPU presents itself as a versatile and of high computing power. It has become a real possibility in the search for solutions for processing large volumes of data, either as a complement or as an alternative to the use of multicore CPUs or distributed systems. The use of the GPU in general purpose computing is of special interest, since for several applications, there are still no sequential formulations that are sufficiently fast to be computed. This tutorial aims to allow the reader to identify algorithms and applications that are candidates for parallel approaches without GPU. generic purpose using graphics hardware without a priori knowledge of 3D graphics systems or parallel systems being essential for the reader.;;IMPA;pt_BR;Published;26;2009;2009-04-04 21:06:12
8168;Marcelo Gattass;Introdução à Programação de Propósito Geral em Hardware Gráfico;The Graphics Processing Unit (GPU) was initially developed as hardware intended to increase the efficiency and power of graphics processing for rendering tasks. Today, the GPU presents itself as a versatile and of high computing power. It has become a real possibility in the search for solutions for processing large volumes of data, either as a complement or as an alternative to the use of multicore CPUs or distributed systems. The use of the GPU in general purpose computing is of special interest, since for several applications, there are still no sequential formulations that are sufficiently fast to be computed. This tutorial aims to allow the reader to identify algorithms and applications that are candidates for parallel approaches without GPU. generic purpose using graphics hardware without a priori knowledge of 3D graphics systems or parallel systems being essential for the reader.;;Tecgraf/PUC-Rio;pt_BR;Published;26;2009;2009-04-04 21:06:12
8260;Alberto Raposo;A Journey through Virtual and Augmented Reality - Reviewing the SVR Symposia from 2004 to 2008;This article intends to provide a panorama of the BrazilianSymposium on Virtual and Augmented Reality (SVR) based on the analysis of its last four editions (2004, 2006, 2007, and 2008). We had reviewed the 124 full papers published in these four editions of the symposium and had analyzed them according to different criteria. These criteria include research topics, technologies used, and research approaches. The goal of this document is to draw an accurate picture of the research in this area in Brazil and try to provide insights about trends, deficiencies, and opportunities that may help guiding future efforts of the Brazilian research community in virtual and augmented reality.;;PUC-RJ;pt_BR;Published;15;2009;2009-04-09 11:19:24
8260;Luciano Soares;A Journey through Virtual and Augmented Reality - Reviewing the SVR Symposia from 2004 to 2008;This article intends to provide a panorama of the BrazilianSymposium on Virtual and Augmented Reality (SVR) based on the analysis of its last four editions (2004, 2006, 2007, and 2008). We had reviewed the 124 full papers published in these four editions of the symposium and had analyzed them according to different criteria. These criteria include research topics, technologies used, and research approaches. The goal of this document is to draw an accurate picture of the research in this area in Brazil and try to provide insights about trends, deficiencies, and opportunities that may help guiding future efforts of the Brazilian research community in virtual and augmented reality.;;PUC-RJ;pt_BR;Published;15;2009;2009-04-09 11:19:24
8260;Judith Kelner;A Journey through Virtual and Augmented Reality - Reviewing the SVR Symposia from 2004 to 2008;This article intends to provide a panorama of the BrazilianSymposium on Virtual and Augmented Reality (SVR) based on the analysis of its last four editions (2004, 2006, 2007, and 2008). We had reviewed the 124 full papers published in these four editions of the symposium and had analyzed them according to different criteria. These criteria include research topics, technologies used, and research approaches. The goal of this document is to draw an accurate picture of the research in this area in Brazil and try to provide insights about trends, deficiencies, and opportunities that may help guiding future efforts of the Brazilian research community in virtual and augmented reality.;;Universidade Federal de Pernambuco - UFPE;pt_BR;Published;15;2009;2009-04-09 11:19:24
8260;Veronica Teichrieb;A Journey through Virtual and Augmented Reality - Reviewing the SVR Symposia from 2004 to 2008;This article intends to provide a panorama of the BrazilianSymposium on Virtual and Augmented Reality (SVR) based on the analysis of its last four editions (2004, 2006, 2007, and 2008). We had reviewed the 124 full papers published in these four editions of the symposium and had analyzed them according to different criteria. These criteria include research topics, technologies used, and research approaches. The goal of this document is to draw an accurate picture of the research in this area in Brazil and try to provide insights about trends, deficiencies, and opportunities that may help guiding future efforts of the Brazilian research community in virtual and augmented reality.;;Universidade federal de Pernambuco -UFPE;pt_BR;Published;15;2009;2009-04-09 11:19:24
8261;Silvano Maneck Malfatti;The Design of a Graphics Engine for the Development of Virtual Reality Applications;This work presents the design and the features of a flexible realtime 3D graphics engine aimed at the development of multimedia applications and collaborative virtual environments. The engine, called EnCIMA (Engine for Collaborative andImmersive Multimedia Applications), enables a fast development process of applications by providing a high level interface, which has been implemented using the C++object-oriented programming paradigm. The main features of the proposed engine are the support to scene management, ability to load static and animated 3D models, particle system effects, network connection management to support collaboration, and collision detection. In addition, the engine supports several specialized interaction devices such as 3D mice, haptic devices, 3D motion trackers, data-gloves, and joystickswith and without force feedback. The engine also enables the developer to choose the way the scene should be rendered to, i.e. using standard display devices, stereoscopy, or even several simultaneous projection for spatially immersive devices. As part of the evaluation process, we have compared the performance of EnCIMA to a game engine and two scene graph toolkits, through the use of a testbed application. The performanceresults and the wide variety of non-conventional interaction devices supported are evidences that EnCIMA can be considered a real time virtual reality engine.;;Laboratório Nacional de Computação Científica, Laboratório ACiMA - RJ;pt_BR;Published;21;2009;2009-04-09 11:38:49
8261;Selan Rodrigues dos Santos;The Design of a Graphics Engine for the Development of Virtual Reality Applications;This work presents the design and the features of a flexible realtime 3D graphics engine aimed at the development of multimedia applications and collaborative virtual environments. The engine, called EnCIMA (Engine for Collaborative andImmersive Multimedia Applications), enables a fast development process of applications by providing a high level interface, which has been implemented using the C++object-oriented programming paradigm. The main features of the proposed engine are the support to scene management, ability to load static and animated 3D models, particle system effects, network connection management to support collaboration, and collision detection. In addition, the engine supports several specialized interaction devices such as 3D mice, haptic devices, 3D motion trackers, data-gloves, and joystickswith and without force feedback. The engine also enables the developer to choose the way the scene should be rendered to, i.e. using standard display devices, stereoscopy, or even several simultaneous projection for spatially immersive devices. As part of the evaluation process, we have compared the performance of EnCIMA to a game engine and two scene graph toolkits, through the use of a testbed application. The performanceresults and the wide variety of non-conventional interaction devices supported are evidences that EnCIMA can be considered a real time virtual reality engine.;;Universidade Federal do Rio Grande do Norte - RN;pt_BR;Published;21;2009;2009-04-09 11:38:49
8261;Luciane Machado Fraga;The Design of a Graphics Engine for the Development of Virtual Reality Applications;This work presents the design and the features of a flexible realtime 3D graphics engine aimed at the development of multimedia applications and collaborative virtual environments. The engine, called EnCIMA (Engine for Collaborative andImmersive Multimedia Applications), enables a fast development process of applications by providing a high level interface, which has been implemented using the C++object-oriented programming paradigm. The main features of the proposed engine are the support to scene management, ability to load static and animated 3D models, particle system effects, network connection management to support collaboration, and collision detection. In addition, the engine supports several specialized interaction devices such as 3D mice, haptic devices, 3D motion trackers, data-gloves, and joystickswith and without force feedback. The engine also enables the developer to choose the way the scene should be rendered to, i.e. using standard display devices, stereoscopy, or even several simultaneous projection for spatially immersive devices. As part of the evaluation process, we have compared the performance of EnCIMA to a game engine and two scene graph toolkits, through the use of a testbed application. The performanceresults and the wide variety of non-conventional interaction devices supported are evidences that EnCIMA can be considered a real time virtual reality engine.;;Laboratório Nacional de Computação Científica, Laboratório ACiMA - RJ;pt_BR;Published;21;2009;2009-04-09 11:38:49
8261;Claudia Marcela Justel;The Design of a Graphics Engine for the Development of Virtual Reality Applications;This work presents the design and the features of a flexible realtime 3D graphics engine aimed at the development of multimedia applications and collaborative virtual environments. The engine, called EnCIMA (Engine for Collaborative andImmersive Multimedia Applications), enables a fast development process of applications by providing a high level interface, which has been implemented using the C++object-oriented programming paradigm. The main features of the proposed engine are the support to scene management, ability to load static and animated 3D models, particle system effects, network connection management to support collaboration, and collision detection. In addition, the engine supports several specialized interaction devices such as 3D mice, haptic devices, 3D motion trackers, data-gloves, and joystickswith and without force feedback. The engine also enables the developer to choose the way the scene should be rendered to, i.e. using standard display devices, stereoscopy, or even several simultaneous projection for spatially immersive devices. As part of the evaluation process, we have compared the performance of EnCIMA to a game engine and two scene graph toolkits, through the use of a testbed application. The performanceresults and the wide variety of non-conventional interaction devices supported are evidences that EnCIMA can be considered a real time virtual reality engine.;;Instituto Militar de Engenharia - RJ;pt_BR;Published;21;2009;2009-04-09 11:38:49
8261;Paulo Fernando Ferreira Rosa;The Design of a Graphics Engine for the Development of Virtual Reality Applications;This work presents the design and the features of a flexible realtime 3D graphics engine aimed at the development of multimedia applications and collaborative virtual environments. The engine, called EnCIMA (Engine for Collaborative andImmersive Multimedia Applications), enables a fast development process of applications by providing a high level interface, which has been implemented using the C++object-oriented programming paradigm. The main features of the proposed engine are the support to scene management, ability to load static and animated 3D models, particle system effects, network connection management to support collaboration, and collision detection. In addition, the engine supports several specialized interaction devices such as 3D mice, haptic devices, 3D motion trackers, data-gloves, and joystickswith and without force feedback. The engine also enables the developer to choose the way the scene should be rendered to, i.e. using standard display devices, stereoscopy, or even several simultaneous projection for spatially immersive devices. As part of the evaluation process, we have compared the performance of EnCIMA to a game engine and two scene graph toolkits, through the use of a testbed application. The performanceresults and the wide variety of non-conventional interaction devices supported are evidences that EnCIMA can be considered a real time virtual reality engine.;;Instituto Militar de Engenharia - RJ;pt_BR;Published;21;2009;2009-04-09 11:38:49
8261;Jauvane Cavalvante de Oliveira;The Design of a Graphics Engine for the Development of Virtual Reality Applications;This work presents the design and the features of a flexible realtime 3D graphics engine aimed at the development of multimedia applications and collaborative virtual environments. The engine, called EnCIMA (Engine for Collaborative andImmersive Multimedia Applications), enables a fast development process of applications by providing a high level interface, which has been implemented using the C++object-oriented programming paradigm. The main features of the proposed engine are the support to scene management, ability to load static and animated 3D models, particle system effects, network connection management to support collaboration, and collision detection. In addition, the engine supports several specialized interaction devices such as 3D mice, haptic devices, 3D motion trackers, data-gloves, and joystickswith and without force feedback. The engine also enables the developer to choose the way the scene should be rendered to, i.e. using standard display devices, stereoscopy, or even several simultaneous projection for spatially immersive devices. As part of the evaluation process, we have compared the performance of EnCIMA to a game engine and two scene graph toolkits, through the use of a testbed application. The performanceresults and the wide variety of non-conventional interaction devices supported are evidences that EnCIMA can be considered a real time virtual reality engine.;;Laboratório Nacional de Computação Científica, Laboratório ACiMA - RJ;pt_BR;Published;21;2009;2009-04-09 11:38:49
8263;Fábio R. de Miranda;Designing and implementing an Spatial Augmented Reality X-Ray;Casting digital-controlled light over real object surfaces, thus creating new textures and special effects, brings to designers and engineers an endless spectrum of possibilities and innovation for their projects. This technology, called SpatialAugmented Reality, can be considered as a kind of "virtual material" that can be manipulated by designers in augmented environments. In order to achieve more knowledge on the use of this technology, we have developed AR X-Ray, a spatialaugmented reality tool. Through the use of a portable projector a Virtual X-Ray is emitted over real walls, allowing visual exploration of internal details of buildings (such as pipes and ducts). This paper presents the design, implementaion and results of this project. Some details regarding solutions for technical problems are also discussed, as following: projector tracking, proper registration of synthetic information over realobjects and the development of a shader that manipulates the transparency of textures to allow the user to view what is behind objects.;;Centro Universitário Senac - SP;pt_BR;Published;27;2009;2009-04-09 11:54:43
8263;Romero Tori;Designing and implementing an Spatial Augmented Reality X-Ray;Casting digital-controlled light over real object surfaces, thus creating new textures and special effects, brings to designers and engineers an endless spectrum of possibilities and innovation for their projects. This technology, called SpatialAugmented Reality, can be considered as a kind of "virtual material" that can be manipulated by designers in augmented environments. In order to achieve more knowledge on the use of this technology, we have developed AR X-Ray, a spatialaugmented reality tool. Through the use of a portable projector a Virtual X-Ray is emitted over real walls, allowing visual exploration of internal details of buildings (such as pipes and ducts). This paper presents the design, implementaion and results of this project. Some details regarding solutions for technical problems are also discussed, as following: projector tracking, proper registration of synthetic information over realobjects and the development of a shader that manipulates the transparency of textures to allow the user to view what is behind objects.;;Centro Universitário Senac - SP;pt_BR;Published;27;2009;2009-04-09 11:54:43
8263;Cláudio E. S. Bueno;Designing and implementing an Spatial Augmented Reality X-Ray;Casting digital-controlled light over real object surfaces, thus creating new textures and special effects, brings to designers and engineers an endless spectrum of possibilities and innovation for their projects. This technology, called SpatialAugmented Reality, can be considered as a kind of "virtual material" that can be manipulated by designers in augmented environments. In order to achieve more knowledge on the use of this technology, we have developed AR X-Ray, a spatialaugmented reality tool. Through the use of a portable projector a Virtual X-Ray is emitted over real walls, allowing visual exploration of internal details of buildings (such as pipes and ducts). This paper presents the design, implementaion and results of this project. Some details regarding solutions for technical problems are also discussed, as following: projector tracking, proper registration of synthetic information over realobjects and the development of a shader that manipulates the transparency of textures to allow the user to view what is behind objects.;;Centro Universitário Senac - SP;pt_BR;Published;27;2009;2009-04-09 11:54:43
8263;Lucas P. Trias;Designing and implementing an Spatial Augmented Reality X-Ray;Casting digital-controlled light over real object surfaces, thus creating new textures and special effects, brings to designers and engineers an endless spectrum of possibilities and innovation for their projects. This technology, called SpatialAugmented Reality, can be considered as a kind of "virtual material" that can be manipulated by designers in augmented environments. In order to achieve more knowledge on the use of this technology, we have developed AR X-Ray, a spatialaugmented reality tool. Through the use of a portable projector a Virtual X-Ray is emitted over real walls, allowing visual exploration of internal details of buildings (such as pipes and ducts). This paper presents the design, implementaion and results of this project. Some details regarding solutions for technical problems are also discussed, as following: projector tracking, proper registration of synthetic information over realobjects and the development of a shader that manipulates the transparency of textures to allow the user to view what is behind objects.;;Centro Universitário Senac - SP;pt_BR;Published;27;2009;2009-04-09 11:54:43
8265;Leandro A. F. Fernandes;A Conceptual Image-Based Data Glove for Computer-Human Interaction;Data gloves are devices equipped with sensors that capture the movements of the hand of the user in order to select or manipulate objects in a virtual world. Data gloves were introduced three decades ago and since then have been used in many 3D interaction techniques. However, good data gloves are too expensive and only a few of them can perceive the full set of hand movements. In this paper we describe the design of an image-based data glove (IBDG) prototype suitable for finger sensible applications, like virtual objects manipulation and interaction approaches. The proposed device uses a camera to track visual markers at finger tips, and a software module tocompute the position of each finger tip and its joints in real-time. To evaluate our concept, we have built a prototype and tested it with 15 volunteers. We also discuss how to improve the engineering of the prototype, how to turn it into a low cost interaction device, as well as other relevant issues about this original concept.;;Instituto de Informática, UFRGS;pt_BR;Published;19;2009;2009-04-09 12:09:32
8265;Vitor F. Pamplona;A Conceptual Image-Based Data Glove for Computer-Human Interaction;Data gloves are devices equipped with sensors that capture the movements of the hand of the user in order to select or manipulate objects in a virtual world. Data gloves were introduced three decades ago and since then have been used in many 3D interaction techniques. However, good data gloves are too expensive and only a few of them can perceive the full set of hand movements. In this paper we describe the design of an image-based data glove (IBDG) prototype suitable for finger sensible applications, like virtual objects manipulation and interaction approaches. The proposed device uses a camera to track visual markers at finger tips, and a software module tocompute the position of each finger tip and its joints in real-time. To evaluate our concept, we have built a prototype and tested it with 15 volunteers. We also discuss how to improve the engineering of the prototype, how to turn it into a low cost interaction device, as well as other relevant issues about this original concept.;;Instituto de Informática, UFRGS;pt_BR;Published;19;2009;2009-04-09 12:09:32
8265;João L. Prauchner;A Conceptual Image-Based Data Glove for Computer-Human Interaction;Data gloves are devices equipped with sensors that capture the movements of the hand of the user in order to select or manipulate objects in a virtual world. Data gloves were introduced three decades ago and since then have been used in many 3D interaction techniques. However, good data gloves are too expensive and only a few of them can perceive the full set of hand movements. In this paper we describe the design of an image-based data glove (IBDG) prototype suitable for finger sensible applications, like virtual objects manipulation and interaction approaches. The proposed device uses a camera to track visual markers at finger tips, and a software module tocompute the position of each finger tip and its joints in real-time. To evaluate our concept, we have built a prototype and tested it with 15 volunteers. We also discuss how to improve the engineering of the prototype, how to turn it into a low cost interaction device, as well as other relevant issues about this original concept.;;Instituto de Informática, UFRGS;pt_BR;Published;19;2009;2009-04-09 12:09:32
8265;Luciana P. Nedel;A Conceptual Image-Based Data Glove for Computer-Human Interaction;Data gloves are devices equipped with sensors that capture the movements of the hand of the user in order to select or manipulate objects in a virtual world. Data gloves were introduced three decades ago and since then have been used in many 3D interaction techniques. However, good data gloves are too expensive and only a few of them can perceive the full set of hand movements. In this paper we describe the design of an image-based data glove (IBDG) prototype suitable for finger sensible applications, like virtual objects manipulation and interaction approaches. The proposed device uses a camera to track visual markers at finger tips, and a software module tocompute the position of each finger tip and its joints in real-time. To evaluate our concept, we have built a prototype and tested it with 15 volunteers. We also discuss how to improve the engineering of the prototype, how to turn it into a low cost interaction device, as well as other relevant issues about this original concept.;;Instituto de Informática, UFRGS;pt_BR;Published;19;2009;2009-04-09 12:09:32
8265;Manuel M. Oliveira;A Conceptual Image-Based Data Glove for Computer-Human Interaction;Data gloves are devices equipped with sensors that capture the movements of the hand of the user in order to select or manipulate objects in a virtual world. Data gloves were introduced three decades ago and since then have been used in many 3D interaction techniques. However, good data gloves are too expensive and only a few of them can perceive the full set of hand movements. In this paper we describe the design of an image-based data glove (IBDG) prototype suitable for finger sensible applications, like virtual objects manipulation and interaction approaches. The proposed device uses a camera to track visual markers at finger tips, and a software module tocompute the position of each finger tip and its joints in real-time. To evaluate our concept, we have built a prototype and tested it with 15 volunteers. We also discuss how to improve the engineering of the prototype, how to turn it into a low cost interaction device, as well as other relevant issues about this original concept.;;Instituto de Informática, UFRGS;pt_BR;Published;19;2009;2009-04-09 12:09:32
8266;Ednaldo Brigante Pizzolato;A Software Framework to Create 3D Browser-Based Speech Enabled Applications;The advances in automatic speech recognition have pushed the humancomputer interface researchers to adopt speech as one mean of input data. It is natural to humans, and complements very well other input interfaces. However, integrating an automatic speech recognizer into a complex system (such as a 3D visualization system or a Virtual Reality system) can be a difficult and time consuming task. In this paper we present our approach to the problem, a software framework requiringminimum additional coding from the application developer. The framework combines voice commands with existing interaction code, automating the task of creating a new speech grammar (to be used by the recognizer). A new listener component for theXj3D was created, which makes transparent to the user the integration between the 3D browser and the recognizer. We believe this is a desirable feature for virtual reality system developers, and also to be used as a rapid prototyping tool when experimenting with speech technology.;;Universidade Federal de São Carlos - SP;pt_BR;Published;19;2009;2009-04-09 12:17:12
8266;Diego Daniel Duarte;A Software Framework to Create 3D Browser-Based Speech Enabled Applications;The advances in automatic speech recognition have pushed the humancomputer interface researchers to adopt speech as one mean of input data. It is natural to humans, and complements very well other input interfaces. However, integrating an automatic speech recognizer into a complex system (such as a 3D visualization system or a Virtual Reality system) can be a difficult and time consuming task. In this paper we present our approach to the problem, a software framework requiringminimum additional coding from the application developer. The framework combines voice commands with existing interaction code, automating the task of creating a new speech grammar (to be used by the recognizer). A new listener component for theXj3D was created, which makes transparent to the user the integration between the 3D browser and the recognizer. We believe this is a desirable feature for virtual reality system developers, and also to be used as a rapid prototyping tool when experimenting with speech technology.;;Universidade Federal de São Carlos - SP;pt_BR;Published;19;2009;2009-04-09 12:17:12
8266;Marcio Merino Fernandes;A Software Framework to Create 3D Browser-Based Speech Enabled Applications;The advances in automatic speech recognition have pushed the humancomputer interface researchers to adopt speech as one mean of input data. It is natural to humans, and complements very well other input interfaces. However, integrating an automatic speech recognizer into a complex system (such as a 3D visualization system or a Virtual Reality system) can be a difficult and time consuming task. In this paper we present our approach to the problem, a software framework requiringminimum additional coding from the application developer. The framework combines voice commands with existing interaction code, automating the task of creating a new speech grammar (to be used by the recognizer). A new listener component for theXj3D was created, which makes transparent to the user the integration between the 3D browser and the recognizer. We believe this is a desirable feature for virtual reality system developers, and also to be used as a rapid prototyping tool when experimenting with speech technology.;;Universidade Federal de São Carlos - SP;pt_BR;Published;19;2009;2009-04-09 12:17:12
8441;Edeilson Milhomem Silva;SWEETS: um Sistema de Recomendação de Especialistas aplicado a uma plataforma de Gestão de Conhecimento;Organizations, with the aim of increasing their level of competitiveness in the market, are constantly looking for new ways to improve productivity and the quality of the products developed, in addition to reducing costs – which is directly related to the increase in net revenue. For these objectives to be achieved, it is essential to fully explore the potential of your employees and the possible relationships that these employees have with each other, that is, finding and sharing tacit knowledge. As tactical knowledge is in people's minds, it is difficult to be formalized and documented, therefore, the ideal would be to identify and recommend the person who holds the knowledge. Given this, this dissertation presents the SWEETS Expert Recommendation System and its implementation in the a.m.i.g.o.s. environment, a knowledge management platform based on concepts focused on social networks. SWEETS was developed in two versions, 1.0 and 2.0. Version 1.0, in a proactive way, brings together people with common specialties, sometimes because of their knowledge (writing profile), sometimes because of their interests (reading profile). Version 2.0 of SWEETS does not act proactively, that is, it is necessary to request a user who is a specialist in a given area, and is based on folksonomy to extract an ontology, essential for identifying the specialties of people in more effectively. This ontology is reflected by the co-occurrence of tags (concepts) in relation to items (instances) and is domain independent – ​​the main contribution of this dissertation. The implementation of SWEETS in a.m.i.g.o.s. aims to bring benefits such as: minimizing the communication problem in the corporation, providing an incentive for social knowledge and sharing knowledge, thus providing the company with the most effective use of its employees' knowledge. Keywords: Recommendation Systems, Web Social Networks, folsonomy and ontology.;;Universidade Federal de Pernambuco;pt_BR;Published;28;2009;2009-04-26 18:52:36
8441;Ricardo A. Costa;SWEETS: um Sistema de Recomendação de Especialistas aplicado a uma plataforma de Gestão de Conhecimento;Organizations, with the aim of increasing their level of competitiveness in the market, are constantly looking for new ways to improve productivity and the quality of the products developed, in addition to reducing costs – which is directly related to the increase in net revenue. For these objectives to be achieved, it is essential to fully explore the potential of your employees and the possible relationships that these employees have with each other, that is, finding and sharing tacit knowledge. As tactical knowledge is in people's minds, it is difficult to be formalized and documented, therefore, the ideal would be to identify and recommend the person who holds the knowledge. Given this, this dissertation presents the SWEETS Expert Recommendation System and its implementation in the a.m.i.g.o.s. environment, a knowledge management platform based on concepts focused on social networks. SWEETS was developed in two versions, 1.0 and 2.0. Version 1.0, in a proactive way, brings together people with common specialties, sometimes because of their knowledge (writing profile), sometimes because of their interests (reading profile). Version 2.0 of SWEETS does not act proactively, that is, it is necessary to request a user who is a specialist in a given area, and is based on folksonomy to extract an ontology, essential for identifying the specialties of people in more effectively. This ontology is reflected by the co-occurrence of tags (concepts) in relation to items (instances) and is domain independent – ​​the main contribution of this dissertation. The implementation of SWEETS in a.m.i.g.o.s. aims to bring benefits such as: minimizing the communication problem in the corporation, providing an incentive for social knowledge and sharing knowledge, thus providing the company with the most effective use of its employees' knowledge. Keywords: Recommendation Systems, Web Social Networks, folsonomy and ontology.;;Centro de Informática, UFPE, Caixa Postal 7851 – Recife, PE - BrasilC.E.S.A.R - Centro de Estudos e Sistemas Avançados do Recife, Rua Bione, 220 – Bairro do Recife - 50.030-390 -Recife, PE - Brasil;pt_BR;Published;28;2009;2009-04-26 18:52:36
8441;Lucas R. B. Schmitz;SWEETS: um Sistema de Recomendação de Especialistas aplicado a uma plataforma de Gestão de Conhecimento;Organizations, with the aim of increasing their level of competitiveness in the market, are constantly looking for new ways to improve productivity and the quality of the products developed, in addition to reducing costs – which is directly related to the increase in net revenue. For these objectives to be achieved, it is essential to fully explore the potential of your employees and the possible relationships that these employees have with each other, that is, finding and sharing tacit knowledge. As tactical knowledge is in people's minds, it is difficult to be formalized and documented, therefore, the ideal would be to identify and recommend the person who holds the knowledge. Given this, this dissertation presents the SWEETS Expert Recommendation System and its implementation in the a.m.i.g.o.s. environment, a knowledge management platform based on concepts focused on social networks. SWEETS was developed in two versions, 1.0 and 2.0. Version 1.0, in a proactive way, brings together people with common specialties, sometimes because of their knowledge (writing profile), sometimes because of their interests (reading profile). Version 2.0 of SWEETS does not act proactively, that is, it is necessary to request a user who is a specialist in a given area, and is based on folksonomy to extract an ontology, essential for identifying the specialties of people in more effectively. This ontology is reflected by the co-occurrence of tags (concepts) in relation to items (instances) and is domain independent – ​​the main contribution of this dissertation. The implementation of SWEETS in a.m.i.g.o.s. aims to bring benefits such as: minimizing the communication problem in the corporation, providing an incentive for social knowledge and sharing knowledge, thus providing the company with the most effective use of its employees' knowledge. Keywords: Recommendation Systems, Web Social Networks, folsonomy and ontology.;;C.E.S.A.R - Centro de Estudos e Sistemas Avançados do Recife, Rua Bione, 220 – Bairro do Recife - 50.030-390 -Recife, PE - Brasil;pt_BR;Published;28;2009;2009-04-26 18:52:36
8441;Silvio R. L. Meira;SWEETS: um Sistema de Recomendação de Especialistas aplicado a uma plataforma de Gestão de Conhecimento;Organizations, with the aim of increasing their level of competitiveness in the market, are constantly looking for new ways to improve productivity and the quality of the products developed, in addition to reducing costs – which is directly related to the increase in net revenue. For these objectives to be achieved, it is essential to fully explore the potential of your employees and the possible relationships that these employees have with each other, that is, finding and sharing tacit knowledge. As tactical knowledge is in people's minds, it is difficult to be formalized and documented, therefore, the ideal would be to identify and recommend the person who holds the knowledge. Given this, this dissertation presents the SWEETS Expert Recommendation System and its implementation in the a.m.i.g.o.s. environment, a knowledge management platform based on concepts focused on social networks. SWEETS was developed in two versions, 1.0 and 2.0. Version 1.0, in a proactive way, brings together people with common specialties, sometimes because of their knowledge (writing profile), sometimes because of their interests (reading profile). Version 2.0 of SWEETS does not act proactively, that is, it is necessary to request a user who is a specialist in a given area, and is based on folksonomy to extract an ontology, essential for identifying the specialties of people in more effectively. This ontology is reflected by the co-occurrence of tags (concepts) in relation to items (instances) and is domain independent – ​​the main contribution of this dissertation. The implementation of SWEETS in a.m.i.g.o.s. aims to bring benefits such as: minimizing the communication problem in the corporation, providing an incentive for social knowledge and sharing knowledge, thus providing the company with the most effective use of its employees' knowledge. Keywords: Recommendation Systems, Web Social Networks, folsonomy and ontology.;;Centro de Informática, UFPE, Caixa Postal 7851 – Recife, PE - BrasilC.E.S.A.R - Centro de Estudos e Sistemas Avançados do Recife, Rua Bione, 220 – Bairro do Recife - 50.030-390 -Recife, PE - Brasil;pt_BR;Published;28;2009;2009-04-26 18:52:36
8611;Richardson Ribeiro;Combinando Modelos de Interação para Melhorar a Coordenação em Sistemas Multiagente;The main contribution of this article is the implementation of a hybrid coordination method based on the combination of previously developed interaction models. Interaction models are based on sharing rewards for learning with multiple agents, in order to interactively discover good quality policies. The exchange of rewards between agents during interaction is a complex task and if carried out inappropriately it can cause delays in learning or even cause unexpected behaviors, making cooperation inefficient and converging on an unsatisfactory policy. Based on these concepts, the hybrid method uses the particularities of each model, reducing possible conflicts between actions with different policy rewards, improving the coordination of agents in reinforcement learning problems. Experimental results show that the hybrid method is capable of accelerating convergence, quickly achieving optimal policies even in large state spaces, surpassing the results of classical reinforcement learning approaches.;;Universidade Tecnológica Federal do Paraná (UTFPR);pt_BR;Published;24;2009;2009-05-14 16:18:10
8611;André Pinz Borges;Combinando Modelos de Interação para Melhorar a Coordenação em Sistemas Multiagente;The main contribution of this article is the implementation of a hybrid coordination method based on the combination of previously developed interaction models. Interaction models are based on sharing rewards for learning with multiple agents, in order to interactively discover good quality policies. The exchange of rewards between agents during interaction is a complex task and if carried out inappropriately it can cause delays in learning or even cause unexpected behaviors, making cooperation inefficient and converging on an unsatisfactory policy. Based on these concepts, the hybrid method uses the particularities of each model, reducing possible conflicts between actions with different policy rewards, improving the coordination of agents in reinforcement learning problems. Experimental results show that the hybrid method is capable of accelerating convergence, quickly achieving optimal policies even in large state spaces, surpassing the results of classical reinforcement learning approaches.;;Pontifícia Universidade Católica do Paraná - PUCPR;pt_BR;Published;24;2009;2009-05-14 16:18:10
8611;Adriano Francisco Ronszcka;Combinando Modelos de Interação para Melhorar a Coordenação em Sistemas Multiagente;The main contribution of this article is the implementation of a hybrid coordination method based on the combination of previously developed interaction models. Interaction models are based on sharing rewards for learning with multiple agents, in order to interactively discover good quality policies. The exchange of rewards between agents during interaction is a complex task and if carried out inappropriately it can cause delays in learning or even cause unexpected behaviors, making cooperation inefficient and converging on an unsatisfactory policy. Based on these concepts, the hybrid method uses the particularities of each model, reducing possible conflicts between actions with different policy rewards, improving the coordination of agents in reinforcement learning problems. Experimental results show that the hybrid method is capable of accelerating convergence, quickly achieving optimal policies even in large state spaces, surpassing the results of classical reinforcement learning approaches.;;Universidade Tecnológica Federal do Paraná (UTFPR);pt_BR;Published;24;2009;2009-05-14 16:18:10
8611;Edson Emílio Scalabrin;Combinando Modelos de Interação para Melhorar a Coordenação em Sistemas Multiagente;The main contribution of this article is the implementation of a hybrid coordination method based on the combination of previously developed interaction models. Interaction models are based on sharing rewards for learning with multiple agents, in order to interactively discover good quality policies. The exchange of rewards between agents during interaction is a complex task and if carried out inappropriately it can cause delays in learning or even cause unexpected behaviors, making cooperation inefficient and converging on an unsatisfactory policy. Based on these concepts, the hybrid method uses the particularities of each model, reducing possible conflicts between actions with different policy rewards, improving the coordination of agents in reinforcement learning problems. Experimental results show that the hybrid method is capable of accelerating convergence, quickly achieving optimal policies even in large state spaces, surpassing the results of classical reinforcement learning approaches.;;Pontifícia Universidade Católica do Paraná - PUCPR;pt_BR;Published;24;2009;2009-05-14 16:18:10
8611;Braulio Coelho Ávila;Combinando Modelos de Interação para Melhorar a Coordenação em Sistemas Multiagente;The main contribution of this article is the implementation of a hybrid coordination method based on the combination of previously developed interaction models. Interaction models are based on sharing rewards for learning with multiple agents, in order to interactively discover good quality policies. The exchange of rewards between agents during interaction is a complex task and if carried out inappropriately it can cause delays in learning or even cause unexpected behaviors, making cooperation inefficient and converging on an unsatisfactory policy. Based on these concepts, the hybrid method uses the particularities of each model, reducing possible conflicts between actions with different policy rewards, improving the coordination of agents in reinforcement learning problems. Experimental results show that the hybrid method is capable of accelerating convergence, quickly achieving optimal policies even in large state spaces, surpassing the results of classical reinforcement learning approaches.;;Pontifícia Universidade Católica do Paraná - PUCPR;pt_BR;Published;24;2009;2009-05-14 16:18:10
8611;Fabrício Enembreck;Combinando Modelos de Interação para Melhorar a Coordenação em Sistemas Multiagente;The main contribution of this article is the implementation of a hybrid coordination method based on the combination of previously developed interaction models. Interaction models are based on sharing rewards for learning with multiple agents, in order to interactively discover good quality policies. The exchange of rewards between agents during interaction is a complex task and if carried out inappropriately it can cause delays in learning or even cause unexpected behaviors, making cooperation inefficient and converging on an unsatisfactory policy. Based on these concepts, the hybrid method uses the particularities of each model, reducing possible conflicts between actions with different policy rewards, improving the coordination of agents in reinforcement learning problems. Experimental results show that the hybrid method is capable of accelerating convergence, quickly achieving optimal policies even in large state spaces, surpassing the results of classical reinforcement learning approaches.;;Pontifícia Universidade Católica do Paraná - PUCPR;pt_BR;Published;24;2009;2009-05-14 16:18:10
9009;Jairo Francisco Souza;Uma abordagem estrutural para calcular similaridade entre conceitos de ontologias;The problem of compatibility between ontologies is still an open problem. To address this problem, we present the algorithm implemented in the GNoSIS system. Our solution makes use of both syntactic and semantic techniques in a structural approach, facilitating the mapping or alignment between ontologies from different domains. This algorithm uses different similarity functions and calculates the degree of similarity between concepts recursively, calculating the result of the similarity function between two concepts based on the degree of total similarity between concepts that are closely related. A validation of the approach is presented in this paper.;;Universidade Federal de Juiz de Fora;pt_BR;Published;20;2009;2009-06-17 21:28:30
9009;Rubens Nascimento Melo;Uma abordagem estrutural para calcular similaridade entre conceitos de ontologias;The problem of compatibility between ontologies is still an open problem. To address this problem, we present the algorithm implemented in the GNoSIS system. Our solution makes use of both syntactic and semantic techniques in a structural approach, facilitating the mapping or alignment between ontologies from different domains. This algorithm uses different similarity functions and calculates the degree of similarity between concepts recursively, calculating the result of the similarity function between two concepts based on the degree of total similarity between concepts that are closely related. A validation of the approach is presented in this paper.;;Pontifícia Universidade Católica do Rio de Janeiro;pt_BR;Published;20;2009;2009-06-17 21:28:30
9009;Jonice Oliveira;Uma abordagem estrutural para calcular similaridade entre conceitos de ontologias;The problem of compatibility between ontologies is still an open problem. To address this problem, we present the algorithm implemented in the GNoSIS system. Our solution makes use of both syntactic and semantic techniques in a structural approach, facilitating the mapping or alignment between ontologies from different domains. This algorithm uses different similarity functions and calculates the degree of similarity between concepts recursively, calculating the result of the similarity function between two concepts based on the degree of total similarity between concepts that are closely related. A validation of the approach is presented in this paper.;;Universidade Federal do Rio de Janeiro;pt_BR;Published;20;2009;2009-06-17 21:28:30
9009;Jano Moreira Souza;Uma abordagem estrutural para calcular similaridade entre conceitos de ontologias;The problem of compatibility between ontologies is still an open problem. To address this problem, we present the algorithm implemented in the GNoSIS system. Our solution makes use of both syntactic and semantic techniques in a structural approach, facilitating the mapping or alignment between ontologies from different domains. This algorithm uses different similarity functions and calculates the degree of similarity between concepts recursively, calculating the result of the similarity function between two concepts based on the degree of total similarity between concepts that are closely related. A validation of the approach is presented in this paper.;;Universidade Federal do Rio de Janeiro;pt_BR;Published;20;2009;2009-06-17 21:28:30
9644;Luciano Jose Senger;Aplicação de redes neurais ART e análise de textura para a classificação do estado de alteração de agregados minerais;A new approach for identifying the state of alteration of mineral aggregates intended for civil construction works is presented. Such identification is of fundamental importance to avoid failures and the occurrence of premature defects when carrying out works that can be attributed to the quality of the aggregate used in terms of its state of alteration. Image processing techniques are used to acquire the histograms of the color channels of the images, followed by calculating the entropy of the histograms, which provides the main characteristics for classification. Finally, an incremental knowledge acquisition and classification model employing ART (Adaptive Resonance Theory) neural networks is constructed to automate the classification process. The classification model is organized in two steps. In the first stage, the aggregates are classified as changed and unchanged, and in a second stage, the group of changed aggregates is classified according to the degree of change. The proposed model presents better classification results when compared to those obtained through other classification algorithms.;;Universidade Estadual de Ponta Grossa;pt_BR;Published;20;2009;2009-08-03 14:37:19
9644;Lilian Tais de Gouveia;Aplicação de redes neurais ART e análise de textura para a classificação do estado de alteração de agregados minerais;A new approach for identifying the state of alteration of mineral aggregates intended for civil construction works is presented. Such identification is of fundamental importance to avoid failures and the occurrence of premature defects when carrying out works that can be attributed to the quality of the aggregate used in terms of its state of alteration. Image processing techniques are used to acquire the histograms of the color channels of the images, followed by calculating the entropy of the histograms, which provides the main characteristics for classification. Finally, an incremental knowledge acquisition and classification model employing ART (Adaptive Resonance Theory) neural networks is constructed to automate the classification process. The classification model is organized in two steps. In the first stage, the aggregates are classified as changed and unchanged, and in a second stage, the group of changed aggregates is classified according to the degree of change. The proposed model presents better classification results when compared to those obtained through other classification algorithms.;;Instituto de Física de São Carlos;pt_BR;Published;20;2009;2009-08-03 14:37:19
10082;Ismayle Sousa Santos;Geração de Testes de Desempenho e Estresse a partir de Testes Funcionais;Although Software Testing is related to quality assurance, it demands time and resources, which is why it ends up not being carried out in part of the projects carried out. In the context of companies that always carry it out, it is clear that functional testing is growing in use and that performance and stress testing is still rarely performed, even in the face of the increase in systems accessible via the Web, in which these tests are considered essential. In this article, a method and its support tool are proposed for generating performance and stress tests for Web systems from functional tests, in an attempt to popularize testing for these non-functional requirements. An experimental study is also presented that concluded that the use of the method and tool can generate gains in terms of reducing effort for generating these tests.;;UNIVERSIDADE FEDERAL DO PIAUÍ;pt_BR;Published;18;2009;2009-09-14 13:51:34
10082;Pedro Alcântara Santos Neto;Geração de Testes de Desempenho e Estresse a partir de Testes Funcionais;Although Software Testing is related to quality assurance, it demands time and resources, which is why it ends up not being carried out in part of the projects carried out. In the context of companies that always carry it out, it is clear that functional testing is growing in use and that performance and stress testing is still rarely performed, even in the face of the increase in systems accessible via the Web, in which these tests are considered essential. In this article, a method and its support tool are proposed for generating performance and stress tests for Web systems from functional tests, in an attempt to popularize testing for these non-functional requirements. An experimental study is also presented that concluded that the use of the method and tool can generate gains in terms of reducing effort for generating these tests.;;UNIVERSIDADE FEDERAL DO PIAUÍ;pt_BR;Published;18;2009;2009-09-14 13:51:34
10082;Rodolfo Resende;Geração de Testes de Desempenho e Estresse a partir de Testes Funcionais;Although Software Testing is related to quality assurance, it demands time and resources, which is why it ends up not being carried out in part of the projects carried out. In the context of companies that always carry it out, it is clear that functional testing is growing in use and that performance and stress testing is still rarely performed, even in the face of the increase in systems accessible via the Web, in which these tests are considered essential. In this article, a method and its support tool are proposed for generating performance and stress tests for Web systems from functional tests, in an attempt to popularize testing for these non-functional requirements. An experimental study is also presented that concluded that the use of the method and tool can generate gains in terms of reducing effort for generating these tests.;;UNIVERSIDADE FEDERAL DE MINAS GERAIS;pt_BR;Published;18;2009;2009-09-14 13:51:34
10385;Marinalva Dias Soares;TSML: A XML-based Format for Exchange of Training Samples for Pattern Recognition in Remote Sensing Images;The availability of large and complex data sets has shifted the focus of pattern recognition towards developing techniques that can efficiently handle these types of data sets. For example, Multiple Classifier Systems claim their ability in reducing the error and complexity of classification by partitioning the data space and combining classifiers predictions. However, it is not an easy task to generate several partitions and moreover to use them in an efficient manner. Another difficult aspect is related to the exchange of training data in different formats among systems to combine classifiers of different and heterogeneous systems. This paper presents a model and structure of training samples based on XML (eXtensible Markup Language) to facilitate the partitioning and exchange among different image classification system. The main contribution is to apply the flexibility of XML that addresses interoperability and communication among heterogeneous systems in partitioning data sets as well as to facilitate interchange of such sets among image processing and pattern recognition systems.;;Instituto Nacional de Pesquisas Espaciais - INPE;pt_BR;Published;17;2009;2009-09-29 21:12:15
10385;Luciano Vieira Dutra;TSML: A XML-based Format for Exchange of Training Samples for Pattern Recognition in Remote Sensing Images;The availability of large and complex data sets has shifted the focus of pattern recognition towards developing techniques that can efficiently handle these types of data sets. For example, Multiple Classifier Systems claim their ability in reducing the error and complexity of classification by partitioning the data space and combining classifiers predictions. However, it is not an easy task to generate several partitions and moreover to use them in an efficient manner. Another difficult aspect is related to the exchange of training data in different formats among systems to combine classifiers of different and heterogeneous systems. This paper presents a model and structure of training samples based on XML (eXtensible Markup Language) to facilitate the partitioning and exchange among different image classification system. The main contribution is to apply the flexibility of XML that addresses interoperability and communication among heterogeneous systems in partitioning data sets as well as to facilitate interchange of such sets among image processing and pattern recognition systems.;;Instituto Nacional de Pesquisas Espaciais - INPE;pt_BR;Published;17;2009;2009-09-29 21:12:15
10385;Nandamudi Vijaykumar;TSML: A XML-based Format for Exchange of Training Samples for Pattern Recognition in Remote Sensing Images;The availability of large and complex data sets has shifted the focus of pattern recognition towards developing techniques that can efficiently handle these types of data sets. For example, Multiple Classifier Systems claim their ability in reducing the error and complexity of classification by partitioning the data space and combining classifiers predictions. However, it is not an easy task to generate several partitions and moreover to use them in an efficient manner. Another difficult aspect is related to the exchange of training data in different formats among systems to combine classifiers of different and heterogeneous systems. This paper presents a model and structure of training samples based on XML (eXtensible Markup Language) to facilitate the partitioning and exchange among different image classification system. The main contribution is to apply the flexibility of XML that addresses interoperability and communication among heterogeneous systems in partitioning data sets as well as to facilitate interchange of such sets among image processing and pattern recognition systems.;;Instituto Nacional de Pesquisas Espaciais - INPE;pt_BR;Published;17;2009;2009-09-29 21:12:15
11066;Luciana Maria Azevedo Nascimento;Implantação de Medição no Processo de Desenvolvimento de Software – Relato de Experiência e Lições Aprendidas;Measurement is recognized as an important process to support software development management, but its effective implementation faces challenges related to the effort and knowledge required to generate useful information for decision making. The challenge is even greater when it comes to immature organizations or those without experience with measurements. As a way of contributing to measurement implementation initiatives in this context, this article reports the experience, strategies and results of measurement implementation in a software development project aiming to provide management with information for analysis and action taking.;;Universidade Federal do Pará;pt_BR;Published;13;2009;2009-10-23 16:17:19
11066;Talita Vieira Ribeiro;Implantação de Medição no Processo de Desenvolvimento de Software – Relato de Experiência e Lições Aprendidas;Measurement is recognized as an important process to support software development management, but its effective implementation faces challenges related to the effort and knowledge required to generate useful information for decision making. The challenge is even greater when it comes to immature organizations or those without experience with measurements. As a way of contributing to measurement implementation initiatives in this context, this article reports the experience, strategies and results of measurement implementation in a software development project aiming to provide management with information for analysis and action taking.;;Universidade Federal do Pará;pt_BR;Published;13;2009;2009-10-23 16:17:19
11066;Carla Alessandra Lima Reis;Implantação de Medição no Processo de Desenvolvimento de Software – Relato de Experiência e Lições Aprendidas;Measurement is recognized as an important process to support software development management, but its effective implementation faces challenges related to the effort and knowledge required to generate useful information for decision making. The challenge is even greater when it comes to immature organizations or those without experience with measurements. As a way of contributing to measurement implementation initiatives in this context, this article reports the experience, strategies and results of measurement implementation in a software development project aiming to provide management with information for analysis and action taking.;;Universidade Federal do Pará;pt_BR;Published;13;2009;2009-10-23 16:17:19
11066;Rodrigo Quites Reis;Implantação de Medição no Processo de Desenvolvimento de Software – Relato de Experiência e Lições Aprendidas;Measurement is recognized as an important process to support software development management, but its effective implementation faces challenges related to the effort and knowledge required to generate useful information for decision making. The challenge is even greater when it comes to immature organizations or those without experience with measurements. As a way of contributing to measurement implementation initiatives in this context, this article reports the experience, strategies and results of measurement implementation in a software development project aiming to provide management with information for analysis and action taking.;;Universidade Federal do Pará;pt_BR;Published;13;2009;2009-10-23 16:17:19
11066;Adailton Magalhães Lima;Implantação de Medição no Processo de Desenvolvimento de Software – Relato de Experiência e Lições Aprendidas;Measurement is recognized as an important process to support software development management, but its effective implementation faces challenges related to the effort and knowledge required to generate useful information for decision making. The challenge is even greater when it comes to immature organizations or those without experience with measurements. As a way of contributing to measurement implementation initiatives in this context, this article reports the experience, strategies and results of measurement implementation in a software development project aiming to provide management with information for analysis and action taking.;;Universidade Federal do Pará;pt_BR;Published;13;2009;2009-10-23 16:17:19
11221;Regiane Kowalek Hanusiak;Identificação da Autoria de Manuscritos com Base em Atributos Genéticos e Genéricos da Escrita;Graphoscopy is an area of ​​forensic science dedicated, among other purposes, to the analysis and identification of the authorship of manuscripts, whether contemporary or ancient. As it is an area where subjectivity predominates in the use of expert techniques or graphometry, it has become a field of interest in computing, in the search for solutions for the standardization and assistance of such procedures. In this context, this article presents a method for analyzing and identifying the authorship of manuscripts, based on the genetic and generic attributes of writing, fundamental components for identifying the authorship of manuscripts. The proposed method uses a global approach to extract graphoscopic and verification attributes.;;Pontifícia Universidade Católica do Paraná;pt_BR;Published;16;2009;2009-11-03 21:02:55
11221;Edson José Rodrigues Justino;Identificação da Autoria de Manuscritos com Base em Atributos Genéticos e Genéricos da Escrita;Graphoscopy is an area of ​​forensic science dedicated, among other purposes, to the analysis and identification of the authorship of manuscripts, whether contemporary or ancient. As it is an area where subjectivity predominates in the use of expert techniques or graphometry, it has become a field of interest in computing, in the search for solutions for the standardization and assistance of such procedures. In this context, this article presents a method for analyzing and identifying the authorship of manuscripts, based on the genetic and generic attributes of writing, fundamental components for identifying the authorship of manuscripts. The proposed method uses a global approach to extract graphoscopic and verification attributes.;;Pontifícia Universidade Católica do Paraná;pt_BR;Published;16;2009;2009-11-03 21:02:55
11221;Luiz Soares Oliveira;Identificação da Autoria de Manuscritos com Base em Atributos Genéticos e Genéricos da Escrita;Graphoscopy is an area of ​​forensic science dedicated, among other purposes, to the analysis and identification of the authorship of manuscripts, whether contemporary or ancient. As it is an area where subjectivity predominates in the use of expert techniques or graphometry, it has become a field of interest in computing, in the search for solutions for the standardization and assistance of such procedures. In this context, this article presents a method for analyzing and identifying the authorship of manuscripts, based on the genetic and generic attributes of writing, fundamental components for identifying the authorship of manuscripts. The proposed method uses a global approach to extract graphoscopic and verification attributes.;;Universidade Federal do Paraná;pt_BR;Published;16;2009;2009-11-03 21:02:55
11221;Robert Sabourin;Identificação da Autoria de Manuscritos com Base em Atributos Genéticos e Genéricos da Escrita;Graphoscopy is an area of ​​forensic science dedicated, among other purposes, to the analysis and identification of the authorship of manuscripts, whether contemporary or ancient. As it is an area where subjectivity predominates in the use of expert techniques or graphometry, it has become a field of interest in computing, in the search for solutions for the standardization and assistance of such procedures. In this context, this article presents a method for analyzing and identifying the authorship of manuscripts, based on the genetic and generic attributes of writing, fundamental components for identifying the authorship of manuscripts. The proposed method uses a global approach to extract graphoscopic and verification attributes.;;Ecole de Technologie Superieure, Université Du Quebec;pt_BR;Published;16;2009;2009-11-03 21:02:55
11296;Marcello Kera;Ambiente Virtual Interativo com Colisão e Deformação de Objetos para Treinamento Médico;Training in medical procedures can benefit from the use of interactive virtual environments that realistically simulate the user's actions. The simulation must emit quick responses regarding the encounter of objects, deformation, restriction of movement or even producing forces and vibrations. This work describes a methodology for creating a virtual environment for medical training. Classes and methods are designed and implemented in the environment using the Java programming language. Object collision and deformation methods are used to incorporate realism into the scene, with items being complex and dependent on the interaction information monitored in the virtual environment. The modeled objects are represented by polygonal meshes. Collision detection between objects is based on hierarchical subdivision of space with octrees and face detection. The mass-spring deformation technique is used to simulate the change in the shape of colliding objects. Experiments are carried out to demonstrate the prototype's functionalities.;;Universidade Federal do Paraná;pt_BR;Published;28;2009;2009-11-08 21:01:49
11296;Helio Pedrini;Ambiente Virtual Interativo com Colisão e Deformação de Objetos para Treinamento Médico;Training in medical procedures can benefit from the use of interactive virtual environments that realistically simulate the user's actions. The simulation must emit quick responses regarding the encounter of objects, deformation, restriction of movement or even producing forces and vibrations. This work describes a methodology for creating a virtual environment for medical training. Classes and methods are designed and implemented in the environment using the Java programming language. Object collision and deformation methods are used to incorporate realism into the scene, with items being complex and dependent on the interaction information monitored in the virtual environment. The modeled objects are represented by polygonal meshes. Collision detection between objects is based on hierarchical subdivision of space with octrees and face detection. The mass-spring deformation technique is used to simulate the change in the shape of colliding objects. Experiments are carried out to demonstrate the prototype's functionalities.;;Universidade Estadual de Campinas;pt_BR;Published;28;2009;2009-11-08 21:01:49
11296;Fátima Nunes;Ambiente Virtual Interativo com Colisão e Deformação de Objetos para Treinamento Médico;Training in medical procedures can benefit from the use of interactive virtual environments that realistically simulate the user's actions. The simulation must emit quick responses regarding the encounter of objects, deformation, restriction of movement or even producing forces and vibrations. This work describes a methodology for creating a virtual environment for medical training. Classes and methods are designed and implemented in the environment using the Java programming language. Object collision and deformation methods are used to incorporate realism into the scene, with items being complex and dependent on the interaction information monitored in the virtual environment. The modeled objects are represented by polygonal meshes. Collision detection between objects is based on hierarchical subdivision of space with octrees and face detection. The mass-spring deformation technique is used to simulate the change in the shape of colliding objects. Experiments are carried out to demonstrate the prototype's functionalities.;;Universidade de São Paulo;pt_BR;Published;28;2009;2009-11-08 21:01:49
11384;Marcos Camponogara;QOC*: utilizando Design Rationale como ferramenta para gerenciar conhecimento em projetos de software;During the development process of a software system, a large amount of knowledge is used and produced as a result of the options analyzed and the decisions made throughout the development of the project. This knowledge is valuable, as it reflects the reasons behind decisions, which makes it easier to understand the direction of the project and provides a global view of it. Therefore, there is a need to find alternatives to organize and maintain this type of knowledge and then turn it into a resource that can facilitate the continuity of software projects or the maintenance of developed systems. In this sense, this article presents research based on Knowledge Management and Design Rationale, which proposes a way of representing and maintaining the reasons that motivated the taking of certain decisions in software projects, considering, for this, a representation based on questioning and in the discussion regarding the best options to address issues that arise during the development of a project.;;PPGCC/PUCRS, Avenida Ipiranga, 6681, Prédio 32, Porto Alegre, RS;pt_BR;Published;14;2009;2009-11-16 9:57:01
11384;Milene Silveira;QOC*: utilizando Design Rationale como ferramenta para gerenciar conhecimento em projetos de software;During the development process of a software system, a large amount of knowledge is used and produced as a result of the options analyzed and the decisions made throughout the development of the project. This knowledge is valuable, as it reflects the reasons behind decisions, which makes it easier to understand the direction of the project and provides a global view of it. Therefore, there is a need to find alternatives to organize and maintain this type of knowledge and then turn it into a resource that can facilitate the continuity of software projects or the maintenance of developed systems. In this sense, this article presents research based on Knowledge Management and Design Rationale, which proposes a way of representing and maintaining the reasons that motivated the taking of certain decisions in software projects, considering, for this, a representation based on questioning and in the discussion regarding the best options to address issues that arise during the development of a project.;;PPGCC/PUCRS, Avenida Ipiranga, 6681, Prédio 32, Porto Alegre, RS;pt_BR;Published;14;2009;2009-11-16 9:57:01
11467;Eliane Colepícolo;Uso da ferramenta PreText para mineração de textos extraídos do NCBI para estudo epistemológico da Informática em Saúde;This article presents the use of the PreText tool as an auxiliary technique for research on the epistemology of Health Informatics (IS), which aims to infer whether IS is characterized as science, technology, technoscience or art. PreText aims to pre-process texts, transforming them into a structured format, using the bag-of-words approach, and was applied to the metadata of 437,289 abstracts of scientific articles extracted from the PubMed Central database. The processing results were exported to a database and related to a collection of terms from a thesaurus specialized in IS built by the authors, called EpistemIS, and to the articles' metadata for generating statistics. Such relationships made it possible to understand the epistemology of SI, inferring that this is an interdisciplinary technoscience that operates in the domains of Life Sciences, Health Sciences and Health Care.;;DIS/UNIFESP;pt_BR;Published;15;2009;2009-11-24 17:25:49
11467;Edson Takashi Matsubara;Uso da ferramenta PreText para mineração de textos extraídos do NCBI para estudo epistemológico da Informática em Saúde;This article presents the use of the PreText tool as an auxiliary technique for research on the epistemology of Health Informatics (IS), which aims to infer whether IS is characterized as science, technology, technoscience or art. PreText aims to pre-process texts, transforming them into a structured format, using the bag-of-words approach, and was applied to the metadata of 437,289 abstracts of scientific articles extracted from the PubMed Central database. The processing results were exported to a database and related to a collection of terms from a thesaurus specialized in IS built by the authors, called EpistemIS, and to the articles' metadata for generating statistics. Such relationships made it possible to understand the epistemology of SI, inferring that this is an interdisciplinary technoscience that operates in the domains of Life Sciences, Health Sciences and Health Care.;;ICMC/USP;pt_BR;Published;15;2009;2009-11-24 17:25:49
11467;Alex Esteves Jaccoud Falcão;Uso da ferramenta PreText para mineração de textos extraídos do NCBI para estudo epistemológico da Informática em Saúde;This article presents the use of the PreText tool as an auxiliary technique for research on the epistemology of Health Informatics (IS), which aims to infer whether IS is characterized as science, technology, technoscience or art. PreText aims to pre-process texts, transforming them into a structured format, using the bag-of-words approach, and was applied to the metadata of 437,289 abstracts of scientific articles extracted from the PubMed Central database. The processing results were exported to a database and related to a collection of terms from a thesaurus specialized in IS built by the authors, called EpistemIS, and to the articles' metadata for generating statistics. Such relationships made it possible to understand the epistemology of SI, inferring that this is an interdisciplinary technoscience that operates in the domains of Life Sciences, Health Sciences and Health Care.;;DIS/UNIFESP;pt_BR;Published;15;2009;2009-11-24 17:25:49
11467;Ivan Torres Pisa;Uso da ferramenta PreText para mineração de textos extraídos do NCBI para estudo epistemológico da Informática em Saúde;This article presents the use of the PreText tool as an auxiliary technique for research on the epistemology of Health Informatics (IS), which aims to infer whether IS is characterized as science, technology, technoscience or art. PreText aims to pre-process texts, transforming them into a structured format, using the bag-of-words approach, and was applied to the metadata of 437,289 abstracts of scientific articles extracted from the PubMed Central database. The processing results were exported to a database and related to a collection of terms from a thesaurus specialized in IS built by the authors, called EpistemIS, and to the articles' metadata for generating statistics. Such relationships made it possible to understand the epistemology of SI, inferring that this is an interdisciplinary technoscience that operates in the domains of Life Sciences, Health Sciences and Health Care.;;DIS/UNIFESP;pt_BR;Published;15;2009;2009-11-24 17:25:49
11468;Bruno Nogueira;ALUPAS: Avaliação de desempenho e consumo de energia de softwares para sistemas embarcados;With the proliferation of portable battery-operated equipment, the design of low-power embedded systems has aroused much interest in recent years. To meet low energy consumption requirements, it is essential, even in the initial phases of development, to have mechanisms that help quickly and accurately analyze possible design alternatives. This work presents ALUPAS, a stochastic simulator based on Colored Petri Nets (CPN) to estimate the performance and energy consumption of software for embedded systems. Experimental results show an accuracy, on average, of 94% using the proposed simulator compared to real values ​​measured in the hardware.;;Universidade Federal de Pernambuco, UFPE;pt_BR;Published;19;2009;2009-11-24 17:35:08
11468;Paulo Maciel;ALUPAS: Avaliação de desempenho e consumo de energia de softwares para sistemas embarcados;With the proliferation of portable battery-operated equipment, the design of low-power embedded systems has aroused much interest in recent years. To meet low energy consumption requirements, it is essential, even in the initial phases of development, to have mechanisms that help quickly and accurately analyze possible design alternatives. This work presents ALUPAS, a stochastic simulator based on Colored Petri Nets (CPN) to estimate the performance and energy consumption of software for embedded systems. Experimental results show an accuracy, on average, of 94% using the proposed simulator compared to real values ​​measured in the hardware.;;Universidade Federal de Pernambuco, UFPE;pt_BR;Published;19;2009;2009-11-24 17:35:08
11468;Gustavo Callou;ALUPAS: Avaliação de desempenho e consumo de energia de softwares para sistemas embarcados;With the proliferation of portable battery-operated equipment, the design of low-power embedded systems has aroused much interest in recent years. To meet low energy consumption requirements, it is essential, even in the initial phases of development, to have mechanisms that help quickly and accurately analyze possible design alternatives. This work presents ALUPAS, a stochastic simulator based on Colored Petri Nets (CPN) to estimate the performance and energy consumption of software for embedded systems. Experimental results show an accuracy, on average, of 94% using the proposed simulator compared to real values ​​measured in the hardware.;;Universidade Federal de Pernambuco, UFPE;pt_BR;Published;19;2009;2009-11-24 17:35:08
11468;Ermeson Andrade;ALUPAS: Avaliação de desempenho e consumo de energia de softwares para sistemas embarcados;With the proliferation of portable battery-operated equipment, the design of low-power embedded systems has aroused much interest in recent years. To meet low energy consumption requirements, it is essential, even in the initial phases of development, to have mechanisms that help quickly and accurately analyze possible design alternatives. This work presents ALUPAS, a stochastic simulator based on Colored Petri Nets (CPN) to estimate the performance and energy consumption of software for embedded systems. Experimental results show an accuracy, on average, of 94% using the proposed simulator compared to real values ​​measured in the hardware.;;Universidade Federal de Pernambuco, UFPE;pt_BR;Published;19;2009;2009-11-24 17:35:08
11468;Eduardo Tavares;ALUPAS: Avaliação de desempenho e consumo de energia de softwares para sistemas embarcados;With the proliferation of portable battery-operated equipment, the design of low-power embedded systems has aroused much interest in recent years. To meet low energy consumption requirements, it is essential, even in the initial phases of development, to have mechanisms that help quickly and accurately analyze possible design alternatives. This work presents ALUPAS, a stochastic simulator based on Colored Petri Nets (CPN) to estimate the performance and energy consumption of software for embedded systems. Experimental results show an accuracy, on average, of 94% using the proposed simulator compared to real values ​​measured in the hardware.;;Universidade Federal de Pernambuco, UFPE;pt_BR;Published;19;2009;2009-11-24 17:35:08
11469;André Luiz Pimentel Queiroz;Avaliação Pró-ativa da Deterioração de Sistemas de Informação por meio de Medidas de Gestão;Issues related to the modernization of information systems have received greater attention and relevance. Many methods have been proposed to support an organization in choosing the best alternative for modernizing a system. However, these methods do not provide the necessary information for early identification of application deterioration, and are used when it has already been decided to take some action to modernize the application, as a reactive way of dealing with the problem. This article presents a measurement-based approach with the aim of continuously monitoring the deterioration of a system, allowing the manager to take adjustment actions as early as possible. Practical applications in different systems of a large company show the feasibility of collecting measurements and indicate deteriorating systems.;;Universidade Católica de Brasília;pt_BR;Published;23;2009;2009-11-24 17:40:31
11469;Nicolas Anquetil;Avaliação Pró-ativa da Deterioração de Sistemas de Informação por meio de Medidas de Gestão;Issues related to the modernization of information systems have received greater attention and relevance. Many methods have been proposed to support an organization in choosing the best alternative for modernizing a system. However, these methods do not provide the necessary information for early identification of application deterioration, and are used when it has already been decided to take some action to modernize the application, as a reactive way of dealing with the problem. This article presents a measurement-based approach with the aim of continuously monitoring the deterioration of a system, allowing the manager to take adjustment actions as early as possible. Practical applications in different systems of a large company show the feasibility of collecting measurements and indicate deteriorating systems.;;Universidade Católica de Brasília;pt_BR;Published;23;2009;2009-11-24 17:40:31
11469;Káthia Marçal de Oliveira;Avaliação Pró-ativa da Deterioração de Sistemas de Informação por meio de Medidas de Gestão;Issues related to the modernization of information systems have received greater attention and relevance. Many methods have been proposed to support an organization in choosing the best alternative for modernizing a system. However, these methods do not provide the necessary information for early identification of application deterioration, and are used when it has already been decided to take some action to modernize the application, as a reactive way of dealing with the problem. This article presents a measurement-based approach with the aim of continuously monitoring the deterioration of a system, allowing the manager to take adjustment actions as early as possible. Practical applications in different systems of a large company show the feasibility of collecting measurements and indicate deteriorating systems.;;Universidade Católica de Brasília;pt_BR;Published;23;2009;2009-11-24 17:40:31
11471;Thiago S. M. C. Farias;High Performance Computing: CUDA as a Supporting Technology for Next Generation Augmented Reality Applications;The main purpose of this survey is presenting the potential ofGPGPU technology for real time markerless augmented reality related processing. CUDA is a GPGPU technology developed by NVIDIA that allows programmers to use the C programming language to code algorithms for execution on the GPU. Applications that require mathematically intensive computation of large amounts of data are ideal targets for GPU computing. In this survey, CUDA architecture will be depicted, together with an optimized programming model for obtaining better results using the parallel approach. A case study, mainly related to tracking algorithms, will also be shown in order to demonstrate the performance improvement in comparison to sequential approaches.;;Centro de Informática, UFPE;pt_BR;Published;25;2009;2009-11-24 17:53:30
11471;João Marcelo N. X. Teixeira;High Performance Computing: CUDA as a Supporting Technology for Next Generation Augmented Reality Applications;The main purpose of this survey is presenting the potential ofGPGPU technology for real time markerless augmented reality related processing. CUDA is a GPGPU technology developed by NVIDIA that allows programmers to use the C programming language to code algorithms for execution on the GPU. Applications that require mathematically intensive computation of large amounts of data are ideal targets for GPU computing. In this survey, CUDA architecture will be depicted, together with an optimized programming model for obtaining better results using the parallel approach. A case study, mainly related to tracking algorithms, will also be shown in order to demonstrate the performance improvement in comparison to sequential approaches.;;Centro de Informática, UFPE;pt_BR;Published;25;2009;2009-11-24 17:53:30
11471;Pedro J. S. Leite;High Performance Computing: CUDA as a Supporting Technology for Next Generation Augmented Reality Applications;The main purpose of this survey is presenting the potential ofGPGPU technology for real time markerless augmented reality related processing. CUDA is a GPGPU technology developed by NVIDIA that allows programmers to use the C programming language to code algorithms for execution on the GPU. Applications that require mathematically intensive computation of large amounts of data are ideal targets for GPU computing. In this survey, CUDA architecture will be depicted, together with an optimized programming model for obtaining better results using the parallel approach. A case study, mainly related to tracking algorithms, will also be shown in order to demonstrate the performance improvement in comparison to sequential approaches.;;Centro de Informática, UFPE;pt_BR;Published;25;2009;2009-11-24 17:53:30
11471;Gabriel F. Almeida;High Performance Computing: CUDA as a Supporting Technology for Next Generation Augmented Reality Applications;The main purpose of this survey is presenting the potential ofGPGPU technology for real time markerless augmented reality related processing. CUDA is a GPGPU technology developed by NVIDIA that allows programmers to use the C programming language to code algorithms for execution on the GPU. Applications that require mathematically intensive computation of large amounts of data are ideal targets for GPU computing. In this survey, CUDA architecture will be depicted, together with an optimized programming model for obtaining better results using the parallel approach. A case study, mainly related to tracking algorithms, will also be shown in order to demonstrate the performance improvement in comparison to sequential approaches.;;Centro de Informática, UFPE;pt_BR;Published;25;2009;2009-11-24 17:53:30
11471;Mozart W. S. Almeida;High Performance Computing: CUDA as a Supporting Technology for Next Generation Augmented Reality Applications;The main purpose of this survey is presenting the potential ofGPGPU technology for real time markerless augmented reality related processing. CUDA is a GPGPU technology developed by NVIDIA that allows programmers to use the C programming language to code algorithms for execution on the GPU. Applications that require mathematically intensive computation of large amounts of data are ideal targets for GPU computing. In this survey, CUDA architecture will be depicted, together with an optimized programming model for obtaining better results using the parallel approach. A case study, mainly related to tracking algorithms, will also be shown in order to demonstrate the performance improvement in comparison to sequential approaches.;;Centro de Informática, UFPE;pt_BR;Published;25;2009;2009-11-24 17:53:30
11471;Veronica Teichrieb;High Performance Computing: CUDA as a Supporting Technology for Next Generation Augmented Reality Applications;The main purpose of this survey is presenting the potential ofGPGPU technology for real time markerless augmented reality related processing. CUDA is a GPGPU technology developed by NVIDIA that allows programmers to use the C programming language to code algorithms for execution on the GPU. Applications that require mathematically intensive computation of large amounts of data are ideal targets for GPU computing. In this survey, CUDA architecture will be depicted, together with an optimized programming model for obtaining better results using the parallel approach. A case study, mainly related to tracking algorithms, will also be shown in order to demonstrate the performance improvement in comparison to sequential approaches.;;Centro de Informática, UFPE;pt_BR;Published;25;2009;2009-11-24 17:53:30
11471;Judith Kelner;High Performance Computing: CUDA as a Supporting Technology for Next Generation Augmented Reality Applications;The main purpose of this survey is presenting the potential ofGPGPU technology for real time markerless augmented reality related processing. CUDA is a GPGPU technology developed by NVIDIA that allows programmers to use the C programming language to code algorithms for execution on the GPU. Applications that require mathematically intensive computation of large amounts of data are ideal targets for GPU computing. In this survey, CUDA architecture will be depicted, together with an optimized programming model for obtaining better results using the parallel approach. A case study, mainly related to tracking algorithms, will also be shown in order to demonstrate the performance improvement in comparison to sequential approaches.;;Centro de Informática, UFPE;pt_BR;Published;25;2009;2009-11-24 17:53:30
11473;João Victor Boechat Gomide;Efeitos Visuais, uma Abordagem a Partir do Processamento Digital de Imagens;This article introduces the most important concepts of digital imaging and discusses how they are used to create visual effects, and, at the same time, provides a panel of the various computer graphics and image processing techniques for cinema and video. Visual effects production makes extensive use of geometric modeling, computer visualization, digital image processing and, more recently, computer vision. The text is divided into five sections. After the introduction, the evolution of the different optical trickery techniques is discussed. Next, the digital image is presented, both in its transformation, when captured on film, and in originally digital formats, the concepts of compression and image description and the possible results with the different formats. In the fourth section, some digital visual effects techniques are discussed, such as color correction, image tracking and motion capture, as well as the available hardware and software and the concepts used in them. Visual effects created on TV Globo by one of the authors are also presented, at each stage of its production. In the last section, perspectives for academic research and smaller productions are discussed.;;Faculdade de Ciências Empresariais, Universidade FUMEC;pt_BR;Published;27;2009;2009-11-24 18:01:35
11473;Arnaldo de Albuquerque Araújo;Efeitos Visuais, uma Abordagem a Partir do Processamento Digital de Imagens;This article introduces the most important concepts of digital imaging and discusses how they are used to create visual effects, and, at the same time, provides a panel of the various computer graphics and image processing techniques for cinema and video. Visual effects production makes extensive use of geometric modeling, computer visualization, digital image processing and, more recently, computer vision. The text is divided into five sections. After the introduction, the evolution of the different optical trickery techniques is discussed. Next, the digital image is presented, both in its transformation, when captured on film, and in originally digital formats, the concepts of compression and image description and the possible results with the different formats. In the fourth section, some digital visual effects techniques are discussed, such as color correction, image tracking and motion capture, as well as the available hardware and software and the concepts used in them. Visual effects created on TV Globo by one of the authors are also presented, at each stage of its production. In the last section, perspectives for academic research and smaller productions are discussed.;;Departamento de Ciência da Computação, UFMG;pt_BR;Published;27;2009;2009-11-24 18:01:35
11477;Maurício Marengoni;Tutorial: Introdução à Visão Computacional usando OpenCV;This tutorial introduces introductory concepts of image processing (filters) and computer vision (segmentation, classification, pattern recognition and tracking). These concepts will be introduced using the OpenCV library, which is distributed free of charge and has extensive documentation on the internet, with examples and practical applications. It will be shown how to obtain and install the tool for different types of platforms and development languages. The concepts of image processing and computer vision will be discussed not only from a theoretical perspective, but implementation examples will also be presented so that readers can understand and use the examples presented in this tutorial.;;Universidade Presbiteriana Mackenzie, Faculdade de Computação e Informática e Pós Graduação em Engenharia Elétrica;pt_BR;Published;35;2009;2009-11-24 18:12:41
11477;Stringhini Stringhini;Tutorial: Introdução à Visão Computacional usando OpenCV;This tutorial introduces introductory concepts of image processing (filters) and computer vision (segmentation, classification, pattern recognition and tracking). These concepts will be introduced using the OpenCV library, which is distributed free of charge and has extensive documentation on the internet, with examples and practical applications. It will be shown how to obtain and install the tool for different types of platforms and development languages. The concepts of image processing and computer vision will be discussed not only from a theoretical perspective, but implementation examples will also be presented so that readers can understand and use the examples presented in this tutorial.;;Universidade Presbiteriana Mackenzie, Faculdade de Computação e Informática;pt_BR;Published;35;2009;2009-11-24 18:12:41
11607;Marcelo da Silva Hounsell;Análise do Controle Motor com Interações usando Dispositivos Convencionais e Realidade Aumentada;This work presents software to aid in the diagnosis of fine motor control problems in humans, called MOSKA (MOtor SKill Analyzer), using interaction techniques with conventional devices (based on devices such as mouse, graphics tablet - tablet) and Augmented Reality ( AR) (based on low-cost devices such as webcam). Both interaction techniques, with AR and without AR, were implemented with the aim of capturing metrics, already validated in previous research, regarding the duration and accuracy of drawings produced by patients. The development led to a system that has new and more significant metrics, a Database system and allowed a comparison of interaction techniques for this type of diagnosis. The implementations were made in Java and, for AR, the jARToolKit library was used as it is popular, free and open source but involves the use of markers, which led to the identification of some limitations of this type of AR. Despite some enthusiasm for being a more modern technology, it has been discovered that certain specific AR techniques are not appropriate for applications that require precision and visual-motor coordination such as fine motor control analysis.;;LARVA - DCC - UDESC;pt_BR;Published;19;2009;2009-12-08 20:48:00
11607;Débora Cristine Xavier;Análise do Controle Motor com Interações usando Dispositivos Convencionais e Realidade Aumentada;This work presents software to aid in the diagnosis of fine motor control problems in humans, called MOSKA (MOtor SKill Analyzer), using interaction techniques with conventional devices (based on devices such as mouse, graphics tablet - tablet) and Augmented Reality ( AR) (based on low-cost devices such as webcam). Both interaction techniques, with AR and without AR, were implemented with the aim of capturing metrics, already validated in previous research, regarding the duration and accuracy of drawings produced by patients. The development led to a system that has new and more significant metrics, a Database system and allowed a comparison of interaction techniques for this type of diagnosis. The implementations were made in Java and, for AR, the jARToolKit library was used as it is popular, free and open source but involves the use of markers, which led to the identification of some limitations of this type of AR. Despite some enthusiasm for being a more modern technology, it has been discovered that certain specific AR techniques are not appropriate for applications that require precision and visual-motor coordination such as fine motor control analysis.;;LARVA - DCC - UDESC;pt_BR;Published;19;2009;2009-12-08 20:48:00
11607;Tânia Brusque Crocetta;Análise do Controle Motor com Interações usando Dispositivos Convencionais e Realidade Aumentada;This work presents software to aid in the diagnosis of fine motor control problems in humans, called MOSKA (MOtor SKill Analyzer), using interaction techniques with conventional devices (based on devices such as mouse, graphics tablet - tablet) and Augmented Reality ( AR) (based on low-cost devices such as webcam). Both interaction techniques, with AR and without AR, were implemented with the aim of capturing metrics, already validated in previous research, regarding the duration and accuracy of drawings produced by patients. The development led to a system that has new and more significant metrics, a Database system and allowed a comparison of interaction techniques for this type of diagnosis. The implementations were made in Java and, for AR, the jARToolKit library was used as it is popular, free and open source but involves the use of markers, which led to the identification of some limitations of this type of AR. Despite some enthusiasm for being a more modern technology, it has been discovered that certain specific AR techniques are not appropriate for applications that require precision and visual-motor coordination such as fine motor control analysis.;;LAPE - CEFID - UDESC;pt_BR;Published;19;2009;2009-12-08 20:48:00
11607;Alexandro Andrade;Análise do Controle Motor com Interações usando Dispositivos Convencionais e Realidade Aumentada;This work presents software to aid in the diagnosis of fine motor control problems in humans, called MOSKA (MOtor SKill Analyzer), using interaction techniques with conventional devices (based on devices such as mouse, graphics tablet - tablet) and Augmented Reality ( AR) (based on low-cost devices such as webcam). Both interaction techniques, with AR and without AR, were implemented with the aim of capturing metrics, already validated in previous research, regarding the duration and accuracy of drawings produced by patients. The development led to a system that has new and more significant metrics, a Database system and allowed a comparison of interaction techniques for this type of diagnosis. The implementations were made in Java and, for AR, the jARToolKit library was used as it is popular, free and open source but involves the use of markers, which led to the identification of some limitations of this type of AR. Despite some enthusiasm for being a more modern technology, it has been discovered that certain specific AR techniques are not appropriate for applications that require precision and visual-motor coordination such as fine motor control analysis.;;LAPE - CEFID - UDESC;pt_BR;Published;19;2009;2009-12-08 20:48:00
11631;Rodrigo Fernandes de Mello;Variation Detection applied in User Signature Verification;Behavior studies have been conducted by scientists and philosophers who approach subjects such as star and planet trajectories, society organizations, living beings evolution and human language. With the advent of computer, new challenges have been observed in order to explore and understand the behavior variations of interactions with systems. Motivated by those challenges, this work proposes a new approach to automatically cluster, detect and identify behavior patterns. In order to validate this approach, we have modeled the knowledge embedded in interactions of handwriting signatures. The generated knowledge models were, afterwards, employed to verify signatures. Obtained results were compared to other related approaches presented in SVC2004, the First International Signature Verification Competition.;;Universidade de São Paulo;pt_BR;Published;21;2009;2009-12-14 11:33:27
11631;Matheus Lorenzo dos Santos;Variation Detection applied in User Signature Verification;Behavior studies have been conducted by scientists and philosophers who approach subjects such as star and planet trajectories, society organizations, living beings evolution and human language. With the advent of computer, new challenges have been observed in order to explore and understand the behavior variations of interactions with systems. Motivated by those challenges, this work proposes a new approach to automatically cluster, detect and identify behavior patterns. In order to validate this approach, we have modeled the knowledge embedded in interactions of handwriting signatures. The generated knowledge models were, afterwards, employed to verify signatures. Obtained results were compared to other related approaches presented in SVC2004, the First International Signature Verification Competition.;;Universidade de São Paulo;pt_BR;Published;21;2009;2009-12-14 11:33:27
11631;Marcelo Keese Albertini;Variation Detection applied in User Signature Verification;Behavior studies have been conducted by scientists and philosophers who approach subjects such as star and planet trajectories, society organizations, living beings evolution and human language. With the advent of computer, new challenges have been observed in order to explore and understand the behavior variations of interactions with systems. Motivated by those challenges, this work proposes a new approach to automatically cluster, detect and identify behavior patterns. In order to validate this approach, we have modeled the knowledge embedded in interactions of handwriting signatures. The generated knowledge models were, afterwards, employed to verify signatures. Obtained results were compared to other related approaches presented in SVC2004, the First International Signature Verification Competition.;;Universidade de São Paulo;pt_BR;Published;21;2009;2009-12-14 11:33:27
11662;Milton Roberto Heinen;Aprendizado e Controle de Robôs Móveis Autônomos Utilizando Atenção Visual;This paper describes a reinforcement learning model capable of learning complex control tasks using continuous actions and states. This model, which is based on the continuous actor-critic, uses networks of normalized radial basis functions to learn the value of states and actions, being able to configure the structure of these networks automatically during learning. Furthermore, a selective visual attention mechanism is used to perceive the environment and states. To validate the proposed model, a relatively complex task for reinforcement learning algorithms was used: driving a ball to the goal in a simulated robot football environment. The experiments carried out demonstrate that the proposed model is capable of carrying out the task in question quite successfully using only visual information.;;UFRGS;pt_BR;Published;14;2009;2009-12-21 15:48:41
11662;Paulo Martins Engel;Aprendizado e Controle de Robôs Móveis Autônomos Utilizando Atenção Visual;This paper describes a reinforcement learning model capable of learning complex control tasks using continuous actions and states. This model, which is based on the continuous actor-critic, uses networks of normalized radial basis functions to learn the value of states and actions, being able to configure the structure of these networks automatically during learning. Furthermore, a selective visual attention mechanism is used to perceive the environment and states. To validate the proposed model, a relatively complex task for reinforcement learning algorithms was used: driving a ball to the goal in a simulated robot football environment. The experiments carried out demonstrate that the proposed model is capable of carrying out the task in question quite successfully using only visual information.;;UFRGS;pt_BR;Published;14;2009;2009-12-21 15:48:41
11695;Richardson Ribeiro;Estratégias de Atualização de Políticas para a Coordenação de Agentes Baseados em Enxames;In this article, the influence of learning parameters of swarm algorithms is analyzed and strategies for updating policies generated by rewards (pheromones) for dynamic environments are proposed. We found that when the parameters of reward-based algorithms are inadequately adjusted, delays in learning and convergence to a non-satisfactory solution can occur. Furthermore, this problem is aggravated in dynamic environments, as adjusting the parameters of such algorithms is not sufficient to guarantee convergence. To solve this problem, we developed strategies that modify pheromone values, improving coordination between agents and allowing convergence even when the environment is dynamically changed. For this, a framework capable of iteratively demonstrating the influence of parameters and strategies was developed. Experimental results show that it is possible to accelerate the convergence to a consistent global policy, surpassing the results of classical swarm-based algorithmic approaches.;;Pontifícia Universidade Católica do Paraná - PUCPR;pt_BR;Published;24;2009;2009-12-21 15:11:23
11695;Fabrício Enembreck;Estratégias de Atualização de Políticas para a Coordenação de Agentes Baseados em Enxames;In this article, the influence of learning parameters of swarm algorithms is analyzed and strategies for updating policies generated by rewards (pheromones) for dynamic environments are proposed. We found that when the parameters of reward-based algorithms are inadequately adjusted, delays in learning and convergence to a non-satisfactory solution can occur. Furthermore, this problem is aggravated in dynamic environments, as adjusting the parameters of such algorithms is not sufficient to guarantee convergence. To solve this problem, we developed strategies that modify pheromone values, improving coordination between agents and allowing convergence even when the environment is dynamically changed. For this, a framework capable of iteratively demonstrating the influence of parameters and strategies was developed. Experimental results show that it is possible to accelerate the convergence to a consistent global policy, surpassing the results of classical swarm-based algorithmic approaches.;;Pontifícia Universidade Católica do Paraná;pt_BR;Published;24;2009;2009-12-21 15:11:23
12104;Flávio Henrique Teles Vieira;Estimação de Probabilidade de Perda de Dados em Redes Através de Modelagem Multifractal de Tráfego e Teoria de Muitas Fontes;In this article, we propose an approach for estimating the probability of byte loss in computer network links considering multifractal properties of traffic flows. More specifically, we derive a mathematical expression for calculating the loss probability for servers with finite buffers fed with multifractal traffic flows. The proposed approach is based on the many-source theory and multifractal traffic modeling based on multiplicative cascades. Finally, we evaluate the proposed loss probability estimation through computer simulations using real traffic series, thus verifying its efficiency as a tool related to the provision of quality of service in computer networks.;;Universidade Federal de Goiás;pt_BR;Published;17;2010;2010-02-19 13:38:29
12104;Scheila Guedes Garcez;Estimação de Probabilidade de Perda de Dados em Redes Através de Modelagem Multifractal de Tráfego e Teoria de Muitas Fontes;In this article, we propose an approach for estimating the probability of byte loss in computer network links considering multifractal properties of traffic flows. More specifically, we derive a mathematical expression for calculating the loss probability for servers with finite buffers fed with multifractal traffic flows. The proposed approach is based on the many-source theory and multifractal traffic modeling based on multiplicative cascades. Finally, we evaluate the proposed loss probability estimation through computer simulations using real traffic series, thus verifying its efficiency as a tool related to the provision of quality of service in computer networks.;;Unicamp;pt_BR;Published;17;2010;2010-02-19 13:38:29
12119;Josceli Maria Tenório;Desenvolvimento e Avaliação de um Protocolo Eletrônico para Atendimento e Monitoramento do Paciente com Doença Celíaca;This study aims to develop an electronic protocol with structured and specific information for the care and monitoring of patients with celiac disease, available on the web, and evaluate it in terms of usability. The structuring of clinical data was done by analyzing consultation forms, medical records and a clinical protocol. A protocol for care during the consultation for clinical diagnosis purposes was designed and the web computing system was modeled and built to support the implementation of this protocol. To evaluate the usability of the web system, the System Usability Scale (SUS) questionnaire was used. The SUS-score resulted in an average value of 83.5±10.0, which indicates that the web system was considered easy to use and in accordance with user satisfaction. The web system properly implemented the protocol. The use of the electronic protocol proved to be valid for the care and monitoring of patients with celiac disease, as it maintained the specificity of clinical data and the routine of the professionals involved.;;Unifesp;pt_BR;Published;10;2010;2010-02-20 15:20:50
12119;Frederico Molina Cohrs;Desenvolvimento e Avaliação de um Protocolo Eletrônico para Atendimento e Monitoramento do Paciente com Doença Celíaca;This study aims to develop an electronic protocol with structured and specific information for the care and monitoring of patients with celiac disease, available on the web, and evaluate it in terms of usability. The structuring of clinical data was done by analyzing consultation forms, medical records and a clinical protocol. A protocol for care during the consultation for clinical diagnosis purposes was designed and the web computing system was modeled and built to support the implementation of this protocol. To evaluate the usability of the web system, the System Usability Scale (SUS) questionnaire was used. The SUS-score resulted in an average value of 83.5±10.0, which indicates that the web system was considered easy to use and in accordance with user satisfaction. The web system properly implemented the protocol. The use of the electronic protocol proved to be valid for the care and monitoring of patients with celiac disease, as it maintained the specificity of clinical data and the routine of the professionals involved.;;Programa de Pós Graduação em Saúde Coletiva, Universidade Federal de São Paulo;pt_BR;Published;10;2010;2010-02-20 15:20:50
12119;Vera Lúcia Sdepanian;Desenvolvimento e Avaliação de um Protocolo Eletrônico para Atendimento e Monitoramento do Paciente com Doença Celíaca;This study aims to develop an electronic protocol with structured and specific information for the care and monitoring of patients with celiac disease, available on the web, and evaluate it in terms of usability. The structuring of clinical data was done by analyzing consultation forms, medical records and a clinical protocol. A protocol for care during the consultation for clinical diagnosis purposes was designed and the web computing system was modeled and built to support the implementation of this protocol. To evaluate the usability of the web system, the System Usability Scale (SUS) questionnaire was used. The SUS-score resulted in an average value of 83.5±10.0, which indicates that the web system was considered easy to use and in accordance with user satisfaction. The web system properly implemented the protocol. The use of the electronic protocol proved to be valid for the care and monitoring of patients with celiac disease, as it maintained the specificity of clinical data and the routine of the professionals involved.;;Departamento de Pediatria, Universidade Federal de São Paulo;pt_BR;Published;10;2010;2010-02-20 15:20:50
12119;Ivan Torres Pisa;Desenvolvimento e Avaliação de um Protocolo Eletrônico para Atendimento e Monitoramento do Paciente com Doença Celíaca;This study aims to develop an electronic protocol with structured and specific information for the care and monitoring of patients with celiac disease, available on the web, and evaluate it in terms of usability. The structuring of clinical data was done by analyzing consultation forms, medical records and a clinical protocol. A protocol for care during the consultation for clinical diagnosis purposes was designed and the web computing system was modeled and built to support the implementation of this protocol. To evaluate the usability of the web system, the System Usability Scale (SUS) questionnaire was used. The SUS-score resulted in an average value of 83.5±10.0, which indicates that the web system was considered easy to use and in accordance with user satisfaction. The web system properly implemented the protocol. The use of the electronic protocol proved to be valid for the care and monitoring of patients with celiac disease, as it maintained the specificity of clinical data and the routine of the professionals involved.;;Departamento de Informática em Saúde, Universidade Federal de São Paulo;pt_BR;Published;10;2010;2010-02-20 15:20:50
12119;Heimar de Fátima Marin;Desenvolvimento e Avaliação de um Protocolo Eletrônico para Atendimento e Monitoramento do Paciente com Doença Celíaca;This study aims to develop an electronic protocol with structured and specific information for the care and monitoring of patients with celiac disease, available on the web, and evaluate it in terms of usability. The structuring of clinical data was done by analyzing consultation forms, medical records and a clinical protocol. A protocol for care during the consultation for clinical diagnosis purposes was designed and the web computing system was modeled and built to support the implementation of this protocol. To evaluate the usability of the web system, the System Usability Scale (SUS) questionnaire was used. The SUS-score resulted in an average value of 83.5±10.0, which indicates that the web system was considered easy to use and in accordance with user satisfaction. The web system properly implemented the protocol. The use of the electronic protocol proved to be valid for the care and monitoring of patients with celiac disease, as it maintained the specificity of clinical data and the routine of the professionals involved.;;Universidade Federalde São Paulo;pt_BR;Published;10;2010;2010-02-20 15:20:50
12327;Gilmário Barbosa Santos;Snake-3D na Modelagem Tridimensional e Medição do Comprimento de Arcos Elétricos;This work describes the unprecedented application of a three-dimensional active contour (snake-3D) to evaluate the length of electrical arcs artificially generated in high voltage towers and captured using a pair of calibrated cameras. Asnake-3D is spatially represented by a B-spline that evolves under the constraints of forces external to the active contour and internal to it. The external forces are obtained from the pairs of images that capture the evolution of the electric arc, while the internal forces are intrinsic to the active contour and determined by the voltage between adjacent control points. In relation to other approaches for recovering the 3D geometry of electric arcs, the model proposed here is independent of the determination of homologous points and a specific positioning of cameras, in addition to providing tracking of the electric arc, taking into account the temporal dependence between consecutive pairs of frames in two videos. For evaluation purposes, experiments were carried out on synthetic images obtained from the simulation of an image acquisition system, as well as experiments with images of real electric arcs produced at CEPEL. The results obtained were considered acceptable, demonstrating the potential of the presented approach.;;UDESC;pt_BR;Published;24;2010;2010-03-05 16:33:03
12327;Clésio Luiz Tozzi;Snake-3D na Modelagem Tridimensional e Medição do Comprimento de Arcos Elétricos;This work describes the unprecedented application of a three-dimensional active contour (snake-3D) to evaluate the length of electrical arcs artificially generated in high voltage towers and captured using a pair of calibrated cameras. Asnake-3D is spatially represented by a B-spline that evolves under the constraints of forces external to the active contour and internal to it. The external forces are obtained from the pairs of images that capture the evolution of the electric arc, while the internal forces are intrinsic to the active contour and determined by the voltage between adjacent control points. In relation to other approaches for recovering the 3D geometry of electric arcs, the model proposed here is independent of the determination of homologous points and a specific positioning of cameras, in addition to providing tracking of the electric arc, taking into account the temporal dependence between consecutive pairs of frames in two videos. For evaluation purposes, experiments were carried out on synthetic images obtained from the simulation of an image acquisition system, as well as experiments with images of real electric arcs produced at CEPEL. The results obtained were considered acceptable, demonstrating the potential of the presented approach.;;UNICAMP;pt_BR;Published;24;2010;2010-03-05 16:33:03
12327;Maria Cristina Tavares;Snake-3D na Modelagem Tridimensional e Medição do Comprimento de Arcos Elétricos;This work describes the unprecedented application of a three-dimensional active contour (snake-3D) to evaluate the length of electrical arcs artificially generated in high voltage towers and captured using a pair of calibrated cameras. Asnake-3D is spatially represented by a B-spline that evolves under the constraints of forces external to the active contour and internal to it. The external forces are obtained from the pairs of images that capture the evolution of the electric arc, while the internal forces are intrinsic to the active contour and determined by the voltage between adjacent control points. In relation to other approaches for recovering the 3D geometry of electric arcs, the model proposed here is independent of the determination of homologous points and a specific positioning of cameras, in addition to providing tracking of the electric arc, taking into account the temporal dependence between consecutive pairs of frames in two videos. For evaluation purposes, experiments were carried out on synthetic images obtained from the simulation of an image acquisition system, as well as experiments with images of real electric arcs produced at CEPEL. The results obtained were considered acceptable, demonstrating the potential of the presented approach.;;UNICAMP;pt_BR;Published;24;2010;2010-03-05 16:33:03
12361;Luis C. Lamb;Editorial: 20 anos da Revista de Informática Teórica e Aplicada;This issue of the Journal of Theoretical and Applied Informatics (RITA) has special significance. This is the first issue published in the year in which the magazine turns 20. For two decades, many worked to build RITA's history. The articles in this issue illustrate that RITA's initial objectives were achieved: we published works across a broad spectrum of Computer Science, allowing researchers from diverse research areas in Brazil and abroad disseminate their results to society.;;Instituto de Informática, UFRGS, Porto Alegre, RS;pt_BR;Published;1;2010;2010-03-08 15:06:56
12361;José Palazzo Moreira de Oliveira;Editorial: 20 anos da Revista de Informática Teórica e Aplicada;This issue of the Journal of Theoretical and Applied Informatics (RITA) has special significance. This is the first issue published in the year in which the magazine turns 20. For two decades, many worked to build RITA's history. The articles in this issue illustrate that RITA's initial objectives were achieved: we published works across a broad spectrum of Computer Science, allowing researchers from diverse research areas in Brazil and abroad disseminate their results to society.;;Instituto de Informática, UFRGS, Porto Alegre, RS;pt_BR;Published;1;2010;2010-03-08 15:06:56
12361;Lisandro Z Granville;Editorial: 20 anos da Revista de Informática Teórica e Aplicada;This issue of the Journal of Theoretical and Applied Informatics (RITA) has special significance. This is the first issue published in the year in which the magazine turns 20. For two decades, many worked to build RITA's history. The articles in this issue illustrate that RITA's initial objectives were achieved: we published works across a broad spectrum of Computer Science, allowing researchers from diverse research areas in Brazil and abroad disseminate their results to society.;;Instituto de Informática, UFRGS, Porto Alegre, RS;pt_BR;Published;1;2010;2010-03-08 15:06:56
12362;Siome Klein Goldenstein;Prefácio à Seção Especial dos Tutoriais do SIBGRAPI'08;Welcome to the special section of SIBGRAPI 2008 tutorials! SIBGRAPI 2008 participants had the opportunity to witness an exceptional set of tutorials that covered a wide range of topics of interest to the community. We had two introductory tutorials: the first on the use of OpenCV, an already established library for developing computer vision applications, and the second on the use of image processing to create special effects. Our advanced tutorials were also very diverse. The first dealt with the use of CUDA, a new language developed by NVIDIA for the use of next-generation graphics cards in general computing problems, in augmented reality applications. Our second advanced tutorial dealt with adjusting surfaces using pseudo-manifolds parameterized. Since the conference, the authors of this tutorial have published the material in other conferences and magazines, and thought it best to make the tutorial material available together with these publications on the project portal, located at http://w3.impa.br/~lvelho/ppm09 /.We hope that everyone enjoys the material in the three tutorials published here in this section.;;Instituto de Computação, UNICAMP, Campinas;pt_BR;Published;0;2010;2010-03-08 15:21:02
12362;Anselmo Antunes Montenegro;Prefácio à Seção Especial dos Tutoriais do SIBGRAPI'08;Welcome to the special section of SIBGRAPI 2008 tutorials! SIBGRAPI 2008 participants had the opportunity to witness an exceptional set of tutorials that covered a wide range of topics of interest to the community. We had two introductory tutorials: the first on the use of OpenCV, an already established library for developing computer vision applications, and the second on the use of image processing to create special effects. Our advanced tutorials were also very diverse. The first dealt with the use of CUDA, a new language developed by NVIDIA for the use of next-generation graphics cards in general computing problems, in augmented reality applications. Our second advanced tutorial dealt with adjusting surfaces using pseudo-manifolds parameterized. Since the conference, the authors of this tutorial have published the material in other conferences and magazines, and thought it best to make the tutorial material available together with these publications on the project portal, located at http://w3.impa.br/~lvelho/ppm09 /.We hope that everyone enjoys the material in the three tutorials published here in this section.;;Instituto de Computação, UFF, Niterói-RJ;pt_BR;Published;0;2010;2010-03-08 15:21:02
12509;Paulo Lumertz;Quantools: A MDA transformation approach;Model driven architecture (MDA) represents a challenge for the companies to increase the benefits of code generation, creating systems by using common libraries and standards. This paper presents the Quantools that is a result of a 5-year project, introducing the benefits of MDA in a Brazilian software development company. Quantools is based on the concept of cartridges that specify the transformation rules and constraints for a particular domain. The tool may be integrated to the modeling activities of the system to check the correctness of models according to the standards previously defined. The Quantools and its Cartridges execute transformations to generate the source code in a particular domain.;;Quantiza Systems Av. Dos Municípios, 5510 Santa Lúcia Campo Bom – RS;pt_BR;Published;3;2010;2010-03-21 11:40:59
12509;Ana Paula Terra Bacelo;Quantools: A MDA transformation approach;Model driven architecture (MDA) represents a challenge for the companies to increase the benefits of code generation, creating systems by using common libraries and standards. This paper presents the Quantools that is a result of a 5-year project, introducing the benefits of MDA in a Brazilian software development company. Quantools is based on the concept of cartridges that specify the transformation rules and constraints for a particular domain. The tool may be integrated to the modeling activities of the system to check the correctness of models according to the standards previously defined. The Quantools and its Cartridges execute transformations to generate the source code in a particular domain.;;Pontifical Catholic of Rio Grande do Sul – Faculty of Informatics Av. Ipiranga, 6681 – Prédio 32 – 6 Floor - Porto Alegre – RS;pt_BR;Published;3;2010;2010-03-21 11:40:59
12509;Toacy Oliveira;Quantools: A MDA transformation approach;Model driven architecture (MDA) represents a challenge for the companies to increase the benefits of code generation, creating systems by using common libraries and standards. This paper presents the Quantools that is a result of a 5-year project, introducing the benefits of MDA in a Brazilian software development company. Quantools is based on the concept of cartridges that specify the transformation rules and constraints for a particular domain. The tool may be integrated to the modeling activities of the system to check the correctness of models according to the standards previously defined. The Quantools and its Cartridges execute transformations to generate the source code in a particular domain.;;University of Waterloo - David R. Cheriton School of Computer Science 200 university Ave N2L 6P2 - Waterloo;pt_BR;Published;3;2010;2010-03-21 11:40:59
12569;Lidia López;HiME: Hierarchical i* Modeling Editor;In this paper, we present HiME, a tool for editing i* models. Thedistinguishing characteristic of HiME is its ability to deal with inheritance. It includes specific operations for declaring an actor as heir of another and then stating the relationships between the intentional elements of both actors.;;-;pt_BR;Published;3;2010;2010-03-27 10:54:32
12569;Xavier Franch;HiME: Hierarchical i* Modeling Editor;In this paper, we present HiME, a tool for editing i* models. Thedistinguishing characteristic of HiME is its ability to deal with inheritance. It includes specific operations for declaring an actor as heir of another and then stating the relationships between the intentional elements of both actors.;;-;pt_BR;Published;3;2010;2010-03-27 10:54:32
12569;Jordi Marco;HiME: Hierarchical i* Modeling Editor;In this paper, we present HiME, a tool for editing i* models. Thedistinguishing characteristic of HiME is its ability to deal with inheritance. It includes specific operations for declaring an actor as heir of another and then stating the relationships between the intentional elements of both actors.;;-;pt_BR;Published;3;2010;2010-03-27 10:54:32
12570;Vânia M. P. Vidal;An Ontology-Based Framework for Heterogeneous Data Sources Integration;Ontologies have been extensively used to model domain-specific knowledge. The main reason for this success is due to their capability to be at the “semantic” level, away from data structures and implementation strategies. In addition, ontology formalisms have allowed certain kinds of reasoning to be automated within a reasonable time complexity. Due to ontology data independence and automated reasoning, ontologies are well suited for integrating heterogeneous databases, enabling interoperability among isparate systems, and specifying interfaces to independent, knowledge-based services.;;-;pt_BR;Published;3;2010;2010-03-27 11:17:00
12570;João C. Pinheiro;An Ontology-Based Framework for Heterogeneous Data Sources Integration;Ontologies have been extensively used to model domain-specific knowledge. The main reason for this success is due to their capability to be at the “semantic” level, away from data structures and implementation strategies. In addition, ontology formalisms have allowed certain kinds of reasoning to be automated within a reasonable time complexity. Due to ontology data independence and automated reasoning, ontologies are well suited for integrating heterogeneous databases, enabling interoperability among isparate systems, and specifying interfaces to independent, knowledge-based services.;;-;pt_BR;Published;3;2010;2010-03-27 11:17:00
12570;Eveline R. Sacramento;An Ontology-Based Framework for Heterogeneous Data Sources Integration;Ontologies have been extensively used to model domain-specific knowledge. The main reason for this success is due to their capability to be at the “semantic” level, away from data structures and implementation strategies. In addition, ontology formalisms have allowed certain kinds of reasoning to be automated within a reasonable time complexity. Due to ontology data independence and automated reasoning, ontologies are well suited for integrating heterogeneous databases, enabling interoperability among isparate systems, and specifying interfaces to independent, knowledge-based services.;;-;pt_BR;Published;3;2010;2010-03-27 11:17:00
12570;José Antonio Fernandes de Macêdo;An Ontology-Based Framework for Heterogeneous Data Sources Integration;Ontologies have been extensively used to model domain-specific knowledge. The main reason for this success is due to their capability to be at the “semantic” level, away from data structures and implementation strategies. In addition, ontology formalisms have allowed certain kinds of reasoning to be automated within a reasonable time complexity. Due to ontology data independence and automated reasoning, ontologies are well suited for integrating heterogeneous databases, enabling interoperability among isparate systems, and specifying interfaces to independent, knowledge-based services.;;-;pt_BR;Published;3;2010;2010-03-27 11:17:00
12570;Bernadette F. Lóscio;An Ontology-Based Framework for Heterogeneous Data Sources Integration;Ontologies have been extensively used to model domain-specific knowledge. The main reason for this success is due to their capability to be at the “semantic” level, away from data structures and implementation strategies. In addition, ontology formalisms have allowed certain kinds of reasoning to be automated within a reasonable time complexity. Due to ontology data independence and automated reasoning, ontologies are well suited for integrating heterogeneous databases, enabling interoperability among isparate systems, and specifying interfaces to independent, knowledge-based services.;;-;pt_BR;Published;3;2010;2010-03-27 11:17:00
12571;Haiyang Sun;A Temporal-Rule Based Verification System for Business Collaboration Reliability;Based on the temporal rules defined for the business processe participating in a business collaboration, we present an implementation for a system called TiCoBTS to verify the reliability of the business collaboration.;;-;pt_BR;Published;3;2010;2010-03-27 11:53:36
12571;Jian Yang;A Temporal-Rule Based Verification System for Business Collaboration Reliability;Based on the temporal rules defined for the business processe participating in a business collaboration, we present an implementation for a system called TiCoBTS to verify the reliability of the business collaboration.;;-;pt_BR;Published;3;2010;2010-03-27 11:53:36
12571;Weiliang Zhao;A Temporal-Rule Based Verification System for Business Collaboration Reliability;Based on the temporal rules defined for the business processe participating in a business collaboration, we present an implementation for a system called TiCoBTS to verify the reliability of the business collaboration.;;-;pt_BR;Published;3;2010;2010-03-27 11:53:36
12572;Douglas da Silva;Semantic Web and Knowledge Management in User Data Privacy;This paper discusses knowledge representation for privacy andaccountability issues.;;-;pt_BR;Published;1;2010;2010-03-27 12:08:24
12572;Mírian Bruckschen;Semantic Web and Knowledge Management in User Data Privacy;This paper discusses knowledge representation for privacy andaccountability issues.;;-;pt_BR;Published;1;2010;2010-03-27 12:08:24
12572;Paulo Bridi;Semantic Web and Knowledge Management in User Data Privacy;This paper discusses knowledge representation for privacy andaccountability issues.;;-;pt_BR;Published;1;2010;2010-03-27 12:08:24
12572;Roger Granada;Semantic Web and Knowledge Management in User Data Privacy;This paper discusses knowledge representation for privacy andaccountability issues.;;-;pt_BR;Published;1;2010;2010-03-27 12:08:24
12572;Alexandre Agustini;Semantic Web and Knowledge Management in User Data Privacy;This paper discusses knowledge representation for privacy andaccountability issues.;;-;pt_BR;Published;1;2010;2010-03-27 12:08:24
12572;Renata Vieira;Semantic Web and Knowledge Management in User Data Privacy;This paper discusses knowledge representation for privacy andaccountability issues.;;-;pt_BR;Published;1;2010;2010-03-27 12:08:24
12572;Caio Northfleet;Semantic Web and Knowledge Management in User Data Privacy;This paper discusses knowledge representation for privacy andaccountability issues.;;-;pt_BR;Published;1;2010;2010-03-27 12:08:24
12572;Prasad Rao;Semantic Web and Knowledge Management in User Data Privacy;This paper discusses knowledge representation for privacy andaccountability issues.;;-;pt_BR;Published;1;2010;2010-03-27 12:08:24
12572;Tomas Sander;Semantic Web and Knowledge Management in User Data Privacy;This paper discusses knowledge representation for privacy andaccountability issues.;;-;pt_BR;Published;1;2010;2010-03-27 12:08:24
12573;Raphael do Vale A. Gomes;MatchMaking – A Tool to Match OWL Schemas;This paper describes a software tool that implements an instancebased schema matching technique for OWL dialects. The technique is based on a matching algorithm that depends on the definition of similarity functions that evaluate the semantic proximity of elements from two different schemas. The tool is engineered to accommodate different similarity functions and variations of the matching algorithm, thereby facilitating experimentation with alternative setups.;;-;pt_BR;Published;5;2010;2010-03-27 12:24:08
12573;Luiz André P. Paes Leme;MatchMaking – A Tool to Match OWL Schemas;This paper describes a software tool that implements an instancebased schema matching technique for OWL dialects. The technique is based on a matching algorithm that depends on the definition of similarity functions that evaluate the semantic proximity of elements from two different schemas. The tool is engineered to accommodate different similarity functions and variations of the matching algorithm, thereby facilitating experimentation with alternative setups.;;-;pt_BR;Published;5;2010;2010-03-27 12:24:08
12573;Marco A. Casanova;MatchMaking – A Tool to Match OWL Schemas;This paper describes a software tool that implements an instancebased schema matching technique for OWL dialects. The technique is based on a matching algorithm that depends on the definition of similarity functions that evaluate the semantic proximity of elements from two different schemas. The tool is engineered to accommodate different similarity functions and variations of the matching algorithm, thereby facilitating experimentation with alternative setups.;;-;pt_BR;Published;5;2010;2010-03-27 12:24:08
12576;Mirel Cosulschi;Web Information Extraction by Semantic Tagging;An important aspect of research for Web information extraction relates to the inference of complex reasoning and correlation based on distributed information available in many different Web data sources. By defining the semantics of information and services available on the Web, the World Wide Web becomes a vast store of information that can be easily  processed by computer applications.;;-;pt_BR;Published;1;2010;2010-03-27 14:50:03
12576;Roberto De Virgilio;Web Information Extraction by Semantic Tagging;An important aspect of research for Web information extraction relates to the inference of complex reasoning and correlation based on distributed information available in many different Web data sources. By defining the semantics of information and services available on the Web, the World Wide Web becomes a vast store of information that can be easily  processed by computer applications.;;-;pt_BR;Published;1;2010;2010-03-27 14:50:03
12576;Tommaso Di Noia;Web Information Extraction by Semantic Tagging;An important aspect of research for Web information extraction relates to the inference of complex reasoning and correlation based on distributed information available in many different Web data sources. By defining the semantics of information and services available on the Web, the World Wide Web becomes a vast store of information that can be easily  processed by computer applications.;;-;pt_BR;Published;1;2010;2010-03-27 14:50:03
12576;Roberto Mirizzi;Web Information Extraction by Semantic Tagging;An important aspect of research for Web information extraction relates to the inference of complex reasoning and correlation based on distributed information available in many different Web data sources. By defining the semantics of information and services available on the Web, the World Wide Web becomes a vast store of information that can be easily  processed by computer applications.;;-;pt_BR;Published;1;2010;2010-03-27 14:50:03
12577;Jinho Kim;SAMSTARplus: An Automatic Tool for Generating Multi- Dimensional Schemas from an Entity-Relationship Diagram;This paper presents a tool that automatically generates multidimensional schemas for data warehouses from OLTP entity-relationship diagrams (ERDs). Based on user’s input parameters, it generates star schemas, snowflake schemas, or a fact constellation schema by taking advantage of only structural information of input ERDs. Hence, SAMSTARplus can help users reduce efforts for designing data warehouses and aids decision making.;;-;pt_BR;Published;3;2010;2010-03-27 15:03:10
12577;Donghoo Kim;SAMSTARplus: An Automatic Tool for Generating Multi- Dimensional Schemas from an Entity-Relationship Diagram;This paper presents a tool that automatically generates multidimensional schemas for data warehouses from OLTP entity-relationship diagrams (ERDs). Based on user’s input parameters, it generates star schemas, snowflake schemas, or a fact constellation schema by taking advantage of only structural information of input ERDs. Hence, SAMSTARplus can help users reduce efforts for designing data warehouses and aids decision making.;;-;pt_BR;Published;3;2010;2010-03-27 15:03:10
12577;Suan Lee;SAMSTARplus: An Automatic Tool for Generating Multi- Dimensional Schemas from an Entity-Relationship Diagram;This paper presents a tool that automatically generates multidimensional schemas for data warehouses from OLTP entity-relationship diagrams (ERDs). Based on user’s input parameters, it generates star schemas, snowflake schemas, or a fact constellation schema by taking advantage of only structural information of input ERDs. Hence, SAMSTARplus can help users reduce efforts for designing data warehouses and aids decision making.;;-;pt_BR;Published;3;2010;2010-03-27 15:03:10
12577;Yang-Sae Moon;SAMSTARplus: An Automatic Tool for Generating Multi- Dimensional Schemas from an Entity-Relationship Diagram;This paper presents a tool that automatically generates multidimensional schemas for data warehouses from OLTP entity-relationship diagrams (ERDs). Based on user’s input parameters, it generates star schemas, snowflake schemas, or a fact constellation schema by taking advantage of only structural information of input ERDs. Hence, SAMSTARplus can help users reduce efforts for designing data warehouses and aids decision making.;;-;pt_BR;Published;3;2010;2010-03-27 15:03:10
12577;Il-Yeol Song;SAMSTARplus: An Automatic Tool for Generating Multi- Dimensional Schemas from an Entity-Relationship Diagram;This paper presents a tool that automatically generates multidimensional schemas for data warehouses from OLTP entity-relationship diagrams (ERDs). Based on user’s input parameters, it generates star schemas, snowflake schemas, or a fact constellation schema by taking advantage of only structural information of input ERDs. Hence, SAMSTARplus can help users reduce efforts for designing data warehouses and aids decision making.;;-;pt_BR;Published;3;2010;2010-03-27 15:03:10
12577;Ritu Khare;SAMSTARplus: An Automatic Tool for Generating Multi- Dimensional Schemas from an Entity-Relationship Diagram;This paper presents a tool that automatically generates multidimensional schemas for data warehouses from OLTP entity-relationship diagrams (ERDs). Based on user’s input parameters, it generates star schemas, snowflake schemas, or a fact constellation schema by taking advantage of only structural information of input ERDs. Hence, SAMSTARplus can help users reduce efforts for designing data warehouses and aids decision making.;;-;pt_BR;Published;3;2010;2010-03-27 15:03:10
12577;Yuan An;SAMSTARplus: An Automatic Tool for Generating Multi- Dimensional Schemas from an Entity-Relationship Diagram;This paper presents a tool that automatically generates multidimensional schemas for data warehouses from OLTP entity-relationship diagrams (ERDs). Based on user’s input parameters, it generates star schemas, snowflake schemas, or a fact constellation schema by taking advantage of only structural information of input ERDs. Hence, SAMSTARplus can help users reduce efforts for designing data warehouses and aids decision making.;;-;pt_BR;Published;3;2010;2010-03-27 15:03:10
12578;Bruno Leonardo Barros Silva;An Approach for Reducing the Gap between BPMN Models and Implementation Artifacts;The need for using high-level modeling tools (e.g. BPMN) isincreasing considerably. The proliferation of the service oriented architectures (SOA) is also apparent. In this context, there is a gap between the developed model and its execution. This work introduces the MoSC Translator which translates models produced in BPMN into executable WS-BPEL processes.;;Federal University of Pernambuco;pt_BR;Published;3;2010;2010-03-27 15:20:20
12578;Robson Wagner Albuquerque de Medeiros;An Approach for Reducing the Gap between BPMN Models and Implementation Artifacts;The need for using high-level modeling tools (e.g. BPMN) isincreasing considerably. The proliferation of the service oriented architectures (SOA) is also apparent. In this context, there is a gap between the developed model and its execution. This work introduces the MoSC Translator which translates models produced in BPMN into executable WS-BPEL processes.;;-;pt_BR;Published;3;2010;2010-03-27 15:20:20
12578;Julio Cesar Damasceno;An Approach for Reducing the Gap between BPMN Models and Implementation Artifacts;The need for using high-level modeling tools (e.g. BPMN) isincreasing considerably. The proliferation of the service oriented architectures (SOA) is also apparent. In this context, there is a gap between the developed model and its execution. This work introduces the MoSC Translator which translates models produced in BPMN into executable WS-BPEL processes.;;-;pt_BR;Published;3;2010;2010-03-27 15:20:20
12578;Fernando Antonio Aires Lins;An Approach for Reducing the Gap between BPMN Models and Implementation Artifacts;The need for using high-level modeling tools (e.g. BPMN) isincreasing considerably. The proliferation of the service oriented architectures (SOA) is also apparent. In this context, there is a gap between the developed model and its execution. This work introduces the MoSC Translator which translates models produced in BPMN into executable WS-BPEL processes.;;-;pt_BR;Published;3;2010;2010-03-27 15:20:20
12578;Nelson Souto Rosa;An Approach for Reducing the Gap between BPMN Models and Implementation Artifacts;The need for using high-level modeling tools (e.g. BPMN) isincreasing considerably. The proliferation of the service oriented architectures (SOA) is also apparent. In this context, there is a gap between the developed model and its execution. This work introduces the MoSC Translator which translates models produced in BPMN into executable WS-BPEL processes.;;-;pt_BR;Published;3;2010;2010-03-27 15:20:20
12578;Paulo Romero Martins Maciel;An Approach for Reducing the Gap between BPMN Models and Implementation Artifacts;The need for using high-level modeling tools (e.g. BPMN) isincreasing considerably. The proliferation of the service oriented architectures (SOA) is also apparent. In this context, there is a gap between the developed model and its execution. This work introduces the MoSC Translator which translates models produced in BPMN into executable WS-BPEL processes.;;-;pt_BR;Published;3;2010;2010-03-27 15:20:20
12578;Bryan Stephenson;An Approach for Reducing the Gap between BPMN Models and Implementation Artifacts;The need for using high-level modeling tools (e.g. BPMN) isincreasing considerably. The proliferation of the service oriented architectures (SOA) is also apparent. In this context, there is a gap between the developed model and its execution. This work introduces the MoSC Translator which translates models produced in BPMN into executable WS-BPEL processes.;;-;pt_BR;Published;3;2010;2010-03-27 15:20:20
12578;Hamid Reza Motahari Nezhad;An Approach for Reducing the Gap between BPMN Models and Implementation Artifacts;The need for using high-level modeling tools (e.g. BPMN) isincreasing considerably. The proliferation of the service oriented architectures (SOA) is also apparent. In this context, there is a gap between the developed model and its execution. This work introduces the MoSC Translator which translates models produced in BPMN into executable WS-BPEL processes.;;-;pt_BR;Published;3;2010;2010-03-27 15:20:20
12578;Jun Li;An Approach for Reducing the Gap between BPMN Models and Implementation Artifacts;The need for using high-level modeling tools (e.g. BPMN) isincreasing considerably. The proliferation of the service oriented architectures (SOA) is also apparent. In this context, there is a gap between the developed model and its execution. This work introduces the MoSC Translator which translates models produced in BPMN into executable WS-BPEL processes.;;-;pt_BR;Published;3;2010;2010-03-27 15:20:20
12578;Caio Northfleet;An Approach for Reducing the Gap between BPMN Models and Implementation Artifacts;The need for using high-level modeling tools (e.g. BPMN) isincreasing considerably. The proliferation of the service oriented architectures (SOA) is also apparent. In this context, there is a gap between the developed model and its execution. This work introduces the MoSC Translator which translates models produced in BPMN into executable WS-BPEL processes.;;-;pt_BR;Published;3;2010;2010-03-27 15:20:20
12578;André Ricardo da Silva Souza;An Approach for Reducing the Gap between BPMN Models and Implementation Artifacts;The need for using high-level modeling tools (e.g. BPMN) isincreasing considerably. The proliferation of the service oriented architectures (SOA) is also apparent. In this context, there is a gap between the developed model and its execution. This work introduces the MoSC Translator which translates models produced in BPMN into executable WS-BPEL processes.;;-;pt_BR;Published;3;2010;2010-03-27 15:20:20
12579;Bernardo Pereira Nunes;A Frame-Based System for Automatic Classification of Semi- Structured Data;The problem of data classification goes back to the definition oftaxonomies covering knowledge areas. With the advent of the Web, the amount of data available increased several orders of magnitude, making manual data classification impossible. This work presents a tool to automatically classify semi-structured data, represented by frames, without any previous knowledge about structured classes. The tool uses a variation of the K-Medoid algorithm and organizes a set of frames into classes, structured as a strict hierarchy.;;-;pt_BR;Published;5;2010;2010-03-27 15:26:24
12579;Marco Antonio Casanova;A Frame-Based System for Automatic Classification of Semi- Structured Data;The problem of data classification goes back to the definition oftaxonomies covering knowledge areas. With the advent of the Web, the amount of data available increased several orders of magnitude, making manual data classification impossible. This work presents a tool to automatically classify semi-structured data, represented by frames, without any previous knowledge about structured classes. The tool uses a variation of the K-Medoid algorithm and organizes a set of frames into classes, structured as a strict hierarchy.;;-;pt_BR;Published;5;2010;2010-03-27 15:26:24
12580;Sonia Bergamaschi;Dealing with Uncertainty in Lexical Annotation;We present ALA, a tool for the automatic lexical annotation (i.e.annotation w.r.t. a thesaurus/lexical resource) of structured and semi-structured data sources and the discovery of probabilistic lexical relationships in a data integration environment. ALA performs automatic lexical annotation through the use of probabilistic annotations, i.e. an annotation is associated to a probability value. By performing probabilistic lexical annotation, we discover probabilistic inter-sources lexical relationships among schema elements. ALA extends the lexical annotation module of the MOMIS data integration system. However, it may be applied in general in the context of schema mapping discovery, ontology merging and data integration system and it is particularly suitable for performing “on-the-fly” data integration or probabilistic ontology matching.;;-;pt_BR;Published;3;2010;2010-03-27 15:46:33
12580;Laura Po;Dealing with Uncertainty in Lexical Annotation;We present ALA, a tool for the automatic lexical annotation (i.e.annotation w.r.t. a thesaurus/lexical resource) of structured and semi-structured data sources and the discovery of probabilistic lexical relationships in a data integration environment. ALA performs automatic lexical annotation through the use of probabilistic annotations, i.e. an annotation is associated to a probability value. By performing probabilistic lexical annotation, we discover probabilistic inter-sources lexical relationships among schema elements. ALA extends the lexical annotation module of the MOMIS data integration system. However, it may be applied in general in the context of schema mapping discovery, ontology merging and data integration system and it is particularly suitable for performing “on-the-fly” data integration or probabilistic ontology matching.;;-;pt_BR;Published;3;2010;2010-03-27 15:46:33
12580;Serena Sorrentino;Dealing with Uncertainty in Lexical Annotation;We present ALA, a tool for the automatic lexical annotation (i.e.annotation w.r.t. a thesaurus/lexical resource) of structured and semi-structured data sources and the discovery of probabilistic lexical relationships in a data integration environment. ALA performs automatic lexical annotation through the use of probabilistic annotations, i.e. an annotation is associated to a probability value. By performing probabilistic lexical annotation, we discover probabilistic inter-sources lexical relationships among schema elements. ALA extends the lexical annotation module of the MOMIS data integration system. However, it may be applied in general in the context of schema mapping discovery, ontology merging and data integration system and it is particularly suitable for performing “on-the-fly” data integration or probabilistic ontology matching.;;-;pt_BR;Published;3;2010;2010-03-27 15:46:33
12580;Alberto Corni;Dealing with Uncertainty in Lexical Annotation;We present ALA, a tool for the automatic lexical annotation (i.e.annotation w.r.t. a thesaurus/lexical resource) of structured and semi-structured data sources and the discovery of probabilistic lexical relationships in a data integration environment. ALA performs automatic lexical annotation through the use of probabilistic annotations, i.e. an annotation is associated to a probability value. By performing probabilistic lexical annotation, we discover probabilistic inter-sources lexical relationships among schema elements. ALA extends the lexical annotation module of the MOMIS data integration system. However, it may be applied in general in the context of schema mapping discovery, ontology merging and data integration system and it is particularly suitable for performing “on-the-fly” data integration or probabilistic ontology matching.;;-;pt_BR;Published;3;2010;2010-03-27 15:46:33
12581;Julio Cesar Damasceno;Towards Generating Richer Code by Binding Security Abstractions to BPMN Task Types;This paper presents an approach for binding security requirements to different BPMN task types to create secure executable business processes.;;Federal University of Pernambuco;pt_BR;Published;1;2010;2010-03-27 16:00:07
12581;Hamid Reza Motahari Nezhad;Towards Generating Richer Code by Binding Security Abstractions to BPMN Task Types;This paper presents an approach for binding security requirements to different BPMN task types to create secure executable business processes.;;Federal University of Pernambuco;pt_BR;Published;1;2010;2010-03-27 16:00:07
12581;Jun Li;Towards Generating Richer Code by Binding Security Abstractions to BPMN Task Types;This paper presents an approach for binding security requirements to different BPMN task types to create secure executable business processes.;;Federal University of Pernambuco;pt_BR;Published;1;2010;2010-03-27 16:00:07
12581;Caio Northfleet;Towards Generating Richer Code by Binding Security Abstractions to BPMN Task Types;This paper presents an approach for binding security requirements to different BPMN task types to create secure executable business processes.;;Federal University of Pernambuco;pt_BR;Published;1;2010;2010-03-27 16:00:07
12581;Bryan Stephenson;Towards Generating Richer Code by Binding Security Abstractions to BPMN Task Types;This paper presents an approach for binding security requirements to different BPMN task types to create secure executable business processes.;;Federal University of Pernambuco;pt_BR;Published;1;2010;2010-03-27 16:00:07
12581;André Ricardo da Silva Souza;Towards Generating Richer Code by Binding Security Abstractions to BPMN Task Types;This paper presents an approach for binding security requirements to different BPMN task types to create secure executable business processes.;;Federal University of Pernambuco;pt_BR;Published;1;2010;2010-03-27 16:00:07
12581;Robson Wagner Albuquerque de Medeiros;Towards Generating Richer Code by Binding Security Abstractions to BPMN Task Types;This paper presents an approach for binding security requirements to different BPMN task types to create secure executable business processes.;;Federal University of Pernambuco;pt_BR;Published;1;2010;2010-03-27 16:00:07
12581;Bruno Leonardo Barros Silva;Towards Generating Richer Code by Binding Security Abstractions to BPMN Task Types;This paper presents an approach for binding security requirements to different BPMN task types to create secure executable business processes.;;-;pt_BR;Published;1;2010;2010-03-27 16:00:07
12581;Fernando Antonio Aires Lins;Towards Generating Richer Code by Binding Security Abstractions to BPMN Task Types;This paper presents an approach for binding security requirements to different BPMN task types to create secure executable business processes.;;-;pt_BR;Published;1;2010;2010-03-27 16:00:07
12581;Nelson Souto Rosa;Towards Generating Richer Code by Binding Security Abstractions to BPMN Task Types;This paper presents an approach for binding security requirements to different BPMN task types to create secure executable business processes.;;-;pt_BR;Published;1;2010;2010-03-27 16:00:07
12581;Paulo Romero Martins Maciel;Towards Generating Richer Code by Binding Security Abstractions to BPMN Task Types;This paper presents an approach for binding security requirements to different BPMN task types to create secure executable business processes.;;-;pt_BR;Published;1;2010;2010-03-27 16:00:07
12582;Irene Garrigós;An Eclipse-based Tool for Model Driven Web Applications with Personalization Support;Web engineering methods provide a systematic approach to develop complex and high quality Web applications. Many of these methods provide, in some way, personalization support. However, none of the existing Web methodologies provide a tool which allow the modeling, automatic generation and deployment of personalized Web applications in a complete way. In this line, they also do not exist tools which allow the designer modifying the personalization strategies at runtime, avoiding to regenerate Web applications from scratch. To tackle these lacks, in this paper we have presented a tool that has been developed togive support to the A-OOH (Adaptive Object Oriented Hypermedia) design method. The main advantages of this tool are the automatic generation and deployment of personalized Web applications with personalization support at runtime.;;-;pt_BR;Published;3;2010;2010-03-27 16:09:23
12582;Octavio Glorio;An Eclipse-based Tool for Model Driven Web Applications with Personalization Support;Web engineering methods provide a systematic approach to develop complex and high quality Web applications. Many of these methods provide, in some way, personalization support. However, none of the existing Web methodologies provide a tool which allow the modeling, automatic generation and deployment of personalized Web applications in a complete way. In this line, they also do not exist tools which allow the designer modifying the personalization strategies at runtime, avoiding to regenerate Web applications from scratch. To tackle these lacks, in this paper we have presented a tool that has been developed togive support to the A-OOH (Adaptive Object Oriented Hypermedia) design method. The main advantages of this tool are the automatic generation and deployment of personalized Web applications with personalization support at runtime.;;-;pt_BR;Published;3;2010;2010-03-27 16:09:23
12582;Paul Hernández;An Eclipse-based Tool for Model Driven Web Applications with Personalization Support;Web engineering methods provide a systematic approach to develop complex and high quality Web applications. Many of these methods provide, in some way, personalization support. However, none of the existing Web methodologies provide a tool which allow the modeling, automatic generation and deployment of personalized Web applications in a complete way. In this line, they also do not exist tools which allow the designer modifying the personalization strategies at runtime, avoiding to regenerate Web applications from scratch. To tackle these lacks, in this paper we have presented a tool that has been developed togive support to the A-OOH (Adaptive Object Oriented Hypermedia) design method. The main advantages of this tool are the automatic generation and deployment of personalized Web applications with personalization support at runtime.;;-;pt_BR;Published;3;2010;2010-03-27 16:09:23
12582;Alejandro Mate;An Eclipse-based Tool for Model Driven Web Applications with Personalization Support;Web engineering methods provide a systematic approach to develop complex and high quality Web applications. Many of these methods provide, in some way, personalization support. However, none of the existing Web methodologies provide a tool which allow the modeling, automatic generation and deployment of personalized Web applications in a complete way. In this line, they also do not exist tools which allow the designer modifying the personalization strategies at runtime, avoiding to regenerate Web applications from scratch. To tackle these lacks, in this paper we have presented a tool that has been developed togive support to the A-OOH (Adaptive Object Oriented Hypermedia) design method. The main advantages of this tool are the automatic generation and deployment of personalized Web applications with personalization support at runtime.;;-;pt_BR;Published;3;2010;2010-03-27 16:09:23
12583;Lucia Castro;Conceptual Modeling: the Linguistic Approach;After more than thirty years of its first introduction, conceptual modeling remains an important research field, which has been recently addressed by the literature on semantic interoperability in its various forms (model integration, service interoperability, knowledge harmonization, taxonomy alignment), domain engineering and the creation of conceptual models through Natural Language Processing (NLP), to name a few. In the database conceptual design, the designer must learn the language used in the Universe of Discourse (UoD) to be modeled, along with its underlying concepts, and then represent such concepts in a modeling language. Thus, the conceptual modeling process can be seen as a translation. For the resulting model to be both detailed and unambiguous, the designer must represent the UoD in a generative language which constructs can convey the same concepts represented in the respective natural language. For the whole process to be effective, we argue that the adoption of modeling languages and methodologies that are based on well-founded ontological theories is required. We propose the use of a linguistic approach for conceptual modeling from natural language texts, and illustrate how it may be applied using the well-founded modeling language OntoUML.;;-;pt_BR;Published;1;2010;2010-03-27 16:15:13
12583;Fernanda Araujo Araujo;Conceptual Modeling: the Linguistic Approach;After more than thirty years of its first introduction, conceptual modeling remains an important research field, which has been recently addressed by the literature on semantic interoperability in its various forms (model integration, service interoperability, knowledge harmonization, taxonomy alignment), domain engineering and the creation of conceptual models through Natural Language Processing (NLP), to name a few. In the database conceptual design, the designer must learn the language used in the Universe of Discourse (UoD) to be modeled, along with its underlying concepts, and then represent such concepts in a modeling language. Thus, the conceptual modeling process can be seen as a translation. For the resulting model to be both detailed and unambiguous, the designer must represent the UoD in a generative language which constructs can convey the same concepts represented in the respective natural language. For the whole process to be effective, we argue that the adoption of modeling languages and methodologies that are based on well-founded ontological theories is required. We propose the use of a linguistic approach for conceptual modeling from natural language texts, and illustrate how it may be applied using the well-founded modeling language OntoUML.;;-;pt_BR;Published;1;2010;2010-03-27 16:15:13
12583;Giancarlo Guizzardi;Conceptual Modeling: the Linguistic Approach;After more than thirty years of its first introduction, conceptual modeling remains an important research field, which has been recently addressed by the literature on semantic interoperability in its various forms (model integration, service interoperability, knowledge harmonization, taxonomy alignment), domain engineering and the creation of conceptual models through Natural Language Processing (NLP), to name a few. In the database conceptual design, the designer must learn the language used in the Universe of Discourse (UoD) to be modeled, along with its underlying concepts, and then represent such concepts in a modeling language. Thus, the conceptual modeling process can be seen as a translation. For the resulting model to be both detailed and unambiguous, the designer must represent the UoD in a generative language which constructs can convey the same concepts represented in the respective natural language. For the whole process to be effective, we argue that the adoption of modeling languages and methodologies that are based on well-founded ontological theories is required. We propose the use of a linguistic approach for conceptual modeling from natural language texts, and illustrate how it may be applied using the well-founded modeling language OntoUML.;;-;pt_BR;Published;1;2010;2010-03-27 16:15:13
12584;Raquel Pau;UMLtoSBVR: An SBVR-based tool to validate UML conceptual schemas;The UMLtoSBVR tool facilitates the interaction between designers and business people in order to refine and validate the information modeled in a UML conceptual schema (CS) to make sure that the CS is correct before starting the implementation phase. To this end, the tool is able to paraphrase the CS, i.e. to describe the CS elements using natural language expressions that can be understood by the business people lacking of the technical background to directly understand the UML notation. As an intermediate step, the tool transforms the UML CS into a SBVR (Semantics of Business Vocabulary and Business Rules) specification. The SBVR standard facilitates the expression of the CS in different natural language styles (as Structured English or Rule Speak).;;-;pt_BR;Published;3;2010;2010-03-27 16:22:14
12584;Jordi Cabot;UMLtoSBVR: An SBVR-based tool to validate UML conceptual schemas;The UMLtoSBVR tool facilitates the interaction between designers and business people in order to refine and validate the information modeled in a UML conceptual schema (CS) to make sure that the CS is correct before starting the implementation phase. To this end, the tool is able to paraphrase the CS, i.e. to describe the CS elements using natural language expressions that can be understood by the business people lacking of the technical background to directly understand the UML notation. As an intermediate step, the tool transforms the UML CS into a SBVR (Semantics of Business Vocabulary and Business Rules) specification. The SBVR standard facilitates the expression of the CS in different natural language styles (as Structured English or Rule Speak).;;-;pt_BR;Published;3;2010;2010-03-27 16:22:14
12584;Ruth Raventós;UMLtoSBVR: An SBVR-based tool to validate UML conceptual schemas;The UMLtoSBVR tool facilitates the interaction between designers and business people in order to refine and validate the information modeled in a UML conceptual schema (CS) to make sure that the CS is correct before starting the implementation phase. To this end, the tool is able to paraphrase the CS, i.e. to describe the CS elements using natural language expressions that can be understood by the business people lacking of the technical background to directly understand the UML notation. As an intermediate step, the tool transforms the UML CS into a SBVR (Semantics of Business Vocabulary and Business Rules) specification. The SBVR standard facilitates the expression of the CS in different natural language styles (as Structured English or Rule Speak).;;-;pt_BR;Published;3;2010;2010-03-27 16:22:14
12585;Mario A.M. Guimarães;The Animated Database Courseware (ADbC) and the Database Design Module;There is a need to extend the breadth and depth of database curricula and to find ways to incorporate newer technologies. One way to address this challenge is through supplementary instructional materials. However, very few supporting materials exist that aid in the teaching of teach database concepts. This was the motivation for constructing the Animated Database Courseware (ADbC) that is funded by NSF Grant #0717707. ADbC has four main modules: database design, SQL, database transactions and database security. The software has a low learning curve. It hasbeen made freely available and is located on the Internet at http://adbc.kennesaw.edu. This demonstration focuses on the Database Design Module.;;Instituto de Informática, UFRGS, Porto Alegre, RS;pt_BR;Published;3;2010;2010-03-27 16:29:44
12585;Meg Murray;The Animated Database Courseware (ADbC) and the Database Design Module;There is a need to extend the breadth and depth of database curricula and to find ways to incorporate newer technologies. One way to address this challenge is through supplementary instructional materials. However, very few supporting materials exist that aid in the teaching of teach database concepts. This was the motivation for constructing the Animated Database Courseware (ADbC) that is funded by NSF Grant #0717707. ADbC has four main modules: database design, SQL, database transactions and database security. The software has a low learning curve. It hasbeen made freely available and is located on the Internet at http://adbc.kennesaw.edu. This demonstration focuses on the Database Design Module.;;-;pt_BR;Published;3;2010;2010-03-27 16:29:44
12586;Ana L.D. Martins;Aumento de Resolução Temporal de Sequências de Imagens do Trato Vocal por meio de Registro das Imagens;Magnetic resonance imaging has been widely used in the study of speech production. Sequences of images of the vocal tract acquired during the emission of words and phonemes allow the dynamic identification of the forms assumed by this acoustic tube. However, it is important to highlight that the spatial and temporal resolutions necessary to identify the movement of speech articulators vary according to the speed and location of this movement, and such information is not known a priori. Existing approaches seek to improve the resolution of image sequences by improving the acquisition process or using more powerful acquisition means, which may be financially unfeasible. The method proposed in this article seeks to improve temporal resolution through a non-rigid registration method proposed in the literature. The movement identified by the registration allows for increased temporal resolution through a motion compensation interpolation technique. The movement present throughout the sequence is considered in the generation of each intermediate image. Therefore, the movement of the speech articulators in these images is coherent with the movement present throughout the sequence. The results indicate the efficiency of the proposed method.;;-;pt_BR;Published;15;2010;2010-03-27 17:07:51
12586;Nelson D.A. Mascarenhas;Aumento de Resolução Temporal de Sequências de Imagens do Trato Vocal por meio de Registro das Imagens;Magnetic resonance imaging has been widely used in the study of speech production. Sequences of images of the vocal tract acquired during the emission of words and phonemes allow the dynamic identification of the forms assumed by this acoustic tube. However, it is important to highlight that the spatial and temporal resolutions necessary to identify the movement of speech articulators vary according to the speed and location of this movement, and such information is not known a priori. Existing approaches seek to improve the resolution of image sequences by improving the acquisition process or using more powerful acquisition means, which may be financially unfeasible. The method proposed in this article seeks to improve temporal resolution through a non-rigid registration method proposed in the literature. The movement identified by the registration allows for increased temporal resolution through a motion compensation interpolation technique. The movement present throughout the sequence is considered in the generation of each intermediate image. Therefore, the movement of the speech articulators in these images is coherent with the movement present throughout the sequence. The results indicate the efficiency of the proposed method.;;-;pt_BR;Published;15;2010;2010-03-27 17:07:51
12586;Cláudio A.T. Suazo;Aumento de Resolução Temporal de Sequências de Imagens do Trato Vocal por meio de Registro das Imagens;Magnetic resonance imaging has been widely used in the study of speech production. Sequences of images of the vocal tract acquired during the emission of words and phonemes allow the dynamic identification of the forms assumed by this acoustic tube. However, it is important to highlight that the spatial and temporal resolutions necessary to identify the movement of speech articulators vary according to the speed and location of this movement, and such information is not known a priori. Existing approaches seek to improve the resolution of image sequences by improving the acquisition process or using more powerful acquisition means, which may be financially unfeasible. The method proposed in this article seeks to improve temporal resolution through a non-rigid registration method proposed in the literature. The movement identified by the registration allows for increased temporal resolution through a motion compensation interpolation technique. The movement present throughout the sequence is considered in the generation of each intermediate image. Therefore, the movement of the speech articulators in these images is coherent with the movement present throughout the sequence. The results indicate the efficiency of the proposed method.;;-;pt_BR;Published;15;2010;2010-03-27 17:07:51
12587;Marcos Proença de Almeida;Eliminação de Ruído Impulsivo Usando um Filtro Mediano Seletivo e Difusão Isotrópica;This work presents an algorithm combining a modification of the standard median filter, based on the impulsive noise detector proposed by Chen, Yang and Cao, with an isotropic diffusion process to remove salt and pepper noise. To eliminate impulsive noise in color images, the idea is to apply the algorithm separately to each color channel. The experiments carried out indicate that the proposed method presents itself as a robust filter for restoring images with large noise densities.;;-;pt_BR;Published;11;2010;2010-03-27 17:17:57
12587;Maurílio Boaventura;Eliminação de Ruído Impulsivo Usando um Filtro Mediano Seletivo e Difusão Isotrópica;This work presents an algorithm combining a modification of the standard median filter, based on the impulsive noise detector proposed by Chen, Yang and Cao, with an isotropic diffusion process to remove salt and pepper noise. To eliminate impulsive noise in color images, the idea is to apply the algorithm separately to each color channel. The experiments carried out indicate that the proposed method presents itself as a robust filter for restoring images with large noise densities.;;-;pt_BR;Published;11;2010;2010-03-27 17:17:57
12588;Osvaldo Severino Jr.;HSM: A New Color Space used in the Processing of Color Images;Inspired on the techniques used by painters to overlap layers of various hues of paint to create oil paintings, and also on observations of the the arrangement of Short-(S), Middle-(M), and Long-(L) wavelength-sensitive cones of the human retina for the interpretation of the colors, this paper proposes the use of the new color space called HSM to the processing of color images. To demonstrate the applicability of the HSM color space in the processing of color images, this paper proposes the pixelbased segmentation of a digital image of “human skin” or “non-skin”, the sketch of the face image and the pixel-based segmentation of the trumpet flowers tree (ype). The performance of the HSM color space in the pixel-based segmentation is compared with the HSV, YCbCr and TSL color spaces while the sketch of the face image is also compared with HSV, YCbCr and TSL colors spaces and the edge detectors of the Sobel, Prewitt, Roberts, Canny and Laplacian of Gaussian methods. The results demonstrate the potential of the proposed color space.;;-;pt_BR;Published;15;2010;2010-03-27 17:31:30
12588;Adilson Gonzaga;HSM: A New Color Space used in the Processing of Color Images;Inspired on the techniques used by painters to overlap layers of various hues of paint to create oil paintings, and also on observations of the the arrangement of Short-(S), Middle-(M), and Long-(L) wavelength-sensitive cones of the human retina for the interpretation of the colors, this paper proposes the use of the new color space called HSM to the processing of color images. To demonstrate the applicability of the HSM color space in the processing of color images, this paper proposes the pixelbased segmentation of a digital image of “human skin” or “non-skin”, the sketch of the face image and the pixel-based segmentation of the trumpet flowers tree (ype). The performance of the HSM color space in the pixel-based segmentation is compared with the HSV, YCbCr and TSL color spaces while the sketch of the face image is also compared with HSV, YCbCr and TSL colors spaces and the edge detectors of the Sobel, Prewitt, Roberts, Canny and Laplacian of Gaussian methods. The results demonstrate the potential of the proposed color space.;;-;pt_BR;Published;15;2010;2010-03-27 17:31:30
12590;Rodrigo Pereira dos Santos;An Approach Based on Maintainability Criteria for Building Aspect-Oriented Software Design Model;Software modeling is an important activity for maintenance since it can facilitate the software comprehension as well as the understanding of its activities towards evolution, correction and adaptation. In this sense, maintainability and its subcharacteristics as presented in the ISO/IEC 9126 standard should be incorporated 0to the artifacts produced in the modeling activity aiming at designing software with characteristics that render its maintenance less costly. Especially in non-trivial software, such as those that are aspect-oriented, the research on maintenance process considering it during the software development is remarkable. These categories of software aim at maintainability and reusability since they provide the separation of concerns. Thus, seeking to reduce the transition effort to the artifacts generated during the Aspect-Oriented Software Development among the different abstraction levels, this paper presents a proposal of maintainability criteria for building aspect-oriented software design models based on the Maintainability Criteria for Implementation Models, on the aSideML language modeling conventions and on the ISO/IEC 9126 standard,;;-;pt_BR;Published;14;2010;2010-03-27 17:43:32
12590;Heitor Augustus Xavier Costa;An Approach Based on Maintainability Criteria for Building Aspect-Oriented Software Design Model;Software modeling is an important activity for maintenance since it can facilitate the software comprehension as well as the understanding of its activities towards evolution, correction and adaptation. In this sense, maintainability and its subcharacteristics as presented in the ISO/IEC 9126 standard should be incorporated 0to the artifacts produced in the modeling activity aiming at designing software with characteristics that render its maintenance less costly. Especially in non-trivial software, such as those that are aspect-oriented, the research on maintenance process considering it during the software development is remarkable. These categories of software aim at maintainability and reusability since they provide the separation of concerns. Thus, seeking to reduce the transition effort to the artifacts generated during the Aspect-Oriented Software Development among the different abstraction levels, this paper presents a proposal of maintainability criteria for building aspect-oriented software design models based on the Maintainability Criteria for Implementation Models, on the aSideML language modeling conventions and on the ISO/IEC 9126 standard,;;-;pt_BR;Published;14;2010;2010-03-27 17:43:32
12590;Cláudia Maria Lima Werner;An Approach Based on Maintainability Criteria for Building Aspect-Oriented Software Design Model;Software modeling is an important activity for maintenance since it can facilitate the software comprehension as well as the understanding of its activities towards evolution, correction and adaptation. In this sense, maintainability and its subcharacteristics as presented in the ISO/IEC 9126 standard should be incorporated 0to the artifacts produced in the modeling activity aiming at designing software with characteristics that render its maintenance less costly. Especially in non-trivial software, such as those that are aspect-oriented, the research on maintenance process considering it during the software development is remarkable. These categories of software aim at maintainability and reusability since they provide the separation of concerns. Thus, seeking to reduce the transition effort to the artifacts generated during the Aspect-Oriented Software Development among the different abstraction levels, this paper presents a proposal of maintainability criteria for building aspect-oriented software design models based on the Maintainability Criteria for Implementation Models, on the aSideML language modeling conventions and on the ISO/IEC 9126 standard,;;-;pt_BR;Published;14;2010;2010-03-27 17:43:32
12590;André Fonseca Amâncio;An Approach Based on Maintainability Criteria for Building Aspect-Oriented Software Design Model;Software modeling is an important activity for maintenance since it can facilitate the software comprehension as well as the understanding of its activities towards evolution, correction and adaptation. In this sense, maintainability and its subcharacteristics as presented in the ISO/IEC 9126 standard should be incorporated 0to the artifacts produced in the modeling activity aiming at designing software with characteristics that render its maintenance less costly. Especially in non-trivial software, such as those that are aspect-oriented, the research on maintenance process considering it during the software development is remarkable. These categories of software aim at maintainability and reusability since they provide the separation of concerns. Thus, seeking to reduce the transition effort to the artifacts generated during the Aspect-Oriented Software Development among the different abstraction levels, this paper presents a proposal of maintainability criteria for building aspect-oriented software design models based on the Maintainability Criteria for Implementation Models, on the aSideML language modeling conventions and on the ISO/IEC 9126 standard,;;-;pt_BR;Published;14;2010;2010-03-27 17:43:32
12590;Paulo Afonso Parreira Júnior;An Approach Based on Maintainability Criteria for Building Aspect-Oriented Software Design Model;Software modeling is an important activity for maintenance since it can facilitate the software comprehension as well as the understanding of its activities towards evolution, correction and adaptation. In this sense, maintainability and its subcharacteristics as presented in the ISO/IEC 9126 standard should be incorporated 0to the artifacts produced in the modeling activity aiming at designing software with characteristics that render its maintenance less costly. Especially in non-trivial software, such as those that are aspect-oriented, the research on maintenance process considering it during the software development is remarkable. These categories of software aim at maintainability and reusability since they provide the separation of concerns. Thus, seeking to reduce the transition effort to the artifacts generated during the Aspect-Oriented Software Development among the different abstraction levels, this paper presents a proposal of maintainability criteria for building aspect-oriented software design models based on the Maintainability Criteria for Implementation Models, on the aSideML language modeling conventions and on the ISO/IEC 9126 standard,;;-;pt_BR;Published;14;2010;2010-03-27 17:43:32
12590;Antônio Maria Pereira de Resende;An Approach Based on Maintainability Criteria for Building Aspect-Oriented Software Design Model;Software modeling is an important activity for maintenance since it can facilitate the software comprehension as well as the understanding of its activities towards evolution, correction and adaptation. In this sense, maintainability and its subcharacteristics as presented in the ISO/IEC 9126 standard should be incorporated 0to the artifacts produced in the modeling activity aiming at designing software with characteristics that render its maintenance less costly. Especially in non-trivial software, such as those that are aspect-oriented, the research on maintenance process considering it during the software development is remarkable. These categories of software aim at maintainability and reusability since they provide the separation of concerns. Thus, seeking to reduce the transition effort to the artifacts generated during the Aspect-Oriented Software Development among the different abstraction levels, this paper presents a proposal of maintainability criteria for building aspect-oriented software design models based on the Maintainability Criteria for Implementation Models, on the aSideML language modeling conventions and on the ISO/IEC 9126 standard,;;-;pt_BR;Published;14;2010;2010-03-27 17:43:32
12590;Fábio Fagundes Silveira;An Approach Based on Maintainability Criteria for Building Aspect-Oriented Software Design Model;Software modeling is an important activity for maintenance since it can facilitate the software comprehension as well as the understanding of its activities towards evolution, correction and adaptation. In this sense, maintainability and its subcharacteristics as presented in the ISO/IEC 9126 standard should be incorporated 0to the artifacts produced in the modeling activity aiming at designing software with characteristics that render its maintenance less costly. Especially in non-trivial software, such as those that are aspect-oriented, the research on maintenance process considering it during the software development is remarkable. These categories of software aim at maintainability and reusability since they provide the separation of concerns. Thus, seeking to reduce the transition effort to the artifacts generated during the Aspect-Oriented Software Development among the different abstraction levels, this paper presents a proposal of maintainability criteria for building aspect-oriented software design models based on the Maintainability Criteria for Implementation Models, on the aSideML language modeling conventions and on the ISO/IEC 9126 standard,;;-;pt_BR;Published;14;2010;2010-03-27 17:43:32
12631;José Palazzo Moreira de Oliveira;Editorial;This issue of the Journal of Theoretical and Applied Informatics (RITA) has special significance. It constitutes an important milestone in this journey that we have undertaken 20 years ago. During this period, many researchers worked to build the history of RITA as reported in this year's first issue. RITA arose from an idea germinated in the Postgraduate Program in Computing at UFRGS, which began a project of great academic value: producing a national scientific journal of high quality and regularity. From 2004 onwards, we increased our efforts to increase RITA's visibility. In these two decades, technology and the culture of access to Journals have changed. The CAPES Digital Library has opened a new perspective of digital access, with students and researchers in Computer Science developing a new culture of access to these resources. From 2010 onwards we will publish three issues exclusively digitally, taking into account the global trend that indicates that electronic journals are the most accessed and read by the academic community. According to the record of access to our pages, we had 25,288 accesses to the electronic version in 2009. However, a large part of the administrative work was oriented towards the physical editing tasks of the Magazine;;Instituto de Informática, UFRGS, Porto Alegre, RS;pt_BR;Published;0;2010;2010-03-31 9:28:02
12643;Renato Ramos da Silva;Avaliação do Desempenho da Utilização de Threads em user level em Linux;The use of threads in computer systems is an important resource to enable the simultaneous execution of several pieces of code in a process. The way thread support is implemented on these systems has a direct impact on performance. In this work, an evaluation of the performance of using user level threads in Linux with the GNU Pth [2], Protothreads [1] and PM2 Marcel [7] libraries is presented. The performance evaluation was conducted using the data collection technique with monitors, and the response time of thread management and synchronization functions for each library and in CPU bound, I/O bound and simultaneous operations (CPU bound and I/O bound).;;Instituto de Ciências Matemáticas e de Computação - ICMC-USP.;pt_BR;Published;20;2010;2010-03-31 17:38:12
12643;Roberto Sadao Yokoyama;Avaliação do Desempenho da Utilização de Threads em user level em Linux;The use of threads in computer systems is an important resource to enable the simultaneous execution of several pieces of code in a process. The way thread support is implemented on these systems has a direct impact on performance. In this work, an evaluation of the performance of using user level threads in Linux with the GNU Pth [2], Protothreads [1] and PM2 Marcel [7] libraries is presented. The performance evaluation was conducted using the data collection technique with monitors, and the response time of thread management and synchronization functions for each library and in CPU bound, I/O bound and simultaneous operations (CPU bound and I/O bound).;;Instituto de Ciências Matemáticas e de Computação Universidade de São Paulo - Campus São CarlosAv. do Trabalhador Sancarlense, 400 - Centro Caixa Postal 668 - CEP 13560-970 São Carlos, SP;pt_BR;Published;20;2010;2010-03-31 17:38:12
12669;Yuri Saito;Modelo Estatístico para a Determinação do Número Ótimo de Iterações  do Filtro de Difusão Anisotrópica Aplicado à Redução de Ruído de  Imagens de Ressonância Magnética do Cérebro;Despite the great success of anisotropic diffusion filters in reducing noise in medical images, a limitation of this iterative approach is the automatic determination of the correct number of algorithm iterations, as a large number of iterations can cause excessive blurring of the edges between the anatomical structures, while a small number may not be enough to completely remove noise from the image. Therefore, this work proposes a statistical model for automatically determining the optimal number of iterations of the anisotropic diffusion filter applied to noise reduction in medical images. The model is determined "off-line" by maximizing the  structural similarity index, which is used in this work as  an objective function to quantitatively evaluate the quality of the  resulting images after each filter iteration. After determining  the model parameters, the optimal number of algorithm iterations  required to remove noise from the image while preserving the edges between anatomical structures is easily obtained. Results applied to 3D brain MRI imaging are presented to illustrate the effectiveness of the proposed method.;;Universidade de São Paulo;pt_BR;Published;18;2010;2010-04-06 23:11:43
12669;Ricardo José Ferrari;Modelo Estatístico para a Determinação do Número Ótimo de Iterações  do Filtro de Difusão Anisotrópica Aplicado à Redução de Ruído de  Imagens de Ressonância Magnética do Cérebro;Despite the great success of anisotropic diffusion filters in reducing noise in medical images, a limitation of this iterative approach is the automatic determination of the correct number of algorithm iterations, as a large number of iterations can cause excessive blurring of the edges between the anatomical structures, while a small number may not be enough to completely remove noise from the image. Therefore, this work proposes a statistical model for automatically determining the optimal number of iterations of the anisotropic diffusion filter applied to noise reduction in medical images. The model is determined "off-line" by maximizing the  structural similarity index, which is used in this work as  an objective function to quantitatively evaluate the quality of the  resulting images after each filter iteration. After determining  the model parameters, the optimal number of algorithm iterations  required to remove noise from the image while preserving the edges between anatomical structures is easily obtained. Results applied to 3D brain MRI imaging are presented to illustrate the effectiveness of the proposed method.;;Universidade Federal de Uberlândia;pt_BR;Published;18;2010;2010-04-06 23:11:43
12669;Jefferson Teixeira;Modelo Estatístico para a Determinação do Número Ótimo de Iterações  do Filtro de Difusão Anisotrópica Aplicado à Redução de Ruído de  Imagens de Ressonância Magnética do Cérebro;Despite the great success of anisotropic diffusion filters in reducing noise in medical images, a limitation of this iterative approach is the automatic determination of the correct number of algorithm iterations, as a large number of iterations can cause excessive blurring of the edges between the anatomical structures, while a small number may not be enough to completely remove noise from the image. Therefore, this work proposes a statistical model for automatically determining the optimal number of iterations of the anisotropic diffusion filter applied to noise reduction in medical images. The model is determined "off-line" by maximizing the  structural similarity index, which is used in this work as  an objective function to quantitatively evaluate the quality of the  resulting images after each filter iteration. After determining  the model parameters, the optimal number of algorithm iterations  required to remove noise from the image while preserving the edges between anatomical structures is easily obtained. Results applied to 3D brain MRI imaging are presented to illustrate the effectiveness of the proposed method.;;Universidade de São Paulo;pt_BR;Published;18;2010;2010-04-06 23:11:43
12669;Paulo M. de Azevedo Marques;Modelo Estatístico para a Determinação do Número Ótimo de Iterações  do Filtro de Difusão Anisotrópica Aplicado à Redução de Ruído de  Imagens de Ressonância Magnética do Cérebro;Despite the great success of anisotropic diffusion filters in reducing noise in medical images, a limitation of this iterative approach is the automatic determination of the correct number of algorithm iterations, as a large number of iterations can cause excessive blurring of the edges between the anatomical structures, while a small number may not be enough to completely remove noise from the image. Therefore, this work proposes a statistical model for automatically determining the optimal number of iterations of the anisotropic diffusion filter applied to noise reduction in medical images. The model is determined "off-line" by maximizing the  structural similarity index, which is used in this work as  an objective function to quantitatively evaluate the quality of the  resulting images after each filter iteration. After determining  the model parameters, the optimal number of algorithm iterations  required to remove noise from the image while preserving the edges between anatomical structures is easily obtained. Results applied to 3D brain MRI imaging are presented to illustrate the effectiveness of the proposed method.;;Universidade de São Paulo;pt_BR;Published;18;2010;2010-04-06 23:11:43
12669;Andre P.L.F. de Carvalho;Modelo Estatístico para a Determinação do Número Ótimo de Iterações  do Filtro de Difusão Anisotrópica Aplicado à Redução de Ruído de  Imagens de Ressonância Magnética do Cérebro;Despite the great success of anisotropic diffusion filters in reducing noise in medical images, a limitation of this iterative approach is the automatic determination of the correct number of algorithm iterations, as a large number of iterations can cause excessive blurring of the edges between the anatomical structures, while a small number may not be enough to completely remove noise from the image. Therefore, this work proposes a statistical model for automatically determining the optimal number of iterations of the anisotropic diffusion filter applied to noise reduction in medical images. The model is determined "off-line" by maximizing the  structural similarity index, which is used in this work as  an objective function to quantitatively evaluate the quality of the  resulting images after each filter iteration. After determining  the model parameters, the optimal number of algorithm iterations  required to remove noise from the image while preserving the edges between anatomical structures is easily obtained. Results applied to 3D brain MRI imaging are presented to illustrate the effectiveness of the proposed method.;;Universidade de São Paulo;pt_BR;Published;18;2010;2010-04-06 23:11:43
12669;Antônio Carlos dos Santos;Modelo Estatístico para a Determinação do Número Ótimo de Iterações  do Filtro de Difusão Anisotrópica Aplicado à Redução de Ruído de  Imagens de Ressonância Magnética do Cérebro;Despite the great success of anisotropic diffusion filters in reducing noise in medical images, a limitation of this iterative approach is the automatic determination of the correct number of algorithm iterations, as a large number of iterations can cause excessive blurring of the edges between the anatomical structures, while a small number may not be enough to completely remove noise from the image. Therefore, this work proposes a statistical model for automatically determining the optimal number of iterations of the anisotropic diffusion filter applied to noise reduction in medical images. The model is determined "off-line" by maximizing the  structural similarity index, which is used in this work as  an objective function to quantitatively evaluate the quality of the  resulting images after each filter iteration. After determining  the model parameters, the optimal number of algorithm iterations  required to remove noise from the image while preserving the edges between anatomical structures is easily obtained. Results applied to 3D brain MRI imaging are presented to illustrate the effectiveness of the proposed method.;;Universidade de São Paulo;pt_BR;Published;18;2010;2010-04-06 23:11:43
12923;Moanir Stábile Filho;Inteligência Swarm e Equilíbrio de Verhulst Aplicados à Alocação de Potência em Redes Ópticas CDMA Particionadas;The use of the code division multiple access (CDMA) technique in purely optical networks was proposed taking into account their wide optical bandwidth. Optical CDMA is a technique that allows multiple users to simultaneously share the same fiber channel asynchronously without \textit{delay} and without the need for resource management (\textit{scheduling}). However, multiple access interference (MAI) levels are significant in these systems due to the nature of incoherent power detection. Reducing this interference effect is crucial in improving the performance of optical CDMA systems. In this sense, it is proposed to use resource allocation mechanisms, particularly power, that guarantee each user a minimum acceptable performance in terms of bit error rate (BER). In the centralized power control strategy, it is necessary that a central node has information about the gain of all links. In turn, in partitioned power control (access network and \textit{broadcast} network) it is verified that the optical signals after the star coupler do not contribute to the determination of the MAI and therefore do not affect the power balance associated with each laser. In this work, the problem of power control for partitioned CDMA optical networks (p-OCDMA) is analyzed from two different points of view: a) Verhulst's mathematical model of population growth, discussed in [1] b) heuristic approach called intelligence \emph{swarm} (PSO -- \textit{particle swarm optimization}). Extensive simulations and numerical results indicated that both the Verhulst model and the PSO are suitable for solving the power control problem in p-OCDMA networks, from the point of view of uni-objective optimization, with satisfactory convergence results for a variety of of operating scenarios for the p-OCDMA system. Finally, a complexity analysis in terms of number of operations is carried out in order to obtain a more realistic comparative picture for the two optimization methodologies, clearly indicating the superiority of the analytical-iterative approach of Verhulst's approach.;;Departamento de Engenharia Elétrica, Universidade Estadual de Londrina;pt_BR;Published;28;2010;2010-04-20 19:50:56
12923;Taufik Abrão;Inteligência Swarm e Equilíbrio de Verhulst Aplicados à Alocação de Potência em Redes Ópticas CDMA Particionadas;The use of the code division multiple access (CDMA) technique in purely optical networks was proposed taking into account their wide optical bandwidth. Optical CDMA is a technique that allows multiple users to simultaneously share the same fiber channel asynchronously without \textit{delay} and without the need for resource management (\textit{scheduling}). However, multiple access interference (MAI) levels are significant in these systems due to the nature of incoherent power detection. Reducing this interference effect is crucial in improving the performance of optical CDMA systems. In this sense, it is proposed to use resource allocation mechanisms, particularly power, that guarantee each user a minimum acceptable performance in terms of bit error rate (BER). In the centralized power control strategy, it is necessary that a central node has information about the gain of all links. In turn, in partitioned power control (access network and \textit{broadcast} network) it is verified that the optical signals after the star coupler do not contribute to the determination of the MAI and therefore do not affect the power balance associated with each laser. In this work, the problem of power control for partitioned CDMA optical networks (p-OCDMA) is analyzed from two different points of view: a) Verhulst's mathematical model of population growth, discussed in [1] b) heuristic approach called intelligence \emph{swarm} (PSO -- \textit{particle swarm optimization}). Extensive simulations and numerical results indicated that both the Verhulst model and the PSO are suitable for solving the power control problem in p-OCDMA networks, from the point of view of uni-objective optimization, with satisfactory convergence results for a variety of of operating scenarios for the p-OCDMA system. Finally, a complexity analysis in terms of number of operations is carried out in order to obtain a more realistic comparative picture for the two optimization methodologies, clearly indicating the superiority of the analytical-iterative approach of Verhulst's approach.;;Departamento de Engenharia Elétrica, Universidade Estadual de Londrina;pt_BR;Published;28;2010;2010-04-20 19:50:56
12923;Lucas Dias Hiera Sampaio;Inteligência Swarm e Equilíbrio de Verhulst Aplicados à Alocação de Potência em Redes Ópticas CDMA Particionadas;The use of the code division multiple access (CDMA) technique in purely optical networks was proposed taking into account their wide optical bandwidth. Optical CDMA is a technique that allows multiple users to simultaneously share the same fiber channel asynchronously without \textit{delay} and without the need for resource management (\textit{scheduling}). However, multiple access interference (MAI) levels are significant in these systems due to the nature of incoherent power detection. Reducing this interference effect is crucial in improving the performance of optical CDMA systems. In this sense, it is proposed to use resource allocation mechanisms, particularly power, that guarantee each user a minimum acceptable performance in terms of bit error rate (BER). In the centralized power control strategy, it is necessary that a central node has information about the gain of all links. In turn, in partitioned power control (access network and \textit{broadcast} network) it is verified that the optical signals after the star coupler do not contribute to the determination of the MAI and therefore do not affect the power balance associated with each laser. In this work, the problem of power control for partitioned CDMA optical networks (p-OCDMA) is analyzed from two different points of view: a) Verhulst's mathematical model of population growth, discussed in [1] b) heuristic approach called intelligence \emph{swarm} (PSO -- \textit{particle swarm optimization}). Extensive simulations and numerical results indicated that both the Verhulst model and the PSO are suitable for solving the power control problem in p-OCDMA networks, from the point of view of uni-objective optimization, with satisfactory convergence results for a variety of of operating scenarios for the p-OCDMA system. Finally, a complexity analysis in terms of number of operations is carried out in order to obtain a more realistic comparative picture for the two optimization methodologies, clearly indicating the superiority of the analytical-iterative approach of Verhulst's approach.;;Departamento de Ciência da Computação, Universidade Estadual de Londrina;pt_BR;Published;28;2010;2010-04-20 19:50:56
12943;José Carlos Ferreira da Rocha;Integração de evidências em redes credais e a regra de Jeffrey;Credal networks provide a scheme for representing imprecise probabilistic models. The inference algorithms usually employed in credal networks compute the range of the posterior probability of an event of interest given evidence of a specific type - evidence that describes the current state of a set of variables. These algorithms do not perform evidential reasoning in the case in which the evidence must be processed according to the conditioning rule proposed by R.C. Jeffrey. Considering this, this article describes a procedure to integrate evidence with Jeffrey's rule when making inferences in credal networks.;;UEPG;pt_BR;Published;14;2010;2010-04-22 18:42:57
12943;Alaine Margarete Guimarães;Integração de evidências em redes credais e a regra de Jeffrey;Credal networks provide a scheme for representing imprecise probabilistic models. The inference algorithms usually employed in credal networks compute the range of the posterior probability of an event of interest given evidence of a specific type - evidence that describes the current state of a set of variables. These algorithms do not perform evidential reasoning in the case in which the evidence must be processed according to the conditioning rule proposed by R.C. Jeffrey. Considering this, this article describes a procedure to integrate evidence with Jeffrey's rule when making inferences in credal networks.;;UEPG;pt_BR;Published;14;2010;2010-04-22 18:42:57
12943;Cassio Polpo de Campos;Integração de evidências em redes credais e a regra de Jeffrey;Credal networks provide a scheme for representing imprecise probabilistic models. The inference algorithms usually employed in credal networks compute the range of the posterior probability of an event of interest given evidence of a specific type - evidence that describes the current state of a set of variables. These algorithms do not perform evidential reasoning in the case in which the evidence must be processed according to the conditioning rule proposed by R.C. Jeffrey. Considering this, this article describes a procedure to integrate evidence with Jeffrey's rule when making inferences in credal networks.;;IDISIA;pt_BR;Published;14;2010;2010-04-22 18:42:57
13764;Sarajane Marques Peres;Tutorial sobre Fuzzy-c-Means e Fuzzy Learning Vector Quantization: Abordagens Híbridas para Tarefas de Agrupamento e Classificação;In this tutorial, a discussion is presented on the Fuzzy-c-Means algorithm and Fuzzy Neural Networks, considering the proposal to insert principles from Fuzzy Set Theory into classical grouping and classification approaches: c-Means algorithm and the Learning Vector neural model Quantization. The motivation for building a hybrid model in this category is to give classical approaches the ability to adequately deal with aspects of uncertainty and imprecision, commonly found in real problems.;;Universidade de São Paulo;pt_BR;Published;43;2010;2010-06-12 22:39:44
13764;Thiago Rocha;Tutorial sobre Fuzzy-c-Means e Fuzzy Learning Vector Quantization: Abordagens Híbridas para Tarefas de Agrupamento e Classificação;In this tutorial, a discussion is presented on the Fuzzy-c-Means algorithm and Fuzzy Neural Networks, considering the proposal to insert principles from Fuzzy Set Theory into classical grouping and classification approaches: c-Means algorithm and the Learning Vector neural model Quantization. The motivation for building a hybrid model in this category is to give classical approaches the ability to adequately deal with aspects of uncertainty and imprecision, commonly found in real problems.;;-;pt_BR;Published;43;2010;2010-06-12 22:39:44
13764;Helton H. Biscaro;Tutorial sobre Fuzzy-c-Means e Fuzzy Learning Vector Quantization: Abordagens Híbridas para Tarefas de Agrupamento e Classificação;In this tutorial, a discussion is presented on the Fuzzy-c-Means algorithm and Fuzzy Neural Networks, considering the proposal to insert principles from Fuzzy Set Theory into classical grouping and classification approaches: c-Means algorithm and the Learning Vector neural model Quantization. The motivation for building a hybrid model in this category is to give classical approaches the ability to adequately deal with aspects of uncertainty and imprecision, commonly found in real problems.;;-;pt_BR;Published;43;2010;2010-06-12 22:39:44
13764;Renata Cristina B. Madeo;Tutorial sobre Fuzzy-c-Means e Fuzzy Learning Vector Quantization: Abordagens Híbridas para Tarefas de Agrupamento e Classificação;In this tutorial, a discussion is presented on the Fuzzy-c-Means algorithm and Fuzzy Neural Networks, considering the proposal to insert principles from Fuzzy Set Theory into classical grouping and classification approaches: c-Means algorithm and the Learning Vector neural model Quantization. The motivation for building a hybrid model in this category is to give classical approaches the ability to adequately deal with aspects of uncertainty and imprecision, commonly found in real problems.;;-;pt_BR;Published;43;2010;2010-06-12 22:39:44
13764;Clodis Boscarioli;Tutorial sobre Fuzzy-c-Means e Fuzzy Learning Vector Quantization: Abordagens Híbridas para Tarefas de Agrupamento e Classificação;In this tutorial, a discussion is presented on the Fuzzy-c-Means algorithm and Fuzzy Neural Networks, considering the proposal to insert principles from Fuzzy Set Theory into classical grouping and classification approaches: c-Means algorithm and the Learning Vector neural model Quantization. The motivation for building a hybrid model in this category is to give classical approaches the ability to adequately deal with aspects of uncertainty and imprecision, commonly found in real problems.;;-;pt_BR;Published;43;2010;2010-06-12 22:39:44
15126;Sérgio Mariano Dias;Um Arcabouço para Desenvolvimento de Algoritmos da Análise Formal de Conceitos;This work presents a framework for the development of formal concept analysis (CFA) algorithms. Its main component is a generator of synthetic formal contexts whose parameters can be adjusted in order to allow testing the algorithms in situations of interest. Also present in the framework are some basic AFC algorithms and some formal contexts obtained from real databases, from relatively small to very large, the latter normally impractical in its original form. To exemplify the evaluation of algorithm performance via the framework, its basic algorithms are evaluated using synthetic and real data. Originally developed to serve as an instrument to aid in learning the main AFC algorithms, nothing prevents it from being used to evaluate new algorithms. For this, it is of particular importance that the parameters in data generation are adjustable in order to cover situations of interest.;;UFMG/SERPRO;pt_BR;Published;26;2010;2010-08-01 14:23:30
15126;Newton José Vieria;Um Arcabouço para Desenvolvimento de Algoritmos da Análise Formal de Conceitos;This work presents a framework for the development of formal concept analysis (CFA) algorithms. Its main component is a generator of synthetic formal contexts whose parameters can be adjusted in order to allow testing the algorithms in situations of interest. Also present in the framework are some basic AFC algorithms and some formal contexts obtained from real databases, from relatively small to very large, the latter normally impractical in its original form. To exemplify the evaluation of algorithm performance via the framework, its basic algorithms are evaluated using synthetic and real data. Originally developed to serve as an instrument to aid in learning the main AFC algorithms, nothing prevents it from being used to evaluate new algorithms. For this, it is of particular importance that the parameters in data generation are adjustable in order to cover situations of interest.;;UFMG;pt_BR;Published;26;2010;2010-08-01 14:23:30
15581;Angélica Alcoforado Mascaro;Detecção de Inclinação em Imagens de Documentos;Digitizing documents contributes to the preservation of information, preventing its loss due to the physical degradation of the paper. Currently, Automatic Document Image Recognition Systems are used to automatically convert the information contained in images into editable text, quickly and without the need for an individual to be present. Thus, making this information searchable through, for example, keywords. Slanting in documents is a frequent problem in these systems and, in general, is  imposed during scanning, when the paper is positioned at an angle other than zero degrees on the scanner axis. In the case of handwritten documents, slant can appear while writing the document itself, especially when the writer does not have an agenda line as a guide. Tilt correction is essential for the good performance of automatic recognition systems. This work addresses the problem of tilt detection in printed and handwritten documents, providing a review of the main methods for tilt detection published in the literature to date. The main techniques are exposed in a categorized way and advantages and limitations of each method are discussed.;;Universidade Federal de Pernambuco;pt_BR;Published;21;2010;2010-08-16 12:59:49
16413;Raquel de Miranda Barbosa;Usando CSP, RSL e o Modelo PopOrg na Especificação Formal de Organizações de SMAs;This article explores the use of traditional formal software engineering methods for the formal specification of multi-agent systems organizations. Particularly, the languages ​​CSP (Communicating Sequential Processes)) and RSL (Raise Specification Language) are used to represent the PopOrg organizational model, the first being used to specify parts of the micro-organizational level of PopOrg systems (organizational role behaviors and exchange processes between roles organizational) and the second, RSL, used to represent the structural organization of PopOrg systems. The article presents some tests and results obtained using these formalisms.;;Universidade Federal do Rio Grande do Sul - UFRGS;pt_BR;Published;22;2010;2010-09-23 14:14:44
16413;Antônio Carlos da Rocha CostA;Usando CSP, RSL e o Modelo PopOrg na Especificação Formal de Organizações de SMAs;This article explores the use of traditional formal software engineering methods for the formal specification of multi-agent systems organizations. Particularly, the languages ​​CSP (Communicating Sequential Processes)) and RSL (Raise Specification Language) are used to represent the PopOrg organizational model, the first being used to specify parts of the micro-organizational level of PopOrg systems (organizational role behaviors and exchange processes between roles organizational) and the second, RSL, used to represent the structural organization of PopOrg systems. The article presents some tests and results obtained using these formalisms.;;Centro de Ciências Computacionais, FURG;pt_BR;Published;22;2010;2010-09-23 14:14:44
16413;Patrícia Cabral de Azevedo R. Tedesco;Usando CSP, RSL e o Modelo PopOrg na Especificação Formal de Organizações de SMAs;This article explores the use of traditional formal software engineering methods for the formal specification of multi-agent systems organizations. Particularly, the languages ​​CSP (Communicating Sequential Processes)) and RSL (Raise Specification Language) are used to represent the PopOrg organizational model, the first being used to specify parts of the micro-organizational level of PopOrg systems (organizational role behaviors and exchange processes between roles organizational) and the second, RSL, used to represent the structural organization of PopOrg systems. The article presents some tests and results obtained using these formalisms.;;Centro de Informática, UFPE;pt_BR;Published;22;2010;2010-09-23 14:14:44
16413;Alexandre Cabral Mota;Usando CSP, RSL e o Modelo PopOrg na Especificação Formal de Organizações de SMAs;This article explores the use of traditional formal software engineering methods for the formal specification of multi-agent systems organizations. Particularly, the languages ​​CSP (Communicating Sequential Processes)) and RSL (Raise Specification Language) are used to represent the PopOrg organizational model, the first being used to specify parts of the micro-organizational level of PopOrg systems (organizational role behaviors and exchange processes between roles organizational) and the second, RSL, used to represent the structural organization of PopOrg systems. The article presents some tests and results obtained using these formalisms.;;Centro de Informática, UFPE;pt_BR;Published;22;2010;2010-09-23 14:14:44
16572;Giovani Parente Farias;Aplicação de Agentes BDI com Percepção Fuzzy em um Modelo Presa-Predador Fuzzy;This paper introduces a fuzzy perception model for BDI agents, with an application in a fuzzy version to the prey-predator problem, where the process of deciding which prey a predator should attack, based on comparing its strength with that of the prey, is based on its fuzzy perception mechanism. In this imperfect information environment, three types of predators are considered, one of them being the BDI agent with fuzzy perception. Several simulations were carried out to comparatively analyze the behaviors of different predator agents in two types of environments: with or without competition between predators.;;Universidade Federal do Rio Grande;pt_BR;Published;25;2010;2010-10-02 12:27:02
16572;Graçaliz Pereira Dimuro;Aplicação de Agentes BDI com Percepção Fuzzy em um Modelo Presa-Predador Fuzzy;This paper introduces a fuzzy perception model for BDI agents, with an application in a fuzzy version to the prey-predator problem, where the process of deciding which prey a predator should attack, based on comparing its strength with that of the prey, is based on its fuzzy perception mechanism. In this imperfect information environment, three types of predators are considered, one of them being the BDI agent with fuzzy perception. Several simulations were carried out to comparatively analyze the behaviors of different predator agents in two types of environments: with or without competition between predators.;;Universidade Federal do Rio Grande;pt_BR;Published;25;2010;2010-10-02 12:27:02
16572;Antônio Carlos Rocha Costa;Aplicação de Agentes BDI com Percepção Fuzzy em um Modelo Presa-Predador Fuzzy;This paper introduces a fuzzy perception model for BDI agents, with an application in a fuzzy version to the prey-predator problem, where the process of deciding which prey a predator should attack, based on comparing its strength with that of the prey, is based on its fuzzy perception mechanism. In this imperfect information environment, three types of predators are considered, one of them being the BDI agent with fuzzy perception. Several simulations were carried out to comparatively analyze the behaviors of different predator agents in two types of environments: with or without competition between predators.;;Universidade Federal do Rio Grande;pt_BR;Published;25;2010;2010-10-02 12:27:02
16730;Valdemar Vicente Graciano Neto;Um Sistema de Apoio à Decisão Baseado em Agentes para Tratamento de Ocorrências no Setor Elétrico;Companies that supply electricity and maintain distribution networks deal daily with repairing faults in the electrical network and allocating specialized vehicles to respond to incidents. In general, such companies have limited human resources to carry out these activities. This article presents an Intelligent Decision Support System (SADI) that aims to support these tasks by offering suggestions on how to distribute and allocate vehicles to carry out maintenance on the electrical grid. SADI's intelligence is implemented with agent technology, an ideal abstraction considering the distributed nature of the problem in question.;;Universidade Federal de Goiás;pt_BR;Published;14;2010;2010-10-15 10:03:19
16730;Ronneesley M. Teles;Um Sistema de Apoio à Decisão Baseado em Agentes para Tratamento de Ocorrências no Setor Elétrico;Companies that supply electricity and maintain distribution networks deal daily with repairing faults in the electrical network and allocating specialized vehicles to respond to incidents. In general, such companies have limited human resources to carry out these activities. This article presents an Intelligent Decision Support System (SADI) that aims to support these tasks by offering suggestions on how to distribute and allocate vehicles to carry out maintenance on the electrical grid. SADI's intelligence is implemented with agent technology, an ideal abstraction considering the distributed nature of the problem in question.;;Universidade Federal de Goiás;pt_BR;Published;14;2010;2010-10-15 10:03:19
16730;Marcos Ivamoto;Um Sistema de Apoio à Decisão Baseado em Agentes para Tratamento de Ocorrências no Setor Elétrico;Companies that supply electricity and maintain distribution networks deal daily with repairing faults in the electrical network and allocating specialized vehicles to respond to incidents. In general, such companies have limited human resources to carry out these activities. This article presents an Intelligent Decision Support System (SADI) that aims to support these tasks by offering suggestions on how to distribute and allocate vehicles to carry out maintenance on the electrical grid. SADI's intelligence is implemented with agent technology, an ideal abstraction considering the distributed nature of the problem in question.;;Universidade Federal de Goiás;pt_BR;Published;14;2010;2010-10-15 10:03:19
16730;Leonardo H. S. Mello;Um Sistema de Apoio à Decisão Baseado em Agentes para Tratamento de Ocorrências no Setor Elétrico;Companies that supply electricity and maintain distribution networks deal daily with repairing faults in the electrical network and allocating specialized vehicles to respond to incidents. In general, such companies have limited human resources to carry out these activities. This article presents an Intelligent Decision Support System (SADI) that aims to support these tasks by offering suggestions on how to distribute and allocate vehicles to carry out maintenance on the electrical grid. SADI's intelligence is implemented with agent technology, an ideal abstraction considering the distributed nature of the problem in question.;;Universidade Federal de Goiás;pt_BR;Published;14;2010;2010-10-15 10:03:19
16730;Cedric Luiz de Carvalho;Um Sistema de Apoio à Decisão Baseado em Agentes para Tratamento de Ocorrências no Setor Elétrico;Companies that supply electricity and maintain distribution networks deal daily with repairing faults in the electrical network and allocating specialized vehicles to respond to incidents. In general, such companies have limited human resources to carry out these activities. This article presents an Intelligent Decision Support System (SADI) that aims to support these tasks by offering suggestions on how to distribute and allocate vehicles to carry out maintenance on the electrical grid. SADI's intelligence is implemented with agent technology, an ideal abstraction considering the distributed nature of the problem in question.;;-;pt_BR;Published;14;2010;2010-10-15 10:03:19
16996;Gilberto Astolfi;Identifying people who are talking about the same topic in social networks, even having a different cultural background;Social media are assumed here as a blending of technology and social interaction for the co-creation of value, as in social networks. We discuss that in such environments, it is possible to find people with common interests that potentially can promote discussions, teaching and learning from each other. However, the approaches to recommend people to interact with in the social network still do not consider the users’ cultural background. This paper describes an approach to identify people who are talking about the same topic in social networks, even having a different cultural background, in order to introduce to each other and leverage interaction to exchange experience, knowledge, etc. Some tests’ results applying the proposal are presented. The approach was adopted in three different topics of discussion and the resultssuggest that the generated cultural understanding improves the chances of identifying people with similar interests.;;Universidade Federal de São Carlos;pt_BR;Published;14;2010;2010-10-29 19:14:09
16996;Vanessa Maia Aguiar Magalhaes;Identifying people who are talking about the same topic in social networks, even having a different cultural background;Social media are assumed here as a blending of technology and social interaction for the co-creation of value, as in social networks. We discuss that in such environments, it is possible to find people with common interests that potentially can promote discussions, teaching and learning from each other. However, the approaches to recommend people to interact with in the social network still do not consider the users’ cultural background. This paper describes an approach to identify people who are talking about the same topic in social networks, even having a different cultural background, in order to introduce to each other and leverage interaction to exchange experience, knowledge, etc. Some tests’ results applying the proposal are presented. The approach was adopted in three different topics of discussion and the resultssuggest that the generated cultural understanding improves the chances of identifying people with similar interests.;;Universidade Federal de São Carlos;pt_BR;Published;14;2010;2010-10-29 19:14:09
16996;Marcos Alexandre Rose Silva;Identifying people who are talking about the same topic in social networks, even having a different cultural background;Social media are assumed here as a blending of technology and social interaction for the co-creation of value, as in social networks. We discuss that in such environments, it is possible to find people with common interests that potentially can promote discussions, teaching and learning from each other. However, the approaches to recommend people to interact with in the social network still do not consider the users’ cultural background. This paper describes an approach to identify people who are talking about the same topic in social networks, even having a different cultural background, in order to introduce to each other and leverage interaction to exchange experience, knowledge, etc. Some tests’ results applying the proposal are presented. The approach was adopted in three different topics of discussion and the resultssuggest that the generated cultural understanding improves the chances of identifying people with similar interests.;;Universidade Federal de São Carlos;pt_BR;Published;14;2010;2010-10-29 19:14:09
16996;Junia Coutinho Anacleto;Identifying people who are talking about the same topic in social networks, even having a different cultural background;Social media are assumed here as a blending of technology and social interaction for the co-creation of value, as in social networks. We discuss that in such environments, it is possible to find people with common interests that potentially can promote discussions, teaching and learning from each other. However, the approaches to recommend people to interact with in the social network still do not consider the users’ cultural background. This paper describes an approach to identify people who are talking about the same topic in social networks, even having a different cultural background, in order to introduce to each other and leverage interaction to exchange experience, knowledge, etc. Some tests’ results applying the proposal are presented. The approach was adopted in three different topics of discussion and the resultssuggest that the generated cultural understanding improves the chances of identifying people with similar interests.;;Universidade Federal de São Carlos;pt_BR;Published;14;2010;2010-10-29 19:14:09
17679;Alana Marques Morais;Planejamento de um Serious Games Voltado para Saúde Bucal em Bebês;Serious games are computer games whose main characteristic is to teach specific aspects of disciplines or train operational and behavioral skills. These games combine the fun of conventional video games with: decision-making elements, assessment systems and pedagogical planning in order to create an engaging learning environment. These characteristics expand the possibilities for these games to operate and, therefore, attract the attention of other areas of knowledge. In this sense, the projects in partnership with healthcare professionals that have contributed significantly to the training, education and information of professionals and patients deserve to be highlighted. It is in this context of multidisciplinary serious games that this article presents the methodology used, based on the evaluation of the Communication Approach, in planning a game aimed at mothers, with the aim of teaching and informing about concepts related to babies' oral health. The term Communication Approach refers to a series of important information in the construction of the game, including: artistic conceptualization, type of narrative, script, plot, environment, interactivity, game genre, plot and gameplay.;;UFPB;pt_BR;Published;17;2010;2010-12-09 15:43:41
17679;Liliane dos Santos Machado;Planejamento de um Serious Games Voltado para Saúde Bucal em Bebês;Serious games are computer games whose main characteristic is to teach specific aspects of disciplines or train operational and behavioral skills. These games combine the fun of conventional video games with: decision-making elements, assessment systems and pedagogical planning in order to create an engaging learning environment. These characteristics expand the possibilities for these games to operate and, therefore, attract the attention of other areas of knowledge. In this sense, the projects in partnership with healthcare professionals that have contributed significantly to the training, education and information of professionals and patients deserve to be highlighted. It is in this context of multidisciplinary serious games that this article presents the methodology used, based on the evaluation of the Communication Approach, in planning a game aimed at mothers, with the aim of teaching and informing about concepts related to babies' oral health. The term Communication Approach refers to a series of important information in the construction of the game, including: artistic conceptualization, type of narrative, script, plot, environment, interactivity, game genre, plot and gameplay.;;UFPB;pt_BR;Published;17;2010;2010-12-09 15:43:41
17679;Ana Maria Gondim Valença;Planejamento de um Serious Games Voltado para Saúde Bucal em Bebês;Serious games are computer games whose main characteristic is to teach specific aspects of disciplines or train operational and behavioral skills. These games combine the fun of conventional video games with: decision-making elements, assessment systems and pedagogical planning in order to create an engaging learning environment. These characteristics expand the possibilities for these games to operate and, therefore, attract the attention of other areas of knowledge. In this sense, the projects in partnership with healthcare professionals that have contributed significantly to the training, education and information of professionals and patients deserve to be highlighted. It is in this context of multidisciplinary serious games that this article presents the methodology used, based on the evaluation of the Communication Approach, in planning a game aimed at mothers, with the aim of teaching and informing about concepts related to babies' oral health. The term Communication Approach refers to a series of important information in the construction of the game, including: artistic conceptualization, type of narrative, script, plot, environment, interactivity, game genre, plot and gameplay.;;UFPB;pt_BR;Published;17;2010;2010-12-09 15:43:41
18244;Alexandre Wagner Chagas Faria;Comparative Analyses of Power Consumption in Arithmetic Algorithms Implementation;Historically, energy management in computer science has been treated as an activity predominantly of hardware optimization. A great part of the effort on the area, even nowadays, is concerned in components activation, deactivation or resources scheduling to provide, as a final result, the reduction of total power consumption. This work is focused on the power consumption subject under the developer point of view, using a reliable power measurement framework, to validate the literature programming premises about programming options, as, for example, multiplication operations are high consuming in power energy. Besides some elementary operations and authors suggestions about alternatives for power consumption reduction on the programming stage, it was also compared two well used and known algorithms for big numbers multiplication, Karatsuba and Toom-Cook. The results lead to conclusions that would help the developer, in programming stage, to choose, in some cases, the best technique for reduction of power consumption, speed up the software developed, or take some decisions to limit the final software to be under some maximum power.;;UNIVERSIDADE FEDERAL DE MINAS GERAIS;pt_BR;Published;16;2011;2011-01-20 14:40:02
18244;Leandro Pfleger de Aguiar;Comparative Analyses of Power Consumption in Arithmetic Algorithms Implementation;Historically, energy management in computer science has been treated as an activity predominantly of hardware optimization. A great part of the effort on the area, even nowadays, is concerned in components activation, deactivation or resources scheduling to provide, as a final result, the reduction of total power consumption. This work is focused on the power consumption subject under the developer point of view, using a reliable power measurement framework, to validate the literature programming premises about programming options, as, for example, multiplication operations are high consuming in power energy. Besides some elementary operations and authors suggestions about alternatives for power consumption reduction on the programming stage, it was also compared two well used and known algorithms for big numbers multiplication, Karatsuba and Toom-Cook. The results lead to conclusions that would help the developer, in programming stage, to choose, in some cases, the best technique for reduction of power consumption, speed up the software developed, or take some decisions to limit the final software to be under some maximum power.;;UNIVERSIDADE FEDERAL DE MINAS GERAIS;pt_BR;Published;16;2011;2011-01-20 14:40:02
18244;Daniel da Silva Diogo Lara;Comparative Analyses of Power Consumption in Arithmetic Algorithms Implementation;Historically, energy management in computer science has been treated as an activity predominantly of hardware optimization. A great part of the effort on the area, even nowadays, is concerned in components activation, deactivation or resources scheduling to provide, as a final result, the reduction of total power consumption. This work is focused on the power consumption subject under the developer point of view, using a reliable power measurement framework, to validate the literature programming premises about programming options, as, for example, multiplication operations are high consuming in power energy. Besides some elementary operations and authors suggestions about alternatives for power consumption reduction on the programming stage, it was also compared two well used and known algorithms for big numbers multiplication, Karatsuba and Toom-Cook. The results lead to conclusions that would help the developer, in programming stage, to choose, in some cases, the best technique for reduction of power consumption, speed up the software developed, or take some decisions to limit the final software to be under some maximum power.;;Universidade Federal de Minas Gerais;pt_BR;Published;16;2011;2011-01-20 14:40:02
18244;Antônio Alfredo Ferreira Loureiro;Comparative Analyses of Power Consumption in Arithmetic Algorithms Implementation;Historically, energy management in computer science has been treated as an activity predominantly of hardware optimization. A great part of the effort on the area, even nowadays, is concerned in components activation, deactivation or resources scheduling to provide, as a final result, the reduction of total power consumption. This work is focused on the power consumption subject under the developer point of view, using a reliable power measurement framework, to validate the literature programming premises about programming options, as, for example, multiplication operations are high consuming in power energy. Besides some elementary operations and authors suggestions about alternatives for power consumption reduction on the programming stage, it was also compared two well used and known algorithms for big numbers multiplication, Karatsuba and Toom-Cook. The results lead to conclusions that would help the developer, in programming stage, to choose, in some cases, the best technique for reduction of power consumption, speed up the software developed, or take some decisions to limit the final software to be under some maximum power.;;Universidade Federal de Minas Gerais;pt_BR;Published;16;2011;2011-01-20 14:40:02
19730;Fabiano Azevedo Dorça;Detecção e Correção Automática de Estilos de Aprendizagem em Sistemas Adaptativos para Educação;One of the most important aspects in adaptive systems for education is the ability to provide personalization according to the specific needs of each student. In this context, this work presents a promising approach for detection and automatic correction of learning styles (EA) based on Markov chains. Most works in this area present complex and inefficient approaches in some aspect. Furthermore, the approach presented in this work has the advantage of making it possible for students to develop new cognitive capabilities, being based on the combination of learning styles (CEA) and the dynamic correction of possible inconsistencies in the student model (ME), taking into account the strong non-deterministic aspect of the teaching-learning process. Promising results were obtained in tests carried out with this approach and are discussed in this work.;;Universidade Federal de Uberlândia;pt_BR;Published;26;2011;2011-04-13 17:38:47
19730;Luciano Vieira Lima;Detecção e Correção Automática de Estilos de Aprendizagem em Sistemas Adaptativos para Educação;One of the most important aspects in adaptive systems for education is the ability to provide personalization according to the specific needs of each student. In this context, this work presents a promising approach for detection and automatic correction of learning styles (EA) based on Markov chains. Most works in this area present complex and inefficient approaches in some aspect. Furthermore, the approach presented in this work has the advantage of making it possible for students to develop new cognitive capabilities, being based on the combination of learning styles (CEA) and the dynamic correction of possible inconsistencies in the student model (ME), taking into account the strong non-deterministic aspect of the teaching-learning process. Promising results were obtained in tests carried out with this approach and are discussed in this work.;;Faculdade de Engenharia Elétrica, Universidade Federal de Uberlândia;pt_BR;Published;26;2011;2011-04-13 17:38:47
19730;Márcia Aparecida Fernandes;Detecção e Correção Automática de Estilos de Aprendizagem em Sistemas Adaptativos para Educação;One of the most important aspects in adaptive systems for education is the ability to provide personalization according to the specific needs of each student. In this context, this work presents a promising approach for detection and automatic correction of learning styles (EA) based on Markov chains. Most works in this area present complex and inefficient approaches in some aspect. Furthermore, the approach presented in this work has the advantage of making it possible for students to develop new cognitive capabilities, being based on the combination of learning styles (CEA) and the dynamic correction of possible inconsistencies in the student model (ME), taking into account the strong non-deterministic aspect of the teaching-learning process. Promising results were obtained in tests carried out with this approach and are discussed in this work.;;Faculdade de Computação, Universidade Federal de Uberlândia;pt_BR;Published;26;2011;2011-04-13 17:38:47
19730;Carlos Roberto Lopes;Detecção e Correção Automática de Estilos de Aprendizagem em Sistemas Adaptativos para Educação;One of the most important aspects in adaptive systems for education is the ability to provide personalization according to the specific needs of each student. In this context, this work presents a promising approach for detection and automatic correction of learning styles (EA) based on Markov chains. Most works in this area present complex and inefficient approaches in some aspect. Furthermore, the approach presented in this work has the advantage of making it possible for students to develop new cognitive capabilities, being based on the combination of learning styles (CEA) and the dynamic correction of possible inconsistencies in the student model (ME), taking into account the strong non-deterministic aspect of the teaching-learning process. Promising results were obtained in tests carried out with this approach and are discussed in this work.;;Faculdade de Computação, Universidade Federal de Uberlândia;pt_BR;Published;26;2011;2011-04-13 17:38:47
20020;Alencar Machado;Sistema Pervasivo de Informação em Saúde Projetado para ser Programado pelo Usuário Clínico;Pervasive Computing is part of the development of programmable and interactive environments, in the quest to help users in their daily activities. Considering that the health system of the future foresees the use of Pervasive Computing to optimize and automate clinical activities, this article presents the ClinicSpace architecture, demonstrating through a problem situation the structure of the architecture to assist clinical tasks (pervasive applications that help the doctor to carry out his activities), seeking to fulfill the requirements of activity-oriented computing. Thus, ClinicSpace can be seen as a context-aware system oriented to clinical tasks.;;Universidade Federal de Santa Maria;pt_BR;Published;15;2011;2011-04-26 11:38:41
20020;Iara Augustin;Sistema Pervasivo de Informação em Saúde Projetado para ser Programado pelo Usuário Clínico;Pervasive Computing is part of the development of programmable and interactive environments, in the quest to help users in their daily activities. Considering that the health system of the future foresees the use of Pervasive Computing to optimize and automate clinical activities, this article presents the ClinicSpace architecture, demonstrating through a problem situation the structure of the architecture to assist clinical tasks (pervasive applications that help the doctor to carry out his activities), seeking to fulfill the requirements of activity-oriented computing. Thus, ClinicSpace can be seen as a context-aware system oriented to clinical tasks.;;Universidade Federal de Santa Maria;pt_BR;Published;15;2011;2011-04-26 11:38:41
20688;Roni Fabio Banaszewski;Protocolo de Leilões Simultâneos com Escalonamento: Aplicação ao Problema de Planejamento de Movimentações de Derivados de Petróleo;In petroleum industry supply chains, meeting demand and maintaining an adequate inventory level for various classes of petroleum products is a crucial issue. Basically, this type of chain is made up of several elements, such as production bases, consumer bases and intermediate terminals, which are connected through a multimodal transport network to provide the movement of products between the bases. These elements must cooperate to achieve a feasible solution with a low processing time. To achieve this objective, this article presents a multi-agent auction protocol for simultaneous negotiations with a scheduling element. This protocol was applied to the aforementioned chain, behaving efficiently by returning a feasible solution in a low processing time.;;Universidade Tecnológica Federal do Paraná - UTFPR;pt_BR;Published;24;2011;2011-06-05 19:53:41
20688;Kelvin Elton Nogueira;Protocolo de Leilões Simultâneos com Escalonamento: Aplicação ao Problema de Planejamento de Movimentações de Derivados de Petróleo;In petroleum industry supply chains, meeting demand and maintaining an adequate inventory level for various classes of petroleum products is a crucial issue. Basically, this type of chain is made up of several elements, such as production bases, consumer bases and intermediate terminals, which are connected through a multimodal transport network to provide the movement of products between the bases. These elements must cooperate to achieve a feasible solution with a low processing time. To achieve this objective, this article presents a multi-agent auction protocol for simultaneous negotiations with a scheduling element. This protocol was applied to the aforementioned chain, behaving efficiently by returning a feasible solution in a low processing time.;;Universidade Tecnológica Federal do Paraná (UTFPR);pt_BR;Published;24;2011;2011-06-05 19:53:41
20688;Jean Marcelo Simão;Protocolo de Leilões Simultâneos com Escalonamento: Aplicação ao Problema de Planejamento de Movimentações de Derivados de Petróleo;In petroleum industry supply chains, meeting demand and maintaining an adequate inventory level for various classes of petroleum products is a crucial issue. Basically, this type of chain is made up of several elements, such as production bases, consumer bases and intermediate terminals, which are connected through a multimodal transport network to provide the movement of products between the bases. These elements must cooperate to achieve a feasible solution with a low processing time. To achieve this objective, this article presents a multi-agent auction protocol for simultaneous negotiations with a scheduling element. This protocol was applied to the aforementioned chain, behaving efficiently by returning a feasible solution in a low processing time.;;Universidade Tecnológica Federal do Paraná (UTFPR);pt_BR;Published;24;2011;2011-06-05 19:53:41
20688;Lúcia Valéria de Arruda;Protocolo de Leilões Simultâneos com Escalonamento: Aplicação ao Problema de Planejamento de Movimentações de Derivados de Petróleo;In petroleum industry supply chains, meeting demand and maintaining an adequate inventory level for various classes of petroleum products is a crucial issue. Basically, this type of chain is made up of several elements, such as production bases, consumer bases and intermediate terminals, which are connected through a multimodal transport network to provide the movement of products between the bases. These elements must cooperate to achieve a feasible solution with a low processing time. To achieve this objective, this article presents a multi-agent auction protocol for simultaneous negotiations with a scheduling element. This protocol was applied to the aforementioned chain, behaving efficiently by returning a feasible solution in a low processing time.;;Universidade Tecnológica Federal do Paraná (UTFPR);pt_BR;Published;24;2011;2011-06-05 19:53:41
20688;Cesar Augusto Tacla;Protocolo de Leilões Simultâneos com Escalonamento: Aplicação ao Problema de Planejamento de Movimentações de Derivados de Petróleo;In petroleum industry supply chains, meeting demand and maintaining an adequate inventory level for various classes of petroleum products is a crucial issue. Basically, this type of chain is made up of several elements, such as production bases, consumer bases and intermediate terminals, which are connected through a multimodal transport network to provide the movement of products between the bases. These elements must cooperate to achieve a feasible solution with a low processing time. To achieve this objective, this article presents a multi-agent auction protocol for simultaneous negotiations with a scheduling element. This protocol was applied to the aforementioned chain, behaving efficiently by returning a feasible solution in a low processing time.;;Universidade Tecnológica Federal do Paraná (UTFPR);pt_BR;Published;24;2011;2011-06-05 19:53:41
20816;Vagner Francisco Le Roy;Utilização do Caminhamento Aleatório na Identificação de Características de Documentos na Língua Portuguesa;Due to the large volume of stored texts, the text mining area has been the focus of numerous research aimed at automatic document classification. The present work aims to evaluate the random walk method in defining the weights of terms in Portuguese texts. This technique uses the co-occurrence of terms as a measure of dependence between the characteristics of words. An undirected graph is used, with the score of each vertex being calculated depending on the probability of being found. The results obtained with random walking were compared with those presented by traditional techniques, and demonstrated that the random walking method proved to be quite effective in the document classification process.;;Centro Universitário de Belo Horizonte- UNI-BH;pt_BR;Published;14;2011;2011-06-14 15:17:11
20816;Ana Paula Ladeira;Utilização do Caminhamento Aleatório na Identificação de Características de Documentos na Língua Portuguesa;Due to the large volume of stored texts, the text mining area has been the focus of numerous research aimed at automatic document classification. The present work aims to evaluate the random walk method in defining the weights of terms in Portuguese texts. This technique uses the co-occurrence of terms as a measure of dependence between the characteristics of words. An undirected graph is used, with the score of each vertex being calculated depending on the probability of being found. The results obtained with random walking were compared with those presented by traditional techniques, and demonstrated that the random walking method proved to be quite effective in the document classification process.;;-;pt_BR;Published;14;2011;2011-06-14 15:17:11
21062;Saulo Popov Zambiasi;Uma Arquitetura Aberta e Orientada a Serviços para Softwares Assistentes Pessoais;Several efforts have been made towards personal assistant software (SAP) with the aim of helping people in their daily activities, at home or at work. Despite the intrinsic complexity that SAPs can have, most work is concerned with developing solutions only for very specific tasks, without concerns about integration and interoperation with other systems (including other SAPs), in addition to not connecting with business systems and respective business processes. Aiming to meet these requirements, this work presents an open reference architecture for SAPs, allowing interoperable instances to be derived, customized, implemented and deployed depending on the characteristics of the organizations' people and processes. An instance was generated and its results analyzed.;;Universidade Federal de Santa Catarina - UFSC;pt_BR;Published;26;2011;2011-06-28 14:35:00
21062;Ricardo J. Rabelo;Uma Arquitetura Aberta e Orientada a Serviços para Softwares Assistentes Pessoais;Several efforts have been made towards personal assistant software (SAP) with the aim of helping people in their daily activities, at home or at work. Despite the intrinsic complexity that SAPs can have, most work is concerned with developing solutions only for very specific tasks, without concerns about integration and interoperation with other systems (including other SAPs), in addition to not connecting with business systems and respective business processes. Aiming to meet these requirements, this work presents an open reference architecture for SAPs, allowing interoperable instances to be derived, customized, implemented and deployed depending on the characteristics of the organizations' people and processes. An instance was generated and its results analyzed.;;-;pt_BR;Published;26;2011;2011-06-28 14:35:00
21506;André Hahn Pereira;LTI Agent Rescue: A Partial Global Approach for Task Allocation in the RoboCup Rescue;Coordination is one of the key issues in cooperative multiagent systems and it also plays an essential role in disaster management. Task allocation is an important phase of the coordination problem, since the decomposition of the objective into tasks is the most natural way to organize work among agents. In this paper, we propose a hybrid task allocation approach that considers the existence of both local and global information in order to coordinate the agents in the RoboCup Rescue. Moreover, we present a comparative analysis between the results obtained by the agents that use the proposed approach and ones obtained by the sample agents provided within the RoboCup Rescue simulator, that follow a decentralized and greedy approach.;;Universidade de São Paulo;pt_BR;Published;21;2011;2011-07-15 17:21:46
21506;Luis Gustavo Nardin;LTI Agent Rescue: A Partial Global Approach for Task Allocation in the RoboCup Rescue;Coordination is one of the key issues in cooperative multiagent systems and it also plays an essential role in disaster management. Task allocation is an important phase of the coordination problem, since the decomposition of the objective into tasks is the most natural way to organize work among agents. In this paper, we propose a hybrid task allocation approach that considers the existence of both local and global information in order to coordinate the agents in the RoboCup Rescue. Moreover, we present a comparative analysis between the results obtained by the agents that use the proposed approach and ones obtained by the sample agents provided within the RoboCup Rescue simulator, that follow a decentralized and greedy approach.;;Universidade de São Paulo;pt_BR;Published;21;2011;2011-07-15 17:21:46
21506;Jaime Simão Sichman;LTI Agent Rescue: A Partial Global Approach for Task Allocation in the RoboCup Rescue;Coordination is one of the key issues in cooperative multiagent systems and it also plays an essential role in disaster management. Task allocation is an important phase of the coordination problem, since the decomposition of the objective into tasks is the most natural way to organize work among agents. In this paper, we propose a hybrid task allocation approach that considers the existence of both local and global information in order to coordinate the agents in the RoboCup Rescue. Moreover, we present a comparative analysis between the results obtained by the agents that use the proposed approach and ones obtained by the sample agents provided within the RoboCup Rescue simulator, that follow a decentralized and greedy approach.;;Universidade de São Paulo;pt_BR;Published;21;2011;2011-07-15 17:21:46
21979;Diogo Fernando Trevisan;Estudo do Padrão Avançado de Criptografia AES – Advanced Encryption Standard;This work portrays the implementation of the Advanced Encryption Standard (AES) cryptographic algorithm. The choice of the AES Algorithm is due to the fact that it is the current advanced encryption standard being selected after a long competition in which the various algorithms were cryptanalyzed by the entire cryptology community. In addition to its efficiency, AES was also designed to allow key expansion when necessary, be implemented at both software and hardware levels and is freely available, which allows its use in different applications without the need for payment. of royalties.;;Universidade Federal da Grande Dourados;pt_BR;Published;11;2011;2011-07-27 19:54:14
21979;Rodrigo P. da Silva Sacchi;Estudo do Padrão Avançado de Criptografia AES – Advanced Encryption Standard;This work portrays the implementation of the Advanced Encryption Standard (AES) cryptographic algorithm. The choice of the AES Algorithm is due to the fact that it is the current advanced encryption standard being selected after a long competition in which the various algorithms were cryptanalyzed by the entire cryptology community. In addition to its efficiency, AES was also designed to allow key expansion when necessary, be implemented at both software and hardware levels and is freely available, which allows its use in different applications without the need for payment. of royalties.;;Universidade Federal da Grande Dourados, UFGD;pt_BR;Published;11;2011;2011-07-27 19:54:14
21979;Lino Sanabria;Estudo do Padrão Avançado de Criptografia AES – Advanced Encryption Standard;This work portrays the implementation of the Advanced Encryption Standard (AES) cryptographic algorithm. The choice of the AES Algorithm is due to the fact that it is the current advanced encryption standard being selected after a long competition in which the various algorithms were cryptanalyzed by the entire cryptology community. In addition to its efficiency, AES was also designed to allow key expansion when necessary, be implemented at both software and hardware levels and is freely available, which allows its use in different applications without the need for payment. of royalties.;;Universidade Federal da Grande Dourados, UFGD;pt_BR;Published;11;2011;2011-07-27 19:54:14
23471;Luciano Antonio Digiampietri;Combinando Workflows e Semântica para Facilitar o Reuso de Software;This article presents an environment for software development based on component reuse, which combines workflow management technologies, semantics and software components. The developed environment can be used by software developers, workflow designers and end users. A real case study is presented to demonstrate the use of the environment.;;Universidade de São Paulo;pt_BR;Published;16;2011;2011-10-03 12:50:55
23471;Jônatas Correa Araújo;Combinando Workflows e Semântica para Facilitar o Reuso de Software;This article presents an environment for software development based on component reuse, which combines workflow management technologies, semantics and software components. The developed environment can be used by software developers, workflow designers and end users. A real case study is presented to demonstrate the use of the environment.;;Universidade de São Paulo;pt_BR;Published;16;2011;2011-10-03 12:50:55
23471;Éric Hainer Ostroski;Combinando Workflows e Semântica para Facilitar o Reuso de Software;This article presents an environment for software development based on component reuse, which combines workflow management technologies, semantics and software components. The developed environment can be used by software developers, workflow designers and end users. A real case study is presented to demonstrate the use of the environment.;;Universidade de São Paulo;pt_BR;Published;16;2011;2011-10-03 12:50:55
23471;Caio Rafael Nascimento Santiago;Combinando Workflows e Semântica para Facilitar o Reuso de Software;This article presents an environment for software development based on component reuse, which combines workflow management technologies, semantics and software components. The developed environment can be used by software developers, workflow designers and end users. A real case study is presented to demonstrate the use of the environment.;;Universidade de São Paulo;pt_BR;Published;16;2011;2011-10-03 12:50:55
23471;José de Jésus Pérez Alcázar;Combinando Workflows e Semântica para Facilitar o Reuso de Software;This article presents an environment for software development based on component reuse, which combines workflow management technologies, semantics and software components. The developed environment can be used by software developers, workflow designers and end users. A real case study is presented to demonstrate the use of the environment.;;Universidade de São Paulo;pt_BR;Published;16;2011;2011-10-03 12:50:55
25070;Cássio Martini Martins Pereira;Common Dissimilarity Measures are Inappropriate for Time Series Clustering;Clustering algorithms have been actively used to identify similar timeseries, providing a better understanding of data. However, common clustering dis-similarity measures disregard time series correlations, yielding poor results. In thispaper, we introduce a dissimilarity measure based on series partial autocorrelations.Experiments compare hierarchical clustering algorithms using the common dissimi-larity measures, such as Euclidean Distance and Dynamic Time Warping, to clustertime series following Box-Jenkins Auto-Regressive models. Results show that ourdissimilarity measure produces better results for both synthetic and real data sets interms of the Adjusted Rand Index and Normalized Hubert Γ statistic. Our findingsconfirm that the choice of dissimilarity measure is crucial for improving time seriesclustering quality.;;Instituto de Ciências Matemáticas e de Computação - ICMC - USP;pt_BR;Published;23;2011;2011-12-29 11:46:39
25070;Rodrigo F. de Mello;Common Dissimilarity Measures are Inappropriate for Time Series Clustering;Clustering algorithms have been actively used to identify similar timeseries, providing a better understanding of data. However, common clustering dis-similarity measures disregard time series correlations, yielding poor results. In thispaper, we introduce a dissimilarity measure based on series partial autocorrelations.Experiments compare hierarchical clustering algorithms using the common dissimi-larity measures, such as Euclidean Distance and Dynamic Time Warping, to clustertime series following Box-Jenkins Auto-Regressive models. Results show that ourdissimilarity measure produces better results for both synthetic and real data sets interms of the Adjusted Rand Index and Normalized Hubert Γ statistic. Our findingsconfirm that the choice of dissimilarity measure is crucial for improving time seriesclustering quality.;;Universidade de São Paulo;pt_BR;Published;23;2011;2011-12-29 11:46:39
25202;Luciana Foss;From UML to SIMULINK CAAM: Formal Specification and Transformation Analysis;UML and Simulink are attractive languages for embedded systems design and modeling. An automatic mapping from UML models to Simulink would be an interesting resource in a seamless design flow, allowing designers to use UML asmodeling language for the whole system and at same time to use facilities for code generation based on Simulink. In a previous work, the UML to Simulink translation was prototyped using a Java implementation. In this paper, we present the formal definition of this translation using graph grammars, as well as its automation, which is supported by the AGG system. With the formalization of the metamodels and translation rules, we can guarantee the correctness of the translation. We also illustrate theeffectiveness of our methodology by means of a case study.;;Universidade Federal de Pelotas;pt_BR;Published;37;2012;2012-01-05 14:38:50
25202;Simone André da Costa Cavalheiro;From UML to SIMULINK CAAM: Formal Specification and Transformation Analysis;UML and Simulink are attractive languages for embedded systems design and modeling. An automatic mapping from UML models to Simulink would be an interesting resource in a seamless design flow, allowing designers to use UML asmodeling language for the whole system and at same time to use facilities for code generation based on Simulink. In a previous work, the UML to Simulink translation was prototyped using a Java implementation. In this paper, we present the formal definition of this translation using graph grammars, as well as its automation, which is supported by the AGG system. With the formalization of the metamodels and translation rules, we can guarantee the correctness of the translation. We also illustrate theeffectiveness of our methodology by means of a case study.;;Universidade Federal de Pelotas;pt_BR;Published;37;2012;2012-01-05 14:38:50
25202;Nícolas Nogueira Bisi;From UML to SIMULINK CAAM: Formal Specification and Transformation Analysis;UML and Simulink are attractive languages for embedded systems design and modeling. An automatic mapping from UML models to Simulink would be an interesting resource in a seamless design flow, allowing designers to use UML asmodeling language for the whole system and at same time to use facilities for code generation based on Simulink. In a previous work, the UML to Simulink translation was prototyped using a Java implementation. In this paper, we present the formal definition of this translation using graph grammars, as well as its automation, which is supported by the AGG system. With the formalization of the metamodels and translation rules, we can guarantee the correctness of the translation. We also illustrate theeffectiveness of our methodology by means of a case study.;;Universidade Federal de Pelotas;pt_BR;Published;37;2012;2012-01-05 14:38:50
25202;Vinícius Steffens Pazzini;From UML to SIMULINK CAAM: Formal Specification and Transformation Analysis;UML and Simulink are attractive languages for embedded systems design and modeling. An automatic mapping from UML models to Simulink would be an interesting resource in a seamless design flow, allowing designers to use UML asmodeling language for the whole system and at same time to use facilities for code generation based on Simulink. In a previous work, the UML to Simulink translation was prototyped using a Java implementation. In this paper, we present the formal definition of this translation using graph grammars, as well as its automation, which is supported by the AGG system. With the formalization of the metamodels and translation rules, we can guarantee the correctness of the translation. We also illustrate theeffectiveness of our methodology by means of a case study.;;Universidade Federal de Pelotas;pt_BR;Published;37;2012;2012-01-05 14:38:50
25202;Lisane Brisolara de Brisolara;From UML to SIMULINK CAAM: Formal Specification and Transformation Analysis;UML and Simulink are attractive languages for embedded systems design and modeling. An automatic mapping from UML models to Simulink would be an interesting resource in a seamless design flow, allowing designers to use UML asmodeling language for the whole system and at same time to use facilities for code generation based on Simulink. In a previous work, the UML to Simulink translation was prototyped using a Java implementation. In this paper, we present the formal definition of this translation using graph grammars, as well as its automation, which is supported by the AGG system. With the formalization of the metamodels and translation rules, we can guarantee the correctness of the translation. We also illustrate theeffectiveness of our methodology by means of a case study.;;Universidade Federal do Rio Grande do Sul;pt_BR;Published;37;2012;2012-01-05 14:38:50
25202;Flávio Rech Wagner;From UML to SIMULINK CAAM: Formal Specification and Transformation Analysis;UML and Simulink are attractive languages for embedded systems design and modeling. An automatic mapping from UML models to Simulink would be an interesting resource in a seamless design flow, allowing designers to use UML asmodeling language for the whole system and at same time to use facilities for code generation based on Simulink. In a previous work, the UML to Simulink translation was prototyped using a Java implementation. In this paper, we present the formal definition of this translation using graph grammars, as well as its automation, which is supported by the AGG system. With the formalization of the metamodels and translation rules, we can guarantee the correctness of the translation. We also illustrate theeffectiveness of our methodology by means of a case study.;;-;pt_BR;Published;37;2012;2012-01-05 14:38:50
25210;Cícero Augusto de S. Camargo;A Graph Grammar to Transform a Dataflow Graph into a Multithread Graph and its Application in Task Scheduling;The scheduling of tasks in a parallel program is an NP-complete problem, where scheduling tasks over multiple processing units requires an effective strategy to maximize the exploitation of the parallel hardware. Several studies focus on the scheduling of parallel programs described into DAGs (Directed Acyclic Graphs). However, this representation does not describe a multithreaded program suitably. This paper shows the structure and semantics of a DCG, an abstraction which describes a multithreaded program, and proposes standards to map structures found in DAGs into segments of a DCG. A graph grammar has been developed to perform the proposed transformation and case studies using DAGs found in the literature validate the transformation process. Besides the automatic translation and precise definition of the mapping, the use of a formal language also allowed the verification of the existence and uniqueness of the out coming model. ;;Universidade Federal de Pelotas;pt_BR;Published;39;2012;2012-01-05 21:30:13
25210;Simone André da Costa Cavalheiro;A Graph Grammar to Transform a Dataflow Graph into a Multithread Graph and its Application in Task Scheduling;The scheduling of tasks in a parallel program is an NP-complete problem, where scheduling tasks over multiple processing units requires an effective strategy to maximize the exploitation of the parallel hardware. Several studies focus on the scheduling of parallel programs described into DAGs (Directed Acyclic Graphs). However, this representation does not describe a multithreaded program suitably. This paper shows the structure and semantics of a DCG, an abstraction which describes a multithreaded program, and proposes standards to map structures found in DAGs into segments of a DCG. A graph grammar has been developed to perform the proposed transformation and case studies using DAGs found in the literature validate the transformation process. Besides the automatic translation and precise definition of the mapping, the use of a formal language also allowed the verification of the existence and uniqueness of the out coming model. ;;Universidade Federal de Pelotas;pt_BR;Published;39;2012;2012-01-05 21:30:13
25210;Luciana Foss;A Graph Grammar to Transform a Dataflow Graph into a Multithread Graph and its Application in Task Scheduling;The scheduling of tasks in a parallel program is an NP-complete problem, where scheduling tasks over multiple processing units requires an effective strategy to maximize the exploitation of the parallel hardware. Several studies focus on the scheduling of parallel programs described into DAGs (Directed Acyclic Graphs). However, this representation does not describe a multithreaded program suitably. This paper shows the structure and semantics of a DCG, an abstraction which describes a multithreaded program, and proposes standards to map structures found in DAGs into segments of a DCG. A graph grammar has been developed to perform the proposed transformation and case studies using DAGs found in the literature validate the transformation process. Besides the automatic translation and precise definition of the mapping, the use of a formal language also allowed the verification of the existence and uniqueness of the out coming model. ;;Universidade Federal de Pelotas;pt_BR;Published;39;2012;2012-01-05 21:30:13
25210;Gerson Geraldo H. Cavalheiro;A Graph Grammar to Transform a Dataflow Graph into a Multithread Graph and its Application in Task Scheduling;The scheduling of tasks in a parallel program is an NP-complete problem, where scheduling tasks over multiple processing units requires an effective strategy to maximize the exploitation of the parallel hardware. Several studies focus on the scheduling of parallel programs described into DAGs (Directed Acyclic Graphs). However, this representation does not describe a multithreaded program suitably. This paper shows the structure and semantics of a DCG, an abstraction which describes a multithreaded program, and proposes standards to map structures found in DAGs into segments of a DCG. A graph grammar has been developed to perform the proposed transformation and case studies using DAGs found in the literature validate the transformation process. Besides the automatic translation and precise definition of the mapping, the use of a formal language also allowed the verification of the existence and uniqueness of the out coming model. ;;Universidade Federal de Pelotas;pt_BR;Published;39;2012;2012-01-05 21:30:13
25515;Lilian Ribeiro Mendes Paiva;Métodos Computacionais e Estatísticos no Estudo das Ondas de Frequência dos Sinais Eletroencefalográficos e o Envelhecimento: uma Abordagem Através da Análise Linear do Discriminante;The Central Nervous System (CNS) and neurological signals carry information that represents changes throughout life. This work seeks to establish some correlation between brain activity as a function of age, based on the recording of electroencephalographic (EEG) signals, in subjects without neurological disorders, during the practice of a certain task. 59 healthy subjects voluntarily participated in this study, divided into 7 groups, aged between 20 and 86 years old and of both sexes. EEG signals were collected in three different experimental protocols during the execution of the Archimedes Spiral. The electrodes were positioned according to the international 10/20 standard, using the C3 and C4 channels in the central region. Statistical analyzes were performed to identify differences between each group. Data were processed in MATLAB software. Among the results obtained, significant differences were observed, through the LDA value. The tool satisfactorily performed the separation of discriminating characteristics, classifying each group of individuals that present a high correlation depending on age. It can be concluded from the analysis of the characteristics used that there is separability between the groups according to age group, contributing significantly to recording the changes that occur during the aging process.;;Universidade Federal de Uberlândia (UFU);pt_BR;Published;20;2012;2012-01-24 21:27:33
25515;Adriano Alves Pereira;Métodos Computacionais e Estatísticos no Estudo das Ondas de Frequência dos Sinais Eletroencefalográficos e o Envelhecimento: uma Abordagem Através da Análise Linear do Discriminante;The Central Nervous System (CNS) and neurological signals carry information that represents changes throughout life. This work seeks to establish some correlation between brain activity as a function of age, based on the recording of electroencephalographic (EEG) signals, in subjects without neurological disorders, during the practice of a certain task. 59 healthy subjects voluntarily participated in this study, divided into 7 groups, aged between 20 and 86 years old and of both sexes. EEG signals were collected in three different experimental protocols during the execution of the Archimedes Spiral. The electrodes were positioned according to the international 10/20 standard, using the C3 and C4 channels in the central region. Statistical analyzes were performed to identify differences between each group. Data were processed in MATLAB software. Among the results obtained, significant differences were observed, through the LDA value. The tool satisfactorily performed the separation of discriminating characteristics, classifying each group of individuals that present a high correlation depending on age. It can be concluded from the analysis of the characteristics used that there is separability between the groups according to age group, contributing significantly to recording the changes that occur during the aging process.;;Universidade Federal de Uberlândia (UFU);pt_BR;Published;20;2012;2012-01-24 21:27:33
25515;Adriano de Oliveira Andrade;Métodos Computacionais e Estatísticos no Estudo das Ondas de Frequência dos Sinais Eletroencefalográficos e o Envelhecimento: uma Abordagem Através da Análise Linear do Discriminante;The Central Nervous System (CNS) and neurological signals carry information that represents changes throughout life. This work seeks to establish some correlation between brain activity as a function of age, based on the recording of electroencephalographic (EEG) signals, in subjects without neurological disorders, during the practice of a certain task. 59 healthy subjects voluntarily participated in this study, divided into 7 groups, aged between 20 and 86 years old and of both sexes. EEG signals were collected in three different experimental protocols during the execution of the Archimedes Spiral. The electrodes were positioned according to the international 10/20 standard, using the C3 and C4 channels in the central region. Statistical analyzes were performed to identify differences between each group. Data were processed in MATLAB software. Among the results obtained, significant differences were observed, through the LDA value. The tool satisfactorily performed the separation of discriminating characteristics, classifying each group of individuals that present a high correlation depending on age. It can be concluded from the analysis of the characteristics used that there is separability between the groups according to age group, contributing significantly to recording the changes that occur during the aging process.;;Universidade Federal de Uberlândia (UFU);pt_BR;Published;20;2012;2012-01-24 21:27:33
25646;Francisco Gomes Oliveira;Seleção Automática de Casos de Teste de Regressão Baseada em Similaridade e Valores;Selective re-testing techniques aim to assist in the selection of regression test cases based on the modifications made. This work presents a new technique: Weighted Similarity Approach for Regression Testing (WSA-RT). This technique uses a testing approach based on models and values, to select the most important test cases among those that interact with the modifications made to the software. Case studies carried out with the technique indicate that WSA-RT is capable of significantly reducing (by 70% to 80%) the size of the set of test cases, thus being able to contribute to reducing regression testing costs without affecting the defect detection capability.;;Universidade Federal de Campina Grande;pt_BR;Published;15;2012;2012-01-30 18:07:32
25646;Patrícia Duarte de Lima Machado;Seleção Automática de Casos de Teste de Regressão Baseada em Similaridade e Valores;Selective re-testing techniques aim to assist in the selection of regression test cases based on the modifications made. This work presents a new technique: Weighted Similarity Approach for Regression Testing (WSA-RT). This technique uses a testing approach based on models and values, to select the most important test cases among those that interact with the modifications made to the software. Case studies carried out with the technique indicate that WSA-RT is capable of significantly reducing (by 70% to 80%) the size of the set of test cases, thus being able to contribute to reducing regression testing costs without affecting the defect detection capability.;;Universidade Federal de Campina Grande;pt_BR;Published;15;2012;2012-01-30 18:07:32
25720;Wesley Klewerton Guez Assunção;Generating Integration Test Orders for Aspect Oriented Software with Multi-objective Algorithms;The problem known as CAITO refers to the determination of an order to integrate and test classes and aspects that minimizes stubbing costs. Such problem is NP-hard and to solve it efficiently, search based algorithms have been used, mainly evolutionary ones. However, the problem is very complex since it involves different factors that may influence the stubbing process, such as complexity measures, contractual issues and so on. These factors are usually in conflict and different possible solutions for the problem exist. To deal properly with this problem, this work explores the use of multi-objective optimization algorithms. The paper presents results from the application of two evolutionary algorithms - NSGA-II and SPEA2 - to the CAITO problem in four real systems, implemented in AspectJ. Both multi-objective algorithms are evaluated and compared with the traditional Tarjan's algorithm and with a mono-objective genetic algorithm. Moreover, it is shown how the tester can use the found solutions, according to the test goals.;;Computer Science Department, Federal University of Paraná (UFPR);pt_BR;Published;26;2012;2012-02-02 11:00:14
25720;Thelma Elita Colanzi;Generating Integration Test Orders for Aspect Oriented Software with Multi-objective Algorithms;The problem known as CAITO refers to the determination of an order to integrate and test classes and aspects that minimizes stubbing costs. Such problem is NP-hard and to solve it efficiently, search based algorithms have been used, mainly evolutionary ones. However, the problem is very complex since it involves different factors that may influence the stubbing process, such as complexity measures, contractual issues and so on. These factors are usually in conflict and different possible solutions for the problem exist. To deal properly with this problem, this work explores the use of multi-objective optimization algorithms. The paper presents results from the application of two evolutionary algorithms - NSGA-II and SPEA2 - to the CAITO problem in four real systems, implemented in AspectJ. Both multi-objective algorithms are evaluated and compared with the traditional Tarjan's algorithm and with a mono-objective genetic algorithm. Moreover, it is shown how the tester can use the found solutions, according to the test goals.;;Computer Science Department, State University of Maringá (UEM);pt_BR;Published;26;2012;2012-02-02 11:00:14
25720;Silvia Regina Vergilio;Generating Integration Test Orders for Aspect Oriented Software with Multi-objective Algorithms;The problem known as CAITO refers to the determination of an order to integrate and test classes and aspects that minimizes stubbing costs. Such problem is NP-hard and to solve it efficiently, search based algorithms have been used, mainly evolutionary ones. However, the problem is very complex since it involves different factors that may influence the stubbing process, such as complexity measures, contractual issues and so on. These factors are usually in conflict and different possible solutions for the problem exist. To deal properly with this problem, this work explores the use of multi-objective optimization algorithms. The paper presents results from the application of two evolutionary algorithms - NSGA-II and SPEA2 - to the CAITO problem in four real systems, implemented in AspectJ. Both multi-objective algorithms are evaluated and compared with the traditional Tarjan's algorithm and with a mono-objective genetic algorithm. Moreover, it is shown how the tester can use the found solutions, according to the test goals.;;Computer Science Department, Federal University of Paraná (UFPR);pt_BR;Published;26;2012;2012-02-02 11:00:14
25720;Aurora Trinidad Ramirez Pozo;Generating Integration Test Orders for Aspect Oriented Software with Multi-objective Algorithms;The problem known as CAITO refers to the determination of an order to integrate and test classes and aspects that minimizes stubbing costs. Such problem is NP-hard and to solve it efficiently, search based algorithms have been used, mainly evolutionary ones. However, the problem is very complex since it involves different factors that may influence the stubbing process, such as complexity measures, contractual issues and so on. These factors are usually in conflict and different possible solutions for the problem exist. To deal properly with this problem, this work explores the use of multi-objective optimization algorithms. The paper presents results from the application of two evolutionary algorithms - NSGA-II and SPEA2 - to the CAITO problem in four real systems, implemented in AspectJ. Both multi-objective algorithms are evaluated and compared with the traditional Tarjan's algorithm and with a mono-objective genetic algorithm. Moreover, it is shown how the tester can use the found solutions, according to the test goals.;;Computer Science Department, Federal University of Paraná (UFPR);pt_BR;Published;26;2012;2012-02-02 11:00:14
25803;George Souza Oliveira;Compilação Just-In-Time: Histórico, Arquitetura, Princípios e Sistemas;Several high-level language implementations focus on developing systems based on just-in-time compilation mechanisms. This mechanism has the attraction of improving the performance of such languages, while maintaining portability. However, at the price of including compilation time in the total execution time. Given this, research in the area has aimed to balance compilation costs with execution efficiency. Early just-in-time compilation systems employed static strategies to select and optimize code regions conducive to good performance. More sophisticated systems have improved such strategies with the aim of applying optimizations more carefully. In this sense, this tutorial presents the principles that underlie just-in-time compilation and its evolution over the years, as well as the approach used by different systems to ensure a balance of cost and efficiency. Although it is difficult to define the best approach, recent work shows that strict strategies for code detection and optimization, together with parallelism capabilities offered by multi-core architectures will form the basis of future just-in-time compilation systems.;;Universidade Estadual de Maringá;pt_BR;Published;39;2012;2012-02-06 20:48:59
25803;Anderson Faustino da Silva;Compilação Just-In-Time: Histórico, Arquitetura, Princípios e Sistemas;Several high-level language implementations focus on developing systems based on just-in-time compilation mechanisms. This mechanism has the attraction of improving the performance of such languages, while maintaining portability. However, at the price of including compilation time in the total execution time. Given this, research in the area has aimed to balance compilation costs with execution efficiency. Early just-in-time compilation systems employed static strategies to select and optimize code regions conducive to good performance. More sophisticated systems have improved such strategies with the aim of applying optimizations more carefully. In this sense, this tutorial presents the principles that underlie just-in-time compilation and its evolution over the years, as well as the approach used by different systems to ensure a balance of cost and efficiency. Although it is difficult to define the best approach, recent work shows that strict strategies for code detection and optimization, together with parallelism capabilities offered by multi-core architectures will form the basis of future just-in-time compilation systems.;;Universidade Estadual de Maringá;pt_BR;Published;39;2012;2012-02-06 20:48:59
25876;André Luiz de Castro Leal;Aplicação de Modelos Intencionais e Sistemas Multiagentes para Estabelecer Políticas de Monitoração de Transparência de Software;The article presents a proposal for monitoring software transparency characteristics from multi-agent systems. It is a first effort to understand intentional policies and models applied to autonomous monitoring agents.;;Universidade Federal Rural do Rio de Janeiro/Departamento de MatemáticaPontifícia Universidade Católica do Rio de Janeiro/Departamento de Informática;pt_BR;Published;27;2012;2012-02-09 21:05:39
25876;Henrique Prado de Sousa;Aplicação de Modelos Intencionais e Sistemas Multiagentes para Estabelecer Políticas de Monitoração de Transparência de Software;The article presents a proposal for monitoring software transparency characteristics from multi-agent systems. It is a first effort to understand intentional policies and models applied to autonomous monitoring agents.;;Pontifícia Universidade Católica do Rio de Janeiro/Departamento de Informática;pt_BR;Published;27;2012;2012-02-09 21:05:39
25876;Julio Cesar Sampaio do Prado Leite;Aplicação de Modelos Intencionais e Sistemas Multiagentes para Estabelecer Políticas de Monitoração de Transparência de Software;The article presents a proposal for monitoring software transparency characteristics from multi-agent systems. It is a first effort to understand intentional policies and models applied to autonomous monitoring agents.;;Pontifícia Universidade Católica do Rio de Janeiro/Departamento de Informática;pt_BR;Published;27;2012;2012-02-09 21:05:39
25876;Carlos Jose Pereira Lucena;Aplicação de Modelos Intencionais e Sistemas Multiagentes para Estabelecer Políticas de Monitoração de Transparência de Software;The article presents a proposal for monitoring software transparency characteristics from multi-agent systems. It is a first effort to understand intentional policies and models applied to autonomous monitoring agents.;;Pontifícia Universidade Católica do Rio de Janeiro/Departamento de Informática;pt_BR;Published;27;2012;2012-02-09 21:05:39
25948;João Pablo Silva da Silva;Um Sistema para Inspeções de Garantia da Qualidade Baseado em Ontologias e Agentes;The implementation of quality assurance practices has a cost/benefit ratio that is difficult to balance. This is because the cost of implementing the practices is direct, while the benefit obtained from them is indirect. In order to improve this relationship, this work presents a support system for quality assurance inspections capable of automating scope definition and addressing non-conformities, in addition to managing records and calculating indicators. The system is made up of an ontology, which maps the semantics involved in quality assurance inspections, and software agents, which implement the rules necessary for automating the mentioned activities. Experiments carried out in a Software Factory showed an improvement in inspection productivity, maximizing coverage, without impacting the effort required.;;Universidade Federal do Pampa;pt_BR;Published;17;2012;2012-02-14 14:31:54
25948;Pablo Dall'Oglio;Um Sistema para Inspeções de Garantia da Qualidade Baseado em Ontologias e Agentes;The implementation of quality assurance practices has a cost/benefit ratio that is difficult to balance. This is because the cost of implementing the practices is direct, while the benefit obtained from them is indirect. In order to improve this relationship, this work presents a support system for quality assurance inspections capable of automating scope definition and addressing non-conformities, in addition to managing records and calculating indicators. The system is made up of an ontology, which maps the semantics involved in quality assurance inspections, and software agents, which implement the rules necessary for automating the mentioned activities. Experiments carried out in a Software Factory showed an improvement in inspection productivity, maximizing coverage, without impacting the effort required.;;Universidade do Vale do Rio dos Sinos;pt_BR;Published;17;2012;2012-02-14 14:31:54
25948;Sérgio Crespo Coelho da Silva Pinto;Um Sistema para Inspeções de Garantia da Qualidade Baseado em Ontologias e Agentes;The implementation of quality assurance practices has a cost/benefit ratio that is difficult to balance. This is because the cost of implementing the practices is direct, while the benefit obtained from them is indirect. In order to improve this relationship, this work presents a support system for quality assurance inspections capable of automating scope definition and addressing non-conformities, in addition to managing records and calculating indicators. The system is made up of an ontology, which maps the semantics involved in quality assurance inspections, and software agents, which implement the rules necessary for automating the mentioned activities. Experiments carried out in a Software Factory showed an improvement in inspection productivity, maximizing coverage, without impacting the effort required.;;Universidade do Vale do Rio dos Sinos;pt_BR;Published;17;2012;2012-02-14 14:31:54
25948;Ig Ibert Bittencourt;Um Sistema para Inspeções de Garantia da Qualidade Baseado em Ontologias e Agentes;The implementation of quality assurance practices has a cost/benefit ratio that is difficult to balance. This is because the cost of implementing the practices is direct, while the benefit obtained from them is indirect. In order to improve this relationship, this work presents a support system for quality assurance inspections capable of automating scope definition and addressing non-conformities, in addition to managing records and calculating indicators. The system is made up of an ontology, which maps the semantics involved in quality assurance inspections, and software agents, which implement the rules necessary for automating the mentioned activities. Experiments carried out in a Software Factory showed an improvement in inspection productivity, maximizing coverage, without impacting the effort required.;;Universidade Federal de Alagoas;pt_BR;Published;17;2012;2012-02-14 14:31:54
25965;Alessandro Nakamuta;Avaliação de Desempenho da Política EBS para Escalonamento em Aplicações com Restrições de Máximo Tempo Médio de Resposta;This work presents a performance evaluation of the EBS algorithm as a scheduling policy in QoS-aware applications under soft real-time temporal requirements. The study analyzes parameters that describe the workload and evaluates their impact on the ability to meet the average response time upper limit specification. The EBS algorithm implements a dynamic priority scheduling policy for flexible real-time applications, which has been investigated in feedback scheduling and online dynamic scheduling architectures. Experiments and results are presented that analyze the influence of the factors considered in the SLA metric.;;Universidade de São Paulo;pt_BR;Published;18;2012;2012-02-15 8:14:17
25965;Lourenço Alves Pereira Jr;Avaliação de Desempenho da Política EBS para Escalonamento em Aplicações com Restrições de Máximo Tempo Médio de Resposta;This work presents a performance evaluation of the EBS algorithm as a scheduling policy in QoS-aware applications under soft real-time temporal requirements. The study analyzes parameters that describe the workload and evaluates their impact on the ability to meet the average response time upper limit specification. The EBS algorithm implements a dynamic priority scheduling policy for flexible real-time applications, which has been investigated in feedback scheduling and online dynamic scheduling architectures. Experiments and results are presented that analyze the influence of the factors considered in the SLA metric.;;Universidade de São Paulo;pt_BR;Published;18;2012;2012-02-15 8:14:17
25965;Francisco José Monaco;Avaliação de Desempenho da Política EBS para Escalonamento em Aplicações com Restrições de Máximo Tempo Médio de Resposta;This work presents a performance evaluation of the EBS algorithm as a scheduling policy in QoS-aware applications under soft real-time temporal requirements. The study analyzes parameters that describe the workload and evaluates their impact on the ability to meet the average response time upper limit specification. The EBS algorithm implements a dynamic priority scheduling policy for flexible real-time applications, which has been investigated in feedback scheduling and online dynamic scheduling architectures. Experiments and results are presented that analyze the influence of the factors considered in the SLA metric.;;Universidade de São Paulo;pt_BR;Published;18;2012;2012-02-15 8:14:17
26274;Carla Negri Lintzmayer;PColorAnt3-RT: Um Algoritmo ACO Paralelo para Coloração de Grafos;In previous works, investigations of three different algorithms based on Ant Colony Optimization (ACO) were presented, applied to the problem of graph coloring with k colors. The results obtained demonstrated that ColorAnt_3-RT is the best among the three ColorAnt-RT algorithms, being able to obtain good and even great solutions for the best values ​​of k described in the literature, in addition to minimizing the number of conflicts. However, in applications where execution time is a crucial factor, ACO algorithms have not been used. This article proposes and demonstrates the use of a parallel approach for a {\it hardware} architecture with shared memory with the aim of reducing the execution time of ColorAnt_3-RT, originating the parallel algorithm PColorAnt_3-RT which is capable of finding good and great solutions in acceptable runtime.;;UNICAMP;pt_BR;Published;21;2012;2012-03-07 13:48:00
26274;Mauro Henrique Mulati;PColorAnt3-RT: Um Algoritmo ACO Paralelo para Coloração de Grafos;In previous works, investigations of three different algorithms based on Ant Colony Optimization (ACO) were presented, applied to the problem of graph coloring with k colors. The results obtained demonstrated that ColorAnt_3-RT is the best among the three ColorAnt-RT algorithms, being able to obtain good and even great solutions for the best values ​​of k described in the literature, in addition to minimizing the number of conflicts. However, in applications where execution time is a crucial factor, ACO algorithms have not been used. This article proposes and demonstrates the use of a parallel approach for a {\it hardware} architecture with shared memory with the aim of reducing the execution time of ColorAnt_3-RT, originating the parallel algorithm PColorAnt_3-RT which is capable of finding good and great solutions in acceptable runtime.;;Universidade Estadual de Maringá;pt_BR;Published;21;2012;2012-03-07 13:48:00
26274;Anderson Faustino da Silva;PColorAnt3-RT: Um Algoritmo ACO Paralelo para Coloração de Grafos;In previous works, investigations of three different algorithms based on Ant Colony Optimization (ACO) were presented, applied to the problem of graph coloring with k colors. The results obtained demonstrated that ColorAnt_3-RT is the best among the three ColorAnt-RT algorithms, being able to obtain good and even great solutions for the best values ​​of k described in the literature, in addition to minimizing the number of conflicts. However, in applications where execution time is a crucial factor, ACO algorithms have not been used. This article proposes and demonstrates the use of a parallel approach for a {\it hardware} architecture with shared memory with the aim of reducing the execution time of ColorAnt_3-RT, originating the parallel algorithm PColorAnt_3-RT which is capable of finding good and great solutions in acceptable runtime.;;Universidade Estadual de Maringá;pt_BR;Published;21;2012;2012-03-07 13:48:00
26296;Murillo Rehder Batista;Decision making for a delivery robot through a fuzzy system;One of the main objectives of robotic researches has been the use of autonomous agents to make ”mundane” tasks. A typical task common in literature is fetching a document from an initial point local and deliver to a speciﬁed destination. In this work, a fuzzy system has been developed to manage a number of requests that a robot has to deliver, which are requested in real time. This system uses a set of fuzzy rules to deﬁne the priority of each request based on conditions such as the time limit to take the document from its source or the score acquired from a successful delivery. A modiﬁcation of A* algorithm to plan paths and the potential ﬁeld technique to manage navigation are used to guide the robot in the environment. Three different sets of fuzzy rules have been tested in order to compare their results with the known algorithm, ﬁrst-in ﬁrst-out, to attend the requests. The Player/Stage simulator has been used to make the tests. The results show that such approach is effective, obtaining better results on criteria which have been considered of greater importance for each set of rules.;;Instituto de Ciências Matemáticas e de Computação - USP São Carlos;pt_BR;Published;18;2012;2012-03-08 16:03:38
26296;Roseli Aparecida Francelin Romero;Decision making for a delivery robot through a fuzzy system;One of the main objectives of robotic researches has been the use of autonomous agents to make ”mundane” tasks. A typical task common in literature is fetching a document from an initial point local and deliver to a speciﬁed destination. In this work, a fuzzy system has been developed to manage a number of requests that a robot has to deliver, which are requested in real time. This system uses a set of fuzzy rules to deﬁne the priority of each request based on conditions such as the time limit to take the document from its source or the score acquired from a successful delivery. A modiﬁcation of A* algorithm to plan paths and the potential ﬁeld technique to manage navigation are used to guide the robot in the environment. Three different sets of fuzzy rules have been tested in order to compare their results with the known algorithm, ﬁrst-in ﬁrst-out, to attend the requests. The Player/Stage simulator has been used to make the tests. The results show that such approach is effective, obtaining better results on criteria which have been considered of greater importance for each set of rules.;;Instituto de Ciências Matemáticas e de Computação - USP São Carlos;pt_BR;Published;18;2012;2012-03-08 16:03:38
26421;Rodrigo Carneiro Brandão;Serviços em Redes Futuras: Objetivos, Tecnologias e Comparação de Iniciativas;The services sector, also known in economics as the third sector, is the new engine of the world economy. It is estimated that more than 50% of countries' gross domestic product is derived from this sector. The technological evolution of the late 90s took the concept of services to the web, through the so-called Web Services. With the increasing use of the Internet in the most diverse areas, there was a need to expand the concept of service on the network to a higher level, not just technical or restricted to information technology, but to also bring to the network the real world services or everyday services. The Internet of Services aims to develop new architectures for the current and future Internet services market. Therefore, this article discusses the role of services in future networks, presenting the key technologies for this purpose, as well as comparing some initiatives that have stood out in this scenario.;;Instituto Nacional de Telecomunicações - INATEL;pt_BR;Published;28;2012;2012-03-14 10:58:37
26421;Agostinho Manuel Vaz;Serviços em Redes Futuras: Objetivos, Tecnologias e Comparação de Iniciativas;The services sector, also known in economics as the third sector, is the new engine of the world economy. It is estimated that more than 50% of countries' gross domestic product is derived from this sector. The technological evolution of the late 90s took the concept of services to the web, through the so-called Web Services. With the increasing use of the Internet in the most diverse areas, there was a need to expand the concept of service on the network to a higher level, not just technical or restricted to information technology, but to also bring to the network the real world services or everyday services. The Internet of Services aims to develop new architectures for the current and future Internet services market. Therefore, this article discusses the role of services in future networks, presenting the key technologies for this purpose, as well as comparing some initiatives that have stood out in this scenario.;;INATEL;pt_BR;Published;28;2012;2012-03-14 10:58:37
26421;Antônio Marcos Alberti;Serviços em Redes Futuras: Objetivos, Tecnologias e Comparação de Iniciativas;The services sector, also known in economics as the third sector, is the new engine of the world economy. It is estimated that more than 50% of countries' gross domestic product is derived from this sector. The technological evolution of the late 90s took the concept of services to the web, through the so-called Web Services. With the increasing use of the Internet in the most diverse areas, there was a need to expand the concept of service on the network to a higher level, not just technical or restricted to information technology, but to also bring to the network the real world services or everyday services. The Internet of Services aims to develop new architectures for the current and future Internet services market. Therefore, this article discusses the role of services in future networks, presenting the key technologies for this purpose, as well as comparing some initiatives that have stood out in this scenario.;;INATEL;pt_BR;Published;28;2012;2012-03-14 10:58:37
26629;Douglas Fernandes Pereira da Silva Junior;Um Sistema de Realização Superficial para Geração de Textos em Português;Natural language generation (GLN) systems - which produce text from non-linguistic data - have a wide range of applications in textual visualization of complex and/or large-volume content. This work focuses on the implementation of a rule-based textual realization module for Brazilian Portuguese, called PortNLG, which handles the task of sentential linearization for computational applications that need to present output data in textual format. PortNLG is presented in the form of a JAVA library, and its results are superior to those of n-gram models in the task of generating newspaper headlines.;;Universidade de São PauloEscola de Artes, Ciências e Humanidades;pt_BR;Published;17;2012;2012-03-22 12:36:50
26629;Eder Miranda de Novais;Um Sistema de Realização Superficial para Geração de Textos em Português;Natural language generation (GLN) systems - which produce text from non-linguistic data - have a wide range of applications in textual visualization of complex and/or large-volume content. This work focuses on the implementation of a rule-based textual realization module for Brazilian Portuguese, called PortNLG, which handles the task of sentential linearization for computational applications that need to present output data in textual format. PortNLG is presented in the form of a JAVA library, and its results are superior to those of n-gram models in the task of generating newspaper headlines.;;Universidade de São PauloEscola de Artes, Ciências e Humanidades;pt_BR;Published;17;2012;2012-03-22 12:36:50
26629;Ivandre Paraboni;Um Sistema de Realização Superficial para Geração de Textos em Português;Natural language generation (GLN) systems - which produce text from non-linguistic data - have a wide range of applications in textual visualization of complex and/or large-volume content. This work focuses on the implementation of a rule-based textual realization module for Brazilian Portuguese, called PortNLG, which handles the task of sentential linearization for computational applications that need to present output data in textual format. PortNLG is presented in the form of a JAVA library, and its results are superior to those of n-gram models in the task of generating newspaper headlines.;;Universidade de São Paulo (USP)Escola de Artes, Ciências e Humanidades (EACH);pt_BR;Published;17;2012;2012-03-22 12:36:50
26905;Leandro Miranda Zatesko;Deterministic and efficient minimal perfect hashing schemes;In this work we present deterministic versions for the hashing schemes of Botelho, Kohayakawa and Ziviani (2005) and by Botelho, Pagh and Ziviani (2007). We also respond to a problem left open in the first of the works, related to the proof of correctness and the complexity analysis of the scheme they proposed. The deterministic versions developed were implemented and tested on data sets with up to 25,000,000 keys, and the verified results were equivalent to those of the original randomized algorithms.;;Universidade Federal do Paraná;pt_BR;Published;16;2012;2012-04-02 13:23:23
26905;Jair Donadelli Jr.;Deterministic and efficient minimal perfect hashing schemes;In this work we present deterministic versions for the hashing schemes of Botelho, Kohayakawa and Ziviani (2005) and by Botelho, Pagh and Ziviani (2007). We also respond to a problem left open in the first of the works, related to the proof of correctness and the complexity analysis of the scheme they proposed. The deterministic versions developed were implemented and tested on data sets with up to 25,000,000 keys, and the verified results were equivalent to those of the original randomized algorithms.;;Universidade Federal do ABC;pt_BR;Published;16;2012;2012-04-02 13:23:23
28371;Eduardo da Silva;Uma Avaliação do Esquema de Gerenciamento de Chave Baseado em Identidade Identity Key Management;Security is one of the main challenges in Mobile Ad Hoc Networks (MANETs). The natural characteristics of MANETs make these networks highly vulnerable to many attacks, from the physical layer to the application layer. There are several algorithms and protocols to deal with these threats. All of these algorithms have a common element, the use of cryptography. Among the cryptographic systems found in the literature, those based on identity seem to best adapt to the MANETs paradigm. Its main advantages are low computational cost and reduced overhead. This work presents an evaluation of the main identity-based cryptographic scheme for MANETs, ​​Identity Key Management (IKM). The assessment was carried out considering the renewal and revocation of keys and the presence of false accusation attacks. The results show that IKM is vulnerable to this attack and partitioning the network can lead it to an unstable state.;;Instituto Federal Catarinense;pt_BR;Published;14;2012;2012-05-09 9:13:21
28371;Luiz Carlos Pessoa Albini;Uma Avaliação do Esquema de Gerenciamento de Chave Baseado em Identidade Identity Key Management;Security is one of the main challenges in Mobile Ad Hoc Networks (MANETs). The natural characteristics of MANETs make these networks highly vulnerable to many attacks, from the physical layer to the application layer. There are several algorithms and protocols to deal with these threats. All of these algorithms have a common element, the use of cryptography. Among the cryptographic systems found in the literature, those based on identity seem to best adapt to the MANETs paradigm. Its main advantages are low computational cost and reduced overhead. This work presents an evaluation of the main identity-based cryptographic scheme for MANETs, ​​Identity Key Management (IKM). The assessment was carried out considering the renewal and revocation of keys and the presence of false accusation attacks. The results show that IKM is vulnerable to this attack and partitioning the network can lead it to an unstable state.;;Universidade Federal do Paraná;pt_BR;Published;14;2012;2012-05-09 9:13:21
28371;Murilo W. S. Lima;Uma Avaliação do Esquema de Gerenciamento de Chave Baseado em Identidade Identity Key Management;Security is one of the main challenges in Mobile Ad Hoc Networks (MANETs). The natural characteristics of MANETs make these networks highly vulnerable to many attacks, from the physical layer to the application layer. There are several algorithms and protocols to deal with these threats. All of these algorithms have a common element, the use of cryptography. Among the cryptographic systems found in the literature, those based on identity seem to best adapt to the MANETs paradigm. Its main advantages are low computational cost and reduced overhead. This work presents an evaluation of the main identity-based cryptographic scheme for MANETs, ​​Identity Key Management (IKM). The assessment was carried out considering the renewal and revocation of keys and the presence of false accusation attacks. The results show that IKM is vulnerable to this attack and partitioning the network can lead it to an unstable state.;;Universidade Federal do Paraná;pt_BR;Published;14;2012;2012-05-09 9:13:21
29997;Marcos Paulo Nicoletti;Simulação do Espalhamento da Influenza na Cidade de Cascavel-PR Utilizando Agentes Computacionais;This work presents and discusses results arising from a computational experiment in which the spread of Influenza was modeled based on the conception of SIRS-type compartmental models, using the model-based agent approach. The flu spreading process was simulated in a specific region of the city of Cascavel-PR, covering part of three neighborhoods. In this environment, 4,653 agents interacted, divided into 6 different types: infants, children, teenagers, young people, adults and elderly agents. Each type of agent presents specific behaviors regarding movement in the environment, for the purposes of study, work, leisure or residential stay. Transmission of the disease occurs through interaction between susceptible and infected individuals. Analyzes were carried out based on the results obtained and work has already begun to map and better characterize the municipality as well as validate the model through data provided by the Municipal Health Department.;;Universidade Estadual do Oeste do Paraná;pt_BR;Published;23;2012;2012-06-20 16:39:47
29997;Claudia Brandelero Rizzi;Simulação do Espalhamento da Influenza na Cidade de Cascavel-PR Utilizando Agentes Computacionais;This work presents and discusses results arising from a computational experiment in which the spread of Influenza was modeled based on the conception of SIRS-type compartmental models, using the model-based agent approach. The flu spreading process was simulated in a specific region of the city of Cascavel-PR, covering part of three neighborhoods. In this environment, 4,653 agents interacted, divided into 6 different types: infants, children, teenagers, young people, adults and elderly agents. Each type of agent presents specific behaviors regarding movement in the environment, for the purposes of study, work, leisure or residential stay. Transmission of the disease occurs through interaction between susceptible and infected individuals. Analyzes were carried out based on the results obtained and work has already begun to map and better characterize the municipality as well as validate the model through data provided by the Municipal Health Department.;;Universidade Estadual do Oeste do Paraná;pt_BR;Published;23;2012;2012-06-20 16:39:47
29997;Rogério Luís Rizzi;Simulação do Espalhamento da Influenza na Cidade de Cascavel-PR Utilizando Agentes Computacionais;This work presents and discusses results arising from a computational experiment in which the spread of Influenza was modeled based on the conception of SIRS-type compartmental models, using the model-based agent approach. The flu spreading process was simulated in a specific region of the city of Cascavel-PR, covering part of three neighborhoods. In this environment, 4,653 agents interacted, divided into 6 different types: infants, children, teenagers, young people, adults and elderly agents. Each type of agent presents specific behaviors regarding movement in the environment, for the purposes of study, work, leisure or residential stay. Transmission of the disease occurs through interaction between susceptible and infected individuals. Analyzes were carried out based on the results obtained and work has already begun to map and better characterize the municipality as well as validate the model through data provided by the Municipal Health Department.;;Universidade Estadual do Oeste do Paraná;pt_BR;Published;23;2012;2012-06-20 16:39:47
30242;Anderson Rocha Tavares;Independent learners in abstract traffic scenarios;Traffic is a phenomena that emerges from individual, uncoordinatedand, most of the times, selfish route choice made by drivers. In general, this leads topoor global and individual performance, regarding travel times and road network loadbalance. This work presents a reinforcement learning based approach for route choicewhich relies solely on drivers experience to guide their decisions. There is no coordinatedlearning mechanism, thus driver agents are independent learners. Our approachis tested on two abstract traffic scenarios and it is compared to other route choice methods.Experimental results show that drivers learn routes in complex scenarios with noprior knowledge. Plus, the approach outperforms the compared route choice methodsregarding drivers’ travel time. Also, satisfactory performance is achieved regardingroad network load balance. The simplicity, realistic assumptions and performance ofthe proposed approach suggests that it is a feasible candidate for implementation innavigation systems for guiding drivers decision regarding route choice.;;Universidade Federal do Rio Grande do Sul;pt_BR;Published;20;2012;2012-06-29 18:42:09
30242;Ana Lucia Cetertich Bazzan;Independent learners in abstract traffic scenarios;Traffic is a phenomena that emerges from individual, uncoordinatedand, most of the times, selfish route choice made by drivers. In general, this leads topoor global and individual performance, regarding travel times and road network loadbalance. This work presents a reinforcement learning based approach for route choicewhich relies solely on drivers experience to guide their decisions. There is no coordinatedlearning mechanism, thus driver agents are independent learners. Our approachis tested on two abstract traffic scenarios and it is compared to other route choice methods.Experimental results show that drivers learn routes in complex scenarios with noprior knowledge. Plus, the approach outperforms the compared route choice methodsregarding drivers’ travel time. Also, satisfactory performance is achieved regardingroad network load balance. The simplicity, realistic assumptions and performance ofthe proposed approach suggests that it is a feasible candidate for implementation innavigation systems for guiding drivers decision regarding route choice.;;-;pt_BR;Published;20;2012;2012-06-29 18:42:09
30317;José Rodrigo Neri;Aplicando Conceitos de Sistemas Multiagentes na Elaboração de um Jogo de Estratégia Simulado;The use of the agent paradigm is increasing in the field of game development. The characteristics of agents, such as autonomy, proactivity and social interaction, can help in the development of more realistic and detailed games. This work aims to integrate several tools and concepts from the AI ​​area for the development of a strategy game. In addition to agents, the concepts of organization, environment, reputation, expert systems and ontologies were used.;;UFSC;pt_BR;Published;24;2012;2012-06-30 21:55:12
30317;Maicon Rafael Zatelli;Aplicando Conceitos de Sistemas Multiagentes na Elaboração de um Jogo de Estratégia Simulado;The use of the agent paradigm is increasing in the field of game development. The characteristics of agents, such as autonomy, proactivity and social interaction, can help in the development of more realistic and detailed games. This work aims to integrate several tools and concepts from the AI ​​area for the development of a strategy game. In addition to agents, the concepts of organization, environment, reputation, expert systems and ontologies were used.;;UFSC;pt_BR;Published;24;2012;2012-06-30 21:55:12
30317;Daniela Maria Uez;Aplicando Conceitos de Sistemas Multiagentes na Elaboração de um Jogo de Estratégia Simulado;The use of the agent paradigm is increasing in the field of game development. The characteristics of agents, such as autonomy, proactivity and social interaction, can help in the development of more realistic and detailed games. This work aims to integrate several tools and concepts from the AI ​​area for the development of a strategy game. In addition to agents, the concepts of organization, environment, reputation, expert systems and ontologies were used.;;UFSC;pt_BR;Published;24;2012;2012-06-30 21:55:12
30317;Rafael Frizzo Callegaro;Aplicando Conceitos de Sistemas Multiagentes na Elaboração de um Jogo de Estratégia Simulado;The use of the agent paradigm is increasing in the field of game development. The characteristics of agents, such as autonomy, proactivity and social interaction, can help in the development of more realistic and detailed games. This work aims to integrate several tools and concepts from the AI ​​area for the development of a strategy game. In addition to agents, the concepts of organization, environment, reputation, expert systems and ontologies were used.;;-;pt_BR;Published;24;2012;2012-06-30 21:55:12
30492;Fernando dos Santos;Extending the RoboCup Rescue to Support Stigmergy: Experiments and Results;Social insects have inspired researches in computer sciences as well asengineers to develop models for coordination and cooperation in multiagent systems.One example of these models is the model of stigmergy. In this model agents useindirect communication (comunication trough the environment) in order to coordinateactions. The RoboCup Rescue simulator is used as a testbed to evaluate this modelin a real world considering a highly constrained scenario of an earthquake. This pa-per investigates the feasibility of using stigmergy in the RoboCup Rescue and theimprovements of performance can be obtained. We extended the RoboCup Rescueenvironment to enable the use of stigmergy by the agents. We compared the results ofa multiagent system that uses stigmergy against two other approaches: a multiagentsystem that uses a greedy strategy and no communication, and a multiagent systemwhere agents communicate via direct messages. Experimental results shown that theuse of stigmergy leads to an improvement on agents’ performance by 9.02% to 38.6%if comparing to the system with no communication and can be statistically equivalentto the system which uses messages, depending on the scenario.;;Universidade do Estado de Santa Catarina (UDESC);pt_BR;Published;16;2012;2012-07-06 21:59:16
30492;Gabriel Rigo da Cruz Jacobsen;Extending the RoboCup Rescue to Support Stigmergy: Experiments and Results;Social insects have inspired researches in computer sciences as well asengineers to develop models for coordination and cooperation in multiagent systems.One example of these models is the model of stigmergy. In this model agents useindirect communication (comunication trough the environment) in order to coordinateactions. The RoboCup Rescue simulator is used as a testbed to evaluate this modelin a real world considering a highly constrained scenario of an earthquake. This pa-per investigates the feasibility of using stigmergy in the RoboCup Rescue and theimprovements of performance can be obtained. We extended the RoboCup Rescueenvironment to enable the use of stigmergy by the agents. We compared the results ofa multiagent system that uses stigmergy against two other approaches: a multiagentsystem that uses a greedy strategy and no communication, and a multiagent systemwhere agents communicate via direct messages. Experimental results shown that theuse of stigmergy leads to an improvement on agents’ performance by 9.02% to 38.6%if comparing to the system with no communication and can be statistically equivalentto the system which uses messages, depending on the scenario.;;Universidade do Estado de Santa Catarina (UDESC);pt_BR;Published;16;2012;2012-07-06 21:59:16
30492;Carlos Alberto Barth;Extending the RoboCup Rescue to Support Stigmergy: Experiments and Results;Social insects have inspired researches in computer sciences as well asengineers to develop models for coordination and cooperation in multiagent systems.One example of these models is the model of stigmergy. In this model agents useindirect communication (comunication trough the environment) in order to coordinateactions. The RoboCup Rescue simulator is used as a testbed to evaluate this modelin a real world considering a highly constrained scenario of an earthquake. This pa-per investigates the feasibility of using stigmergy in the RoboCup Rescue and theimprovements of performance can be obtained. We extended the RoboCup Rescueenvironment to enable the use of stigmergy by the agents. We compared the results ofa multiagent system that uses stigmergy against two other approaches: a multiagentsystem that uses a greedy strategy and no communication, and a multiagent systemwhere agents communicate via direct messages. Experimental results shown that theuse of stigmergy leads to an improvement on agents’ performance by 9.02% to 38.6%if comparing to the system with no communication and can be statistically equivalentto the system which uses messages, depending on the scenario.;;Universidade do Estado de Santa Catarina (UDESC);pt_BR;Published;16;2012;2012-07-06 21:59:16
36371;Rogério Galante Negri;Aplicação de Modelos de Aprendizado Semissupervisionado na Classificação de Imagens de Sensoriamento Remoto;In the most diverse applications, the scarcity of information for the proper training and use of supervised Machine Learning methods is a persistent problem. This fact motivated the development of the semi-supervised learning paradigm, which can be understood as a combination of concepts from the supervised and unsupervised paradigms. The way learning is conducted allows organizing semi-supervised methods into different models. This work presents a comparative study between different semi-supervised learning models. A semi-supervised version of the SVM method is also proposed, which achieved better performance in the comparisons carried out.;;Instituto Nacional de Pesquisas Espaciais/Divisão de Processamento de Imagens;pt_BR;Published;23;2012;2012-12-16 18:08:47
36371;Sidnei João Siqueira Sant'Anna;Aplicação de Modelos de Aprendizado Semissupervisionado na Classificação de Imagens de Sensoriamento Remoto;In the most diverse applications, the scarcity of information for the proper training and use of supervised Machine Learning methods is a persistent problem. This fact motivated the development of the semi-supervised learning paradigm, which can be understood as a combination of concepts from the supervised and unsupervised paradigms. The way learning is conducted allows organizing semi-supervised methods into different models. This work presents a comparative study between different semi-supervised learning models. A semi-supervised version of the SVM method is also proposed, which achieved better performance in the comparisons carried out.;;Instituto Nacional de Pesquisas Espaciais/Divisão de Processamento de Imagens;pt_BR;Published;23;2012;2012-12-16 18:08:47
36371;Luciano Vieira Dutra;Aplicação de Modelos de Aprendizado Semissupervisionado na Classificação de Imagens de Sensoriamento Remoto;In the most diverse applications, the scarcity of information for the proper training and use of supervised Machine Learning methods is a persistent problem. This fact motivated the development of the semi-supervised learning paradigm, which can be understood as a combination of concepts from the supervised and unsupervised paradigms. The way learning is conducted allows organizing semi-supervised methods into different models. This work presents a comparative study between different semi-supervised learning models. A semi-supervised version of the SVM method is also proposed, which achieved better performance in the comparisons carried out.;;Instituto Nacional de Pesquisas Espaciais/Divisão de Processamento de Imagens;pt_BR;Published;23;2012;2012-12-16 18:08:47
36756;George Souza Oliveira;Prolog: A Linguagem, A Máquina Abstrata de Warren e Implementações;The main motivation for using logic programming is to allow programmers to describe what they want separately from how to achieve that goal. In this context Prolog is the most popular logic programming language, with the Warren abstract machine as its standard execution model. The choice of this machine as an execution model was due to the fact that it offers several advantages, such as easy compilation, portability and compact code. This tutorial aims to describe the main characteristics of the Prolog language, its execution model and present the main Prolog systems.;;Universidade Estadual de Maringá;pt_BR;Published;32;2013;2013-01-10 18:02:11
36756;Anderson Faustino da Silva;Prolog: A Linguagem, A Máquina Abstrata de Warren e Implementações;The main motivation for using logic programming is to allow programmers to describe what they want separately from how to achieve that goal. In this context Prolog is the most popular logic programming language, with the Warren abstract machine as its standard execution model. The choice of this machine as an execution model was due to the fact that it offers several advantages, such as easy compilation, portability and compact code. This tutorial aims to describe the main characteristics of the Prolog language, its execution model and present the main Prolog systems.;;Universidade Estadual de Maringá;pt_BR;Published;32;2013;2013-01-10 18:02:11
36801;Ewerton Daniel de Lima;Seleção de Transformações Baseada em Estatística;Among the various transformations provided by a compiler, it is a challenge, even for the most experienced programmer, to know which ones will generate the best target code for a given source code. In this context, the development of an automated selector of good transformations is a challenge nowadays. Considering the problem of automatic selection of transformations, the objective of this article is to describe a statistical approach to select good transformations for a given source code. Using the statistical approach presented in this work, despite being simple, is capable of obtaining good results. In a set of 10 programs, the average speedup achieved in relation to the more aggressive LLVM approach was 1.0514, indicating a gain of 5.14. In the worst case, the proposed approach obtained a speedup of 1 and in the best case of 1.19, indicating a gain of 0% and 19%, respectively.;;Universidade Estadual de Maringá;pt_BR;Published;17;2013;2013-01-15 15:44:29
36801;Tiago Cariolano de Souza Xavier;Seleção de Transformações Baseada em Estatística;Among the various transformations provided by a compiler, it is a challenge, even for the most experienced programmer, to know which ones will generate the best target code for a given source code. In this context, the development of an automated selector of good transformations is a challenge nowadays. Considering the problem of automatic selection of transformations, the objective of this article is to describe a statistical approach to select good transformations for a given source code. Using the statistical approach presented in this work, despite being simple, is capable of obtaining good results. In a set of 10 programs, the average speedup achieved in relation to the more aggressive LLVM approach was 1.0514, indicating a gain of 5.14. In the worst case, the proposed approach obtained a speedup of 1 and in the best case of 1.19, indicating a gain of 0% and 19%, respectively.;;Universidade Estadual de Maringá;pt_BR;Published;17;2013;2013-01-15 15:44:29
36801;Anderson Faustino da Silva;Seleção de Transformações Baseada em Estatística;Among the various transformations provided by a compiler, it is a challenge, even for the most experienced programmer, to know which ones will generate the best target code for a given source code. In this context, the development of an automated selector of good transformations is a challenge nowadays. Considering the problem of automatic selection of transformations, the objective of this article is to describe a statistical approach to select good transformations for a given source code. Using the statistical approach presented in this work, despite being simple, is capable of obtaining good results. In a set of 10 programs, the average speedup achieved in relation to the more aggressive LLVM approach was 1.0514, indicating a gain of 5.14. In the worst case, the proposed approach obtained a speedup of 1 and in the best case of 1.19, indicating a gain of 0% and 19%, respectively.;;Universidade Estadual de Maringá;pt_BR;Published;17;2013;2013-01-15 15:44:29
36851;Marcos Antonio Quináia;Teste de Linha de Produto de Software Baseado em Mutação de Variabilidades;The OVM (Orthogonal Variability Model) has been adopted to define and document the variabilities of a software product line (LPS), and to derive products for testing. Given the enormous space of variability of most applications, test criteria based on combinations of variability, or basic path testing, are generally used to select a subset of products, the most representative. These criteria, however, do not consider defects that may be present in the OVM. To increase confidence that LPS products meet the specification, an approach based on mutation testing is presented in this work. Mutation operators are introduced and evaluated in two case studies. The results show that other types of defects are revealed in comparison with the existing criteria.;;Universidade Estadual do Centro-Oeste - UNICENTRO;pt_BR;Published;20;2013;2013-01-18 17:43:27
36851;Johnny Maikeo Ferreira;Teste de Linha de Produto de Software Baseado em Mutação de Variabilidades;The OVM (Orthogonal Variability Model) has been adopted to define and document the variabilities of a software product line (LPS), and to derive products for testing. Given the enormous space of variability of most applications, test criteria based on combinations of variability, or basic path testing, are generally used to select a subset of products, the most representative. These criteria, however, do not consider defects that may be present in the OVM. To increase confidence that LPS products meet the specification, an approach based on mutation testing is presented in this work. Mutation operators are introduced and evaluated in two case studies. The results show that other types of defects are revealed in comparison with the existing criteria.;;Universidade Federal do Paraná - UFPR;pt_BR;Published;20;2013;2013-01-18 17:43:27
36851;Silvia Regina Vergilio;Teste de Linha de Produto de Software Baseado em Mutação de Variabilidades;The OVM (Orthogonal Variability Model) has been adopted to define and document the variabilities of a software product line (LPS), and to derive products for testing. Given the enormous space of variability of most applications, test criteria based on combinations of variability, or basic path testing, are generally used to select a subset of products, the most representative. These criteria, however, do not consider defects that may be present in the OVM. To increase confidence that LPS products meet the specification, an approach based on mutation testing is presented in this work. Mutation operators are introduced and evaluated in two case studies. The results show that other types of defects are revealed in comparison with the existing criteria.;;Universidade Federal do Paraná - UFPR;pt_BR;Published;20;2013;2013-01-18 17:43:27
37349;Henrique Batista Silva;Analysis of Using Metric Access Methods for Visual Search of Objects in Video Databases;This article presents an approach to object retrieval that searches for and localizes all the occurrences of an object in a video database, given a query image of the object. Our proposal is based on text-retrieval methods in which video key frames are represented by a dense set of viewpoint invariant region descriptors that enable recognition to proceed successfully despite changes in camera viewpoint, lighting, and partial occlusions. Vector quantizing these region descriptors provides a visual analogy of a word - a visual word. Those words are grouped into a visual vocabulary which is used to index all key frames from the video database. Efficient retrieval is then achieved by employing methods from statistical text retrieval, including inverted file systems, and text-document frequency weightings. Though works in the literature have only adopted a simple sequential scan during search, we investigate the use of different metric access methods (MAM): M-tree, Slim-tree, and D-index, in order to accelerate the processing of similarity queries. In addition, a ranking strategy based on the spatial layout of the regions (spatial consistency) is fully described and evaluated. Experimental results have shown that the adoption of MAMs not only has improved the search performance but also has reduced the influence of the vocabulary size over test results, which may improve the scalability of our proposal. Finally, the application of spatial consistency has produced a very significant improvement of the results.;;DCC / UFMG;pt_BR;Published;15;2013;2013-02-25 21:16:01
37349;Zenilton Kleber Gonçalves Patrocínio;Analysis of Using Metric Access Methods for Visual Search of Objects in Video Databases;This article presents an approach to object retrieval that searches for and localizes all the occurrences of an object in a video database, given a query image of the object. Our proposal is based on text-retrieval methods in which video key frames are represented by a dense set of viewpoint invariant region descriptors that enable recognition to proceed successfully despite changes in camera viewpoint, lighting, and partial occlusions. Vector quantizing these region descriptors provides a visual analogy of a word - a visual word. Those words are grouped into a visual vocabulary which is used to index all key frames from the video database. Efficient retrieval is then achieved by employing methods from statistical text retrieval, including inverted file systems, and text-document frequency weightings. Though works in the literature have only adopted a simple sequential scan during search, we investigate the use of different metric access methods (MAM): M-tree, Slim-tree, and D-index, in order to accelerate the processing of similarity queries. In addition, a ranking strategy based on the spatial layout of the regions (spatial consistency) is fully described and evaluated. Experimental results have shown that the adoption of MAMs not only has improved the search performance but also has reduced the influence of the vocabulary size over test results, which may improve the scalability of our proposal. Finally, the application of spatial consistency has produced a very significant improvement of the results.;;ICEI / PUC Minas;pt_BR;Published;15;2013;2013-02-25 21:16:01
37349;Silvio Jamil Ferzoli Guimarães;Analysis of Using Metric Access Methods for Visual Search of Objects in Video Databases;This article presents an approach to object retrieval that searches for and localizes all the occurrences of an object in a video database, given a query image of the object. Our proposal is based on text-retrieval methods in which video key frames are represented by a dense set of viewpoint invariant region descriptors that enable recognition to proceed successfully despite changes in camera viewpoint, lighting, and partial occlusions. Vector quantizing these region descriptors provides a visual analogy of a word - a visual word. Those words are grouped into a visual vocabulary which is used to index all key frames from the video database. Efficient retrieval is then achieved by employing methods from statistical text retrieval, including inverted file systems, and text-document frequency weightings. Though works in the literature have only adopted a simple sequential scan during search, we investigate the use of different metric access methods (MAM): M-tree, Slim-tree, and D-index, in order to accelerate the processing of similarity queries. In addition, a ranking strategy based on the spatial layout of the regions (spatial consistency) is fully described and evaluated. Experimental results have shown that the adoption of MAMs not only has improved the search performance but also has reduced the influence of the vocabulary size over test results, which may improve the scalability of our proposal. Finally, the application of spatial consistency has produced a very significant improvement of the results.;;ICEI / PUC Minas;pt_BR;Published;15;2013;2013-02-25 21:16:01
37768;Guilherme Perin;Filtragem Wavelet de Sinais Cardíacos através de Algoritmos Adaptativos;In this work, the Azzalini, Farge and Schneider algorithm, used in image analysis, is modified, producing an adaptive and a recursive version for filtering cardiac signals. Through these algorithms a cutoff threshold is obtained based on the noise variance and the wavelet series of the analyzed signal is then truncated. Orthonormal Daubechies wavelets are considered. To validate the proposed algorithm, ECG signals from the MIT-BIH database with different levels of noise are filtered and the SNR quality measure is calculated for each case. The simulations show the performance of the proposed algorithm for different noise intensities, and for different levels of the wavelet transform, and in comparison to two other filtering methods.;;Universidade Federal de Santa MariaPPGI - LANA - GMICRO;pt_BR;Published;16;2013;2013-03-12 9:55:33
37768;Alice de Jesus Kozakevicius;Filtragem Wavelet de Sinais Cardíacos através de Algoritmos Adaptativos;In this work, the Azzalini, Farge and Schneider algorithm, used in image analysis, is modified, producing an adaptive and a recursive version for filtering cardiac signals. Through these algorithms a cutoff threshold is obtained based on the noise variance and the wavelet series of the analyzed signal is then truncated. Orthonormal Daubechies wavelets are considered. To validate the proposed algorithm, ECG signals from the MIT-BIH database with different levels of noise are filtered and the SNR quality measure is calculated for each case. The simulations show the performance of the proposed algorithm for different noise intensities, and for different levels of the wavelet transform, and in comparison to two other filtering methods.;;Universidade Federal de Santa Maria - LANA - PPGI;pt_BR;Published;16;2013;2013-03-12 9:55:33
37870;Hérico Valiati;Uma Estratégia Baseada em Difusão de Informação para Determinação de Conteúdos Relevantes e Usuários Influentes em Redes Sociais;Social networks have played an increasingly fundamental role as a means for disseminating information, ideas and influences among their members. The target problem of this work is to determine both influential users and relevant content, that is, to order both groups of users and the content disseminated by them. We propose a new technique that is based on an intuitive and circular definition of relevance and influence. This technique is described in detail, as well as its efficient implementation. It was validated using two real Twitter databases for recommendation purposes. The results obtained show that the proposed technique presents gains of 37% when compared to a collaborative filtering method, at the same time that both influential users and relevant content are qualitatively superior. We also present a case study, in which we apply the proposed technique in a real Web scenario. The analyzes prove that our method can identify influential users and can be easily adopted in different application scenarios.;;Departamento de Ciência da Computação – Universidade  Federal de Minas Gerais;pt_BR;Published;25;2013;2013-03-16 23:41:52
37870;Arlei Silva;Uma Estratégia Baseada em Difusão de Informação para Determinação de Conteúdos Relevantes e Usuários Influentes em Redes Sociais;Social networks have played an increasingly fundamental role as a means for disseminating information, ideas and influences among their members. The target problem of this work is to determine both influential users and relevant content, that is, to order both groups of users and the content disseminated by them. We propose a new technique that is based on an intuitive and circular definition of relevance and influence. This technique is described in detail, as well as its efficient implementation. It was validated using two real Twitter databases for recommendation purposes. The results obtained show that the proposed technique presents gains of 37% when compared to a collaborative filtering method, at the same time that both influential users and relevant content are qualitatively superior. We also present a case study, in which we apply the proposed technique in a real Web scenario. The analyzes prove that our method can identify influential users and can be easily adopted in different application scenarios.;;Computer Science Department – University of California, Santa Barbara;pt_BR;Published;25;2013;2013-03-16 23:41:52
37870;Sara Guimarães;Uma Estratégia Baseada em Difusão de Informação para Determinação de Conteúdos Relevantes e Usuários Influentes em Redes Sociais;Social networks have played an increasingly fundamental role as a means for disseminating information, ideas and influences among their members. The target problem of this work is to determine both influential users and relevant content, that is, to order both groups of users and the content disseminated by them. We propose a new technique that is based on an intuitive and circular definition of relevance and influence. This technique is described in detail, as well as its efficient implementation. It was validated using two real Twitter databases for recommendation purposes. The results obtained show that the proposed technique presents gains of 37% when compared to a collaborative filtering method, at the same time that both influential users and relevant content are qualitatively superior. We also present a case study, in which we apply the proposed technique in a real Web scenario. The analyzes prove that our method can identify influential users and can be easily adopted in different application scenarios.;;Departamento de Ciência da Computação – Universidade  Federal de Minas Gerais;pt_BR;Published;25;2013;2013-03-16 23:41:52
37870;Wagner Meira Jr.;Uma Estratégia Baseada em Difusão de Informação para Determinação de Conteúdos Relevantes e Usuários Influentes em Redes Sociais;Social networks have played an increasingly fundamental role as a means for disseminating information, ideas and influences among their members. The target problem of this work is to determine both influential users and relevant content, that is, to order both groups of users and the content disseminated by them. We propose a new technique that is based on an intuitive and circular definition of relevance and influence. This technique is described in detail, as well as its efficient implementation. It was validated using two real Twitter databases for recommendation purposes. The results obtained show that the proposed technique presents gains of 37% when compared to a collaborative filtering method, at the same time that both influential users and relevant content are qualitatively superior. We also present a case study, in which we apply the proposed technique in a real Web scenario. The analyzes prove that our method can identify influential users and can be easily adopted in different application scenarios.;;Departamento de Ciência da Computação – Universidade  Federal de Minas Gerais;pt_BR;Published;25;2013;2013-03-16 23:41:52
39702;Celso Antonio Alves Kaestner;Support Vector Machines and Kernel Functions for Text Processing;This work presents kernel functions that can be used in conjunction with the Support Vector Machine – SVM – learning algorithm to solve the automatic text classification task. Initially the Vector Space Model for text processing is presented. According to this model text is seen as a set of vectors in a high dimensional space then extensions and alternative models are derived, and some preprocessing procedures are discussed. The SVM learning algorithm, largely employed for text classification, is outlined: its decision procedure is obtained as a solution of an optimization problem. The “kernel trick”, that allows the algorithm to be applied in non-linearly separable cases, is presented, as well as some kernel functions that are currently used in text applications. Finally some text classification experiments employing the SVM classifier are conducted, in order to illustrate some text preprocessing techniques and the presented kernel functions.;;Informatics DepartmentFederal University of Technology - Paraná - UTFPRAv. Sete de Setembro, 3165Curitiba, Paraná - Brazil80.230-901Phone: +55 (41) 3310-4644Fax: +55 (41) 3310-4646;en_US;Published;24;2013;2013-05-06 13:13:48
40202;Soraia de Souza Reis;Assessing the Semiotic Inspection Method – The Evaluators’ Perspective;This paper presents an assessment of the Semiotic Inspection Methodaimed at understanding its costs, benefits, advantages and disadvantages from the evaluators’perspective. We applied a questionnaire to novice evaluators and interviewedthe authors of the method (representing the experts’ evaluators). An analysis of theresponses shows interesting insights and characteristics of the method.;;Universidade Federal de Minas Gerais;pt_BR;Published;27;2013;2013-05-28 11:01:44
40202;Raquel Oliveira Prates;Assessing the Semiotic Inspection Method – The Evaluators’ Perspective;This paper presents an assessment of the Semiotic Inspection Methodaimed at understanding its costs, benefits, advantages and disadvantages from the evaluators’perspective. We applied a questionnaire to novice evaluators and interviewedthe authors of the method (representing the experts’ evaluators). An analysis of theresponses shows interesting insights and characteristics of the method.;;Universidade Federal de Minas Gerais;pt_BR;Published;27;2013;2013-05-28 11:01:44
40994;Evando Carlos Pessini;Expressiveness of Automatic Semantic Web Service Composition Approaches: A Survey based on Workﬂow Patterns;This survey evaluates a number of techniques for the automatic composition of Semantic Web Services. The evaluation takes into account the different workflow patterns that can be expressed by the solutions proposed by each individual technique. Only fully automatic approaches are considered here, i.e., composition techniques that do not demand user interference in the composition process nor rely on complex input (such as templates or other forms of abstract composition models). The main contribution of this paper is to discuss the strengths and weaknesses of the approaches presented in the literature for the automatic generation of semantic web services composition from the workflow patterns perspective.;;Universidade Tecnológica Federal do Paraná;en_US;Published;31;2013;2013-07-05 14:55:03
41936;Lucas Carvalho;Introdução aos Sistemas de Recomendação para Grupos;Recommender Systems traditionally recommend items to individual users. In some scenarios, however, recommending a group of individuals is more appropriate. There is not yet a good volume of scientific work focused on so-called Recommendation Systems for Groups. One of the great peculiarities of these systems is, for example, how to properly deal with the preferences of their members when generating recommendations. With the aim of contributing to the evolution of related research here in Brazil, this tutorial addresses this and other issues related to the topic. After presenting and discussing several important aspects, such as classifications, main issues, strategies for aggregating individual preferences, forms of evaluation, alternative approaches in progress and recommending the sequence of items, this tutorial also contributes by proposing a new aggregation strategy and carry out the entire appropriate experimentation process, including comparative evaluation with the most established strategies in the literature.;;Universidade Federal de SergipeDepartamento de Computação (DCOMP/CCET/UFS);pt_BR;Published;32;2013;2013-08-19 18:39:51
41936;Hendrik Macedo;Introdução aos Sistemas de Recomendação para Grupos;Recommender Systems traditionally recommend items to individual users. In some scenarios, however, recommending a group of individuals is more appropriate. There is not yet a good volume of scientific work focused on so-called Recommendation Systems for Groups. One of the great peculiarities of these systems is, for example, how to properly deal with the preferences of their members when generating recommendations. With the aim of contributing to the evolution of related research here in Brazil, this tutorial addresses this and other issues related to the topic. After presenting and discussing several important aspects, such as classifications, main issues, strategies for aggregating individual preferences, forms of evaluation, alternative approaches in progress and recommending the sequence of items, this tutorial also contributes by proposing a new aggregation strategy and carry out the entire appropriate experimentation process, including comparative evaluation with the most established strategies in the literature.;;Universidade Federal de SergipeDepartamento de Computação (DCOMP/CCET/UFS);pt_BR;Published;32;2013;2013-08-19 18:39:51
43300;Francisca Raquel de Vasconcelos Silveira;Seleção de Casos de Teste para Agentes Racionais;Rational agents are a promising computing technology for the development of complex distributed systems. Despite the references available to guide the designer of these agents, there are few testing techniques proposed to validate these systems. It is known that this validation depends on the selected test cases which must provide information regarding the components in the agent structure that are performing unsatisfactorily. This work presents a formulation for the problem of selecting test cases for rational agents through a solution approach based on a utility-oriented agent to select test cases through population-based metaheuristics to test the performance of rational agents. For each test case, interactions between Agent and Environment are performed to obtain the corresponding performance evaluation. As a result, we obtain a set of test cases in which the agent was not well evaluated, with the respective evaluation. Based on this result, the approach identifies the objectives that are not being satisfied and the failures presented by the agent and passes them on to the designer.;;IFCE;pt_BR;Published;14;2013;2013-10-30 0:38:34
43300;Gustavo Lima de Campos;Seleção de Casos de Teste para Agentes Racionais;Rational agents are a promising computing technology for the development of complex distributed systems. Despite the references available to guide the designer of these agents, there are few testing techniques proposed to validate these systems. It is known that this validation depends on the selected test cases which must provide information regarding the components in the agent structure that are performing unsatisfactorily. This work presents a formulation for the problem of selecting test cases for rational agents through a solution approach based on a utility-oriented agent to select test cases through population-based metaheuristics to test the performance of rational agents. For each test case, interactions between Agent and Environment are performed to obtain the corresponding performance evaluation. As a result, we obtain a set of test cases in which the agent was not well evaluated, with the respective evaluation. Based on this result, the approach identifies the objectives that are not being satisfied and the failures presented by the agent and passes them on to the designer.;;UECE;pt_BR;Published;14;2013;2013-10-30 0:38:34
43300;Mariela Inés Cortés;Seleção de Casos de Teste para Agentes Racionais;Rational agents are a promising computing technology for the development of complex distributed systems. Despite the references available to guide the designer of these agents, there are few testing techniques proposed to validate these systems. It is known that this validation depends on the selected test cases which must provide information regarding the components in the agent structure that are performing unsatisfactorily. This work presents a formulation for the problem of selecting test cases for rational agents through a solution approach based on a utility-oriented agent to select test cases through population-based metaheuristics to test the performance of rational agents. For each test case, interactions between Agent and Environment are performed to obtain the corresponding performance evaluation. As a result, we obtain a set of test cases in which the agent was not well evaluated, with the respective evaluation. Based on this result, the approach identifies the objectives that are not being satisfied and the failures presented by the agent and passes them on to the designer.;;UECE;pt_BR;Published;14;2013;2013-10-30 0:38:34
43468;Douglas Melo;XIRU: Interface de Rede Extensível para Integração de Núcleos a uma Rede-em-Chip;To integrate the cores of an integrated system into a NoC (Network-on-Chip), it is necessary to use communication interfaces that adapt the cores' protocol to that of the network and that offer the necessary communication services to the cores. This article describes the architecture of an extensible network interface for integrating cores into the SoCIN (System-on-Chip Interconnection Network) NoC. The proposed interface uses an architecture structured in three layers that perform protocol adaptation, data packaging/unpacking and packet sending/reception, among other services. The article describes the architecture of the network interface and presents the results of its validation and synthesis in silicon.;;Universidade do Vale do Itajaí;pt_BR;Published;21;2013;2013-11-06 17:49:20
43468;Michelle Wangham;XIRU: Interface de Rede Extensível para Integração de Núcleos a uma Rede-em-Chip;To integrate the cores of an integrated system into a NoC (Network-on-Chip), it is necessary to use communication interfaces that adapt the cores' protocol to that of the network and that offer the necessary communication services to the cores. This article describes the architecture of an extensible network interface for integrating cores into the SoCIN (System-on-Chip Interconnection Network) NoC. The proposed interface uses an architecture structured in three layers that perform protocol adaptation, data packaging/unpacking and packet sending/reception, among other services. The article describes the architecture of the network interface and presents the results of its validation and synthesis in silicon.;;Universidade do Vale do Itajaí;pt_BR;Published;21;2013;2013-11-06 17:49:20
43468;Cesar Zeferino;XIRU: Interface de Rede Extensível para Integração de Núcleos a uma Rede-em-Chip;To integrate the cores of an integrated system into a NoC (Network-on-Chip), it is necessary to use communication interfaces that adapt the cores' protocol to that of the network and that offer the necessary communication services to the cores. This article describes the architecture of an extensible network interface for integrating cores into the SoCIN (System-on-Chip Interconnection Network) NoC. The proposed interface uses an architecture structured in three layers that perform protocol adaptation, data packaging/unpacking and packet sending/reception, among other services. The article describes the architecture of the network interface and presents the results of its validation and synthesis in silicon.;;Universidade do Vale do Itajaí;pt_BR;Published;21;2013;2013-11-06 17:49:20
43510;Cesar Albenes Zeferino;Segurança em Redes-em-Chip:  Conceitos e Revisão do Estado da Arte;Networks-on-Chip were proposed to meet the needs of communication scalability in computing systems integrated on a single chip. Just like traditional distributed systems, an integrated system and its network are susceptible to attacks on their security properties. This article presents a bibliographic survey carried out to characterize which techniques have been used to provide security in integrated systems based on Networks-on-Chips. The works were classified according to the type of attack, the security property affected, the security mechanism used and the component adopted. The article identifies the attacks and properties most addressed by these works, as well as the main security mechanisms used and the components in which they are implemented.;;Universidade do Vale do Itajaí;pt_BR;Published;16;2013;2013-11-07 10:18:01
43510;Sidnei Baron;Segurança em Redes-em-Chip:  Conceitos e Revisão do Estado da Arte;Networks-on-Chip were proposed to meet the needs of communication scalability in computing systems integrated on a single chip. Just like traditional distributed systems, an integrated system and its network are susceptible to attacks on their security properties. This article presents a bibliographic survey carried out to characterize which techniques have been used to provide security in integrated systems based on Networks-on-Chips. The works were classified according to the type of attack, the security property affected, the security mechanism used and the component adopted. The article identifies the attacks and properties most addressed by these works, as well as the main security mechanisms used and the components in which they are implemented.;;Universidade do Vale do Itajaí;pt_BR;Published;16;2013;2013-11-07 10:18:01
43510;Michelle Silva Wangham;Segurança em Redes-em-Chip:  Conceitos e Revisão do Estado da Arte;Networks-on-Chip were proposed to meet the needs of communication scalability in computing systems integrated on a single chip. Just like traditional distributed systems, an integrated system and its network are susceptible to attacks on their security properties. This article presents a bibliographic survey carried out to characterize which techniques have been used to provide security in integrated systems based on Networks-on-Chips. The works were classified according to the type of attack, the security property affected, the security mechanism used and the component adopted. The article identifies the attacks and properties most addressed by these works, as well as the main security mechanisms used and the components in which they are implemented.;;Universidade do Vale do Itajaí;pt_BR;Published;16;2013;2013-11-07 10:18:01
44199;André Assis Lôbo de Oliveira;O Teste de Mutação apoiado pelo Algoritmo Genético Coevolucionário com Classificação Genética Controlada;This article is in the field of co-evolutionary genetic algorithms that aim to select good subsets of test cases and mutants, in the context of Mutation Testing. From this field of study, two existing approaches were selected and evaluated. This assessment supported the development of a new Coevolutionary Algorithm with Controlled Genetic Classification (AGC − CGC). To analyze the approach, 164 experiments were carried out comparing the results of the proposed algorithm with three other methods applied in four real benchmarks. The results reveal a significant improvement of AGC − CGC over the other approaches when considering increasing the mutation score without markedly increasing the execution time.;;Uiversidade Federal de Goiás;pt_BR;Published;25;2013;2013-12-14 22:23:45
44199;Celso G Camilo Junior;O Teste de Mutação apoiado pelo Algoritmo Genético Coevolucionário com Classificação Genética Controlada;This article is in the field of co-evolutionary genetic algorithms that aim to select good subsets of test cases and mutants, in the context of Mutation Testing. From this field of study, two existing approaches were selected and evaluated. This assessment supported the development of a new Coevolutionary Algorithm with Controlled Genetic Classification (AGC − CGC). To analyze the approach, 164 experiments were carried out comparing the results of the proposed algorithm with three other methods applied in four real benchmarks. The results reveal a significant improvement of AGC − CGC over the other approaches when considering increasing the mutation score without markedly increasing the execution time.;;-;pt_BR;Published;25;2013;2013-12-14 22:23:45
44199;Auri Marcelo Rizzo Vincenzi;O Teste de Mutação apoiado pelo Algoritmo Genético Coevolucionário com Classificação Genética Controlada;This article is in the field of co-evolutionary genetic algorithms that aim to select good subsets of test cases and mutants, in the context of Mutation Testing. From this field of study, two existing approaches were selected and evaluated. This assessment supported the development of a new Coevolutionary Algorithm with Controlled Genetic Classification (AGC − CGC). To analyze the approach, 164 experiments were carried out comparing the results of the proposed algorithm with three other methods applied in four real benchmarks. The results reveal a significant improvement of AGC − CGC over the other approaches when considering increasing the mutation score without markedly increasing the execution time.;;-;pt_BR;Published;25;2013;2013-12-14 22:23:45
46848;Eanes Torres Pereira;Local Binary Patterns Applied to Breast Cancer Classification in Mammographies;Among all cancer types, breast cancer is the one with the second highest incidence rate for women. Mammography is the most used method for breast cancer detection, as it reveals abnormalities such as masses, calcifications, asymmetries and architectural distortions. In this paper, we propose a classification method for breast cancer that has been tested for six different cancer types: CALC, CIRC, SPIC, MISC, ARCH, ASYM. The proposed approach is composed of a SVM classifier trained with LBP features. The MIAS image database was used in the experiments and ROC curves were generated. To the best of our knowledge, our approach is the first to handle those six different cancer types using the same technique. One important result of the proposed approach is that it was tested over six different breast cancer types proving to be generic enough to obtain high classification results in all cases.;;Universidade Federal de Campina Grande;en_US;Published;14;2014;2014-05-06 9:47:47
46848;Sidney Pimentel Eleutério;Local Binary Patterns Applied to Breast Cancer Classification in Mammographies;Among all cancer types, breast cancer is the one with the second highest incidence rate for women. Mammography is the most used method for breast cancer detection, as it reveals abnormalities such as masses, calcifications, asymmetries and architectural distortions. In this paper, we propose a classification method for breast cancer that has been tested for six different cancer types: CALC, CIRC, SPIC, MISC, ARCH, ASYM. The proposed approach is composed of a SVM classifier trained with LBP features. The MIAS image database was used in the experiments and ROC curves were generated. To the best of our knowledge, our approach is the first to handle those six different cancer types using the same technique. One important result of the proposed approach is that it was tested over six different breast cancer types proving to be generic enough to obtain high classification results in all cases.;;Universidade Estadual da Paraíba;en_US;Published;14;2014;2014-05-06 9:47:47
46848;João Marques Carvalho;Local Binary Patterns Applied to Breast Cancer Classification in Mammographies;Among all cancer types, breast cancer is the one with the second highest incidence rate for women. Mammography is the most used method for breast cancer detection, as it reveals abnormalities such as masses, calcifications, asymmetries and architectural distortions. In this paper, we propose a classification method for breast cancer that has been tested for six different cancer types: CALC, CIRC, SPIC, MISC, ARCH, ASYM. The proposed approach is composed of a SVM classifier trained with LBP features. The MIAS image database was used in the experiments and ROC curves were generated. To the best of our knowledge, our approach is the first to handle those six different cancer types using the same technique. One important result of the proposed approach is that it was tested over six different breast cancer types proving to be generic enough to obtain high classification results in all cases.;;Universidade Federal de Campina Grande;en_US;Published;14;2014;2014-05-06 9:47:47
48184;Leanderson André;Tutorial Sobre o Uso de Técnicas para Controle de Parâmetros em Algoritmos de Inteligência de Enxame e Computação Evolutiva;Nature has been a great source of inspiration for developing computational approaches to optimization. Two large groups that represent this class of biologically inspired algorithms are Swarm Intelligence and Evolutionary Computing. Such algorithms are called metaheuristics and are recognized as efficient approaches for solving complex problems. Both Swarm Intelligence and Evolutionary Computing algorithms share common characteristics such as the use of stochastic components during the optimization process and varied configuration parameters. Adjusting the parameters of an algorithm plays an important role in defining its behavior, guiding the search and, consequently, interfering in the quality of the solutions found. However, adjusting the parameters is not a simple task, becoming an optimization problem within the problem being optimized. Furthermore, a suitable setting for the parameters may change during the optimization process. There are two ways to adjust the parameters of an algorithm. The \textit{offline} adjustment that is performed before executing the algorithm and the parameter values ​​remain fixed, and the \textit{online} control where the parameter values ​​can change during the optimization process. This article focuses on reviewing the \textit{online} control strategies for parameters applied in the main algorithms of Evolutionary Computing and Swarm Intelligence. As a result, this review analyzes and points out the main techniques and algorithms used and suggests some directions for future research.;;Universidade do Estado de Santa Catarina - UDESC;pt_BR;Published;45;2014;2014-06-27 16:53:28
48184;Rafael Stubs Parpinelli;Tutorial Sobre o Uso de Técnicas para Controle de Parâmetros em Algoritmos de Inteligência de Enxame e Computação Evolutiva;Nature has been a great source of inspiration for developing computational approaches to optimization. Two large groups that represent this class of biologically inspired algorithms are Swarm Intelligence and Evolutionary Computing. Such algorithms are called metaheuristics and are recognized as efficient approaches for solving complex problems. Both Swarm Intelligence and Evolutionary Computing algorithms share common characteristics such as the use of stochastic components during the optimization process and varied configuration parameters. Adjusting the parameters of an algorithm plays an important role in defining its behavior, guiding the search and, consequently, interfering in the quality of the solutions found. However, adjusting the parameters is not a simple task, becoming an optimization problem within the problem being optimized. Furthermore, a suitable setting for the parameters may change during the optimization process. There are two ways to adjust the parameters of an algorithm. The \textit{offline} adjustment that is performed before executing the algorithm and the parameter values ​​remain fixed, and the \textit{online} control where the parameter values ​​can change during the optimization process. This article focuses on reviewing the \textit{online} control strategies for parameters applied in the main algorithms of Evolutionary Computing and Swarm Intelligence. As a result, this review analyzes and points out the main techniques and algorithms used and suggests some directions for future research.;;Universidade do Estado de Santa Catarina - UDESC;pt_BR;Published;45;2014;2014-06-27 16:53:28
48395;Geisla de Albuquerque Melo;Reconhecimento de Padrões em Tecnologia Portável – Um Estudo de Caso;Digital image processing (PDI) is present in several areas of knowledge, such as medicine and engineering. It is directly linked to automatic pattern recognition. Artificial neural networks (ANNs) have been used to assist in this process, aiming for greater reliability. Android is the most used platform on cell phones in the world. Brazil is the second largest soybean producer in the world, but the diseases that attack it are a limiting factor in production. The objective of this work was to automatically recognize the severity levels of soybean powdery mildew. Leaves with and without the disease were photographed. The photos went through filters and had color and texture characteristics extracted via PDI. These characteristics were training and testing data from an ANN, which classified the levels. The mobile technology platform was a smartphone with the Android operating system. An improvement in the prediction accuracy rate was observed according to combinations of characteristics, for the same RNA structure. In this case study, it was concluded, comparing two possible sets of data, that the set with texture characteristics obtained better assertiveness. The feasibility of the study was confirmed, which opens the way for new research opportunities, aiming to improve the techniques used.;;Universidade Estadual de Ponta Grossa;pt_BR;Published;8;2014;2014-07-04 3:09:18
48395;Ivo Mario Mathias;Reconhecimento de Padrões em Tecnologia Portável – Um Estudo de Caso;Digital image processing (PDI) is present in several areas of knowledge, such as medicine and engineering. It is directly linked to automatic pattern recognition. Artificial neural networks (ANNs) have been used to assist in this process, aiming for greater reliability. Android is the most used platform on cell phones in the world. Brazil is the second largest soybean producer in the world, but the diseases that attack it are a limiting factor in production. The objective of this work was to automatically recognize the severity levels of soybean powdery mildew. Leaves with and without the disease were photographed. The photos went through filters and had color and texture characteristics extracted via PDI. These characteristics were training and testing data from an ANN, which classified the levels. The mobile technology platform was a smartphone with the Android operating system. An improvement in the prediction accuracy rate was observed according to combinations of characteristics, for the same RNA structure. In this case study, it was concluded, comparing two possible sets of data, that the set with texture characteristics obtained better assertiveness. The feasibility of the study was confirmed, which opens the way for new research opportunities, aiming to improve the techniques used.;;Universidade Estadual de Ponta Grossa;pt_BR;Published;8;2014;2014-07-04 3:09:18
48395;Ariângelo Hauer Dias;Reconhecimento de Padrões em Tecnologia Portável – Um Estudo de Caso;Digital image processing (PDI) is present in several areas of knowledge, such as medicine and engineering. It is directly linked to automatic pattern recognition. Artificial neural networks (ANNs) have been used to assist in this process, aiming for greater reliability. Android is the most used platform on cell phones in the world. Brazil is the second largest soybean producer in the world, but the diseases that attack it are a limiting factor in production. The objective of this work was to automatically recognize the severity levels of soybean powdery mildew. Leaves with and without the disease were photographed. The photos went through filters and had color and texture characteristics extracted via PDI. These characteristics were training and testing data from an ANN, which classified the levels. The mobile technology platform was a smartphone with the Android operating system. An improvement in the prediction accuracy rate was observed according to combinations of characteristics, for the same RNA structure. In this case study, it was concluded, comparing two possible sets of data, that the set with texture characteristics obtained better assertiveness. The feasibility of the study was confirmed, which opens the way for new research opportunities, aiming to improve the techniques used.;;Universidade Estadual de Ponta Grossa;pt_BR;Published;8;2014;2014-07-04 3:09:18
49045;Tiago Prado Oliveira;Predição do tráfego de rede de computadores usando redes neurais tradicionais e de aprendizagem profunda;In this article, four different approaches for predicting computer network traffic are compared, using traffic from computer network devices that connect to the Internet and using Artificial Neural Networks (ANN) for prediction, namely: (1)  Multilayer Perceptron (MLP) with Backpropagation for training (2) MLP with Resilient Backpropagation (Rprop) (3) Recurrent Neural Network (RNN) (4)  Stacked Autoencoder (SAE) with deep learning. It is also presented that a simpler neural network model, such as RNN and MLP, can be more efficient than more complex models, such as SAE. Internet traffic prediction is an important task for many applications, such as adaptive applications, congestion control, admission control, anomaly detection, and bandwidth allocation. Additionally, efficient resource management methods such as bandwidth can be used to improve performance and reduce costs by improving Quality of Service (QoS). The popularity of new deep neural networks has increased in many areas, but there is a lack of studies regarding the prediction of time series, such as Internet traffic.;;Universidade Federal de Uberlândia;pt_BR;Published;18;2014;2014-07-22 9:26:43
49045;Jamil Salem Barbar;Predição do tráfego de rede de computadores usando redes neurais tradicionais e de aprendizagem profunda;In this article, four different approaches for predicting computer network traffic are compared, using traffic from computer network devices that connect to the Internet and using Artificial Neural Networks (ANN) for prediction, namely: (1)  Multilayer Perceptron (MLP) with Backpropagation for training (2) MLP with Resilient Backpropagation (Rprop) (3) Recurrent Neural Network (RNN) (4)  Stacked Autoencoder (SAE) with deep learning. It is also presented that a simpler neural network model, such as RNN and MLP, can be more efficient than more complex models, such as SAE. Internet traffic prediction is an important task for many applications, such as adaptive applications, congestion control, admission control, anomaly detection, and bandwidth allocation. Additionally, efficient resource management methods such as bandwidth can be used to improve performance and reduce costs by improving Quality of Service (QoS). The popularity of new deep neural networks has increased in many areas, but there is a lack of studies regarding the prediction of time series, such as Internet traffic.;;Universidade Federal de Uberlândia;pt_BR;Published;18;2014;2014-07-22 9:26:43
49045;Alexsandro Santos Soares;Predição do tráfego de rede de computadores usando redes neurais tradicionais e de aprendizagem profunda;In this article, four different approaches for predicting computer network traffic are compared, using traffic from computer network devices that connect to the Internet and using Artificial Neural Networks (ANN) for prediction, namely: (1)  Multilayer Perceptron (MLP) with Backpropagation for training (2) MLP with Resilient Backpropagation (Rprop) (3) Recurrent Neural Network (RNN) (4)  Stacked Autoencoder (SAE) with deep learning. It is also presented that a simpler neural network model, such as RNN and MLP, can be more efficient than more complex models, such as SAE. Internet traffic prediction is an important task for many applications, such as adaptive applications, congestion control, admission control, anomaly detection, and bandwidth allocation. Additionally, efficient resource management methods such as bandwidth can be used to improve performance and reduce costs by improving Quality of Service (QoS). The popularity of new deep neural networks has increased in many areas, but there is a lack of studies regarding the prediction of time series, such as Internet traffic.;;Universidade Federal de Uberlândia;pt_BR;Published;18;2014;2014-07-22 9:26:43
49242;Thiago Assis de Oliveira Rodrigues;Predição de função de proteínas através da extração de características físico-químicas;With the conclusion of the Genome project, the number of new proteins discovered has grown, but due to the high cost and slow process of discovering the function of proteins, only a small portion of them have their function known. This work presents a methodology for predicting protein function through the extraction of characteristics from their structures, present in the StingDB database, the use of the Discrete Cosine Transform, the coding of the primary structure, class balancing and the use of Machines of Support Vectors. The average values ​​obtained for precision, sensitivity, accuracy and specificity were 80%, 71%, 74% and 77%, respectively. The results were compared with other works in the literature, and showed a 10% increase in the accuracy rate.;;PUC MINAS;pt_BR;Published;22;2014;2014-07-29 23:33:48
49242;Larissa Fernandes Leijôto;Predição de função de proteínas através da extração de características físico-químicas;With the conclusion of the Genome project, the number of new proteins discovered has grown, but due to the high cost and slow process of discovering the function of proteins, only a small portion of them have their function known. This work presents a methodology for predicting protein function through the extraction of characteristics from their structures, present in the StingDB database, the use of the Discrete Cosine Transform, the coding of the primary structure, class balancing and the use of Machines of Support Vectors. The average values ​​obtained for precision, sensitivity, accuracy and specificity were 80%, 71%, 74% and 77%, respectively. The results were compared with other works in the literature, and showed a 10% increase in the accuracy rate.;;PUC MINAS;pt_BR;Published;22;2014;2014-07-29 23:33:48
49242;Poliane C. Oliveira Brandão;Predição de função de proteínas através da extração de características físico-químicas;With the conclusion of the Genome project, the number of new proteins discovered has grown, but due to the high cost and slow process of discovering the function of proteins, only a small portion of them have their function known. This work presents a methodology for predicting protein function through the extraction of characteristics from their structures, present in the StingDB database, the use of the Discrete Cosine Transform, the coding of the primary structure, class balancing and the use of Machines of Support Vectors. The average values ​​obtained for precision, sensitivity, accuracy and specificity were 80%, 71%, 74% and 77%, respectively. The results were compared with other works in the literature, and showed a 10% increase in the accuracy rate.;;Pontifícia Universidade Católica de Minas Gerais;pt_BR;Published;22;2014;2014-07-29 23:33:48
49242;Cristiane Neri Nobre;Predição de função de proteínas através da extração de características físico-químicas;With the conclusion of the Genome project, the number of new proteins discovered has grown, but due to the high cost and slow process of discovering the function of proteins, only a small portion of them have their function known. This work presents a methodology for predicting protein function through the extraction of characteristics from their structures, present in the StingDB database, the use of the Discrete Cosine Transform, the coding of the primary structure, class balancing and the use of Machines of Support Vectors. The average values ​​obtained for precision, sensitivity, accuracy and specificity were 80%, 71%, 74% and 77%, respectively. The results were compared with other works in the literature, and showed a 10% increase in the accuracy rate.;;-;pt_BR;Published;22;2014;2014-07-29 23:33:48
49491;Thiago Teixeira Santos;SciPy and OpenCV as an interactive computing environment for computer vision;In research and development (R&D), interactive computing environments are a frequently employed alternative for data exploration, algorithm development and prototyping. In the last twelve years, a popular scientific computing environment flourished around the Python programming language. Most of this environment is part of (or built over) a software stack named SciPy Stack. Combined with OpenCV’s Python interface, this environment becomes an alternative for current computer vision R&D. This tutorial introduces such an environment and shows how it can address different steps of computer vision research, from initial data exploration to parallel computing implementations. Several code examples are presented. They deal with problems from simple image processing to inference by machine learning. All examples are also available as IPython notebooks.;;Embrapa Agricultural Informatics;en_US;Published;35;2014;2014-08-12 14:29:54
49492;Tiago Jose de Carvalho;Visual Computing and Machine Learning Techniques for Digital Forensics;It is impressive how fast science has improved day by day in so many different fields. In special, technology advances are shocking so many people bringing to their reality facts that previously were beyond their imagination. Inspired by methods earlier presented in scientific fiction shows, the computer science community has created a new research area named Digital Forensics, which aims at developing and deploying methods for fighting against digital crimes such as digital image forgery.This work presents some of the main concepts associated with Digital Forensics and, complementarily, presents some recent and powerful techniques relying on Computer Graphics, Image Processing, Computer Vision and Machine Learning concepts for detecting forgeries in photographs. Some topics addressed in this work include: sourceattribution, spoofing detection, pornography detection, multimedia phylogeny, and forgery detection. Finally, this work highlights the challenges and open problems in Digital Image Forensics to provide the readers with the myriad opportunities available for research.;;University of Campinas;en_US;Published;25;2014;2014-08-10 10:20:31
49492;Helio Pedrini;Visual Computing and Machine Learning Techniques for Digital Forensics;It is impressive how fast science has improved day by day in so many different fields. In special, technology advances are shocking so many people bringing to their reality facts that previously were beyond their imagination. Inspired by methods earlier presented in scientific fiction shows, the computer science community has created a new research area named Digital Forensics, which aims at developing and deploying methods for fighting against digital crimes such as digital image forgery.This work presents some of the main concepts associated with Digital Forensics and, complementarily, presents some recent and powerful techniques relying on Computer Graphics, Image Processing, Computer Vision and Machine Learning concepts for detecting forgeries in photographs. Some topics addressed in this work include: sourceattribution, spoofing detection, pornography detection, multimedia phylogeny, and forgery detection. Finally, this work highlights the challenges and open problems in Digital Image Forensics to provide the readers with the myriad opportunities available for research.;;University of Campinas;en_US;Published;25;2014;2014-08-10 10:20:31
49492;Anderson de Rezende Rocha;Visual Computing and Machine Learning Techniques for Digital Forensics;It is impressive how fast science has improved day by day in so many different fields. In special, technology advances are shocking so many people bringing to their reality facts that previously were beyond their imagination. Inspired by methods earlier presented in scientific fiction shows, the computer science community has created a new research area named Digital Forensics, which aims at developing and deploying methods for fighting against digital crimes such as digital image forgery.This work presents some of the main concepts associated with Digital Forensics and, complementarily, presents some recent and powerful techniques relying on Computer Graphics, Image Processing, Computer Vision and Machine Learning concepts for detecting forgeries in photographs. Some topics addressed in this work include: sourceattribution, spoofing detection, pornography detection, multimedia phylogeny, and forgery detection. Finally, this work highlights the challenges and open problems in Digital Image Forensics to provide the readers with the myriad opportunities available for research.;;University of Campinas;en_US;Published;25;2014;2014-08-10 10:20:31
49694;Santiago Viertel;Programação matemática e imersões métricas para aproximações em problemas de corte;In this work, we present a theoretical study on NP-hard graph cut optimization problems and the state of the art of approximation algorithms that make use of mathematical programming techniques and metric spaces. Three graph cutting problems were modeled by entire mathematical programs, the latter being relaxed to admit solutions with continuous values. The solution of these programs describe unit vectors, for the problem of the maximum cut points on a (k − 1)-simplex, for the problem of the minimum multiseparator cut and a metric, for the problem of the most dispersed cut. Each algorithm presented makes a decision based on the solution of the mathematical program.;;Universidade Federal do Paraná (UFPR);pt_BR;Published;23;2014;2014-08-19 22:48:59
49694;André Luís Vignatti;Programação matemática e imersões métricas para aproximações em problemas de corte;In this work, we present a theoretical study on NP-hard graph cut optimization problems and the state of the art of approximation algorithms that make use of mathematical programming techniques and metric spaces. Three graph cutting problems were modeled by entire mathematical programs, the latter being relaxed to admit solutions with continuous values. The solution of these programs describe unit vectors, for the problem of the maximum cut points on a (k − 1)-simplex, for the problem of the minimum multiseparator cut and a metric, for the problem of the most dispersed cut. Each algorithm presented makes a decision based on the solution of the mathematical program.;;Universidade Federal do Paraná (UFPR);pt_BR;Published;23;2014;2014-08-19 22:48:59
51121;Juliana Kaizer Vizzotto;QJava: A Monadic Java Library for Quantum Programming;To help the understanding and development of quantum algorithms there is an effort focused on the investigation of new semantic models and programming languages for quantum computing. Researchers in computer science have the challenge of deve loping programming languages to support the creation, analysis, modeling and simulation of high level quantum algorithms. Based on previous works that use monads inside the programming language Haskell to elegantly explain the odd characteristics of quantum computation (like superposition and entanglement), in this work we present a monadic Java library for quantum programming. We use the extension of the programming language Java called BGGA Closure, that allow the manipulation of anonymous functions (closures) inside Java. We exemplify the use of the library with an implementation of the Toffoli quantum circuit.;;Universidade Federal de Santa Maria;en_US;Published;24;2014;2014-10-22 0:10:25
51121;Bruno Crestani Calegaro;QJava: A Monadic Java Library for Quantum Programming;To help the understanding and development of quantum algorithms there is an effort focused on the investigation of new semantic models and programming languages for quantum computing. Researchers in computer science have the challenge of deve loping programming languages to support the creation, analysis, modeling and simulation of high level quantum algorithms. Based on previous works that use monads inside the programming language Haskell to elegantly explain the odd characteristics of quantum computation (like superposition and entanglement), in this work we present a monadic Java library for quantum programming. We use the extension of the programming language Java called BGGA Closure, that allow the manipulation of anonymous functions (closures) inside Java. We exemplify the use of the library with an implementation of the Toffoli quantum circuit.;;Instituto Federal de Santa Catarina;en_US;Published;24;2014;2014-10-22 0:10:25
51452;Carlos Henrique Villa Pinto;Deconvolução Cega Aplicada à Correção de Artefatos de Movimento em Imagens de Vídeo de Microscopia Intravital para Detecção Automática de Leucócitos;Intravital microscopy (IM) is a powerful tool used in the biomedical field for the in vivo observation and imaging of biological systems, having an important application in the study of leukocyte-endothelium interactions that occur in the microcirculation of various animal tissues under normal and pathological conditions. Given that the control of image acquisition conditions in in vivo analysis is quite limited, the presence of blur and motion artifacts in the acquired video frame sequences, resulting from the animal's inevitable breathing and heartbeat, constitutes one of the main MI obstacles. This significantly compromises the detection and tracking of leukocytes throughout the frames, both in visual analyzes and those carried out by automatic (computational) methods. Thus, this work carried out the study and application of two blind deconvolution techniques (deconvblind algorithm and Shan et al. method) for the correction of motion artifacts in MI video images, making a quantitative assessment of how such restoration affects the performance of two automatic leukocyte detection methods (pattern matching and local phase symmetry). Precision versus recall analyzes were conducted in order to evaluate the effectiveness of automatic detection in images of good visual quality, images degraded by motion and images restored with each technique. From this study, it was concluded that the method by Shan et al. has the greatest potential for leukocyte restoration and that blind deconvolution techniques indeed have utility as a pre-processing step for MI videos for in vivo analyses, especially when the automatic leukocyte detection method is more sensitive to errors.;;Universidade Federal de São Carlos (UFSCar);pt_BR;Published;22;2014;2014-11-10 10:42:18
51452;Bruno César Gregório da Silva;Deconvolução Cega Aplicada à Correção de Artefatos de Movimento em Imagens de Vídeo de Microscopia Intravital para Detecção Automática de Leucócitos;Intravital microscopy (IM) is a powerful tool used in the biomedical field for the in vivo observation and imaging of biological systems, having an important application in the study of leukocyte-endothelium interactions that occur in the microcirculation of various animal tissues under normal and pathological conditions. Given that the control of image acquisition conditions in in vivo analysis is quite limited, the presence of blur and motion artifacts in the acquired video frame sequences, resulting from the animal's inevitable breathing and heartbeat, constitutes one of the main MI obstacles. This significantly compromises the detection and tracking of leukocytes throughout the frames, both in visual analyzes and those carried out by automatic (computational) methods. Thus, this work carried out the study and application of two blind deconvolution techniques (deconvblind algorithm and Shan et al. method) for the correction of motion artifacts in MI video images, making a quantitative assessment of how such restoration affects the performance of two automatic leukocyte detection methods (pattern matching and local phase symmetry). Precision versus recall analyzes were conducted in order to evaluate the effectiveness of automatic detection in images of good visual quality, images degraded by motion and images restored with each technique. From this study, it was concluded that the method by Shan et al. has the greatest potential for leukocyte restoration and that blind deconvolution techniques indeed have utility as a pre-processing step for MI videos for in vivo analyses, especially when the automatic leukocyte detection method is more sensitive to errors.;;Universidade Federal de São Carlos (UFSCar);pt_BR;Published;22;2014;2014-11-10 10:42:18
51452;Paulo Guilherme de Lima Freire;Deconvolução Cega Aplicada à Correção de Artefatos de Movimento em Imagens de Vídeo de Microscopia Intravital para Detecção Automática de Leucócitos;Intravital microscopy (IM) is a powerful tool used in the biomedical field for the in vivo observation and imaging of biological systems, having an important application in the study of leukocyte-endothelium interactions that occur in the microcirculation of various animal tissues under normal and pathological conditions. Given that the control of image acquisition conditions in in vivo analysis is quite limited, the presence of blur and motion artifacts in the acquired video frame sequences, resulting from the animal's inevitable breathing and heartbeat, constitutes one of the main MI obstacles. This significantly compromises the detection and tracking of leukocytes throughout the frames, both in visual analyzes and those carried out by automatic (computational) methods. Thus, this work carried out the study and application of two blind deconvolution techniques (deconvblind algorithm and Shan et al. method) for the correction of motion artifacts in MI video images, making a quantitative assessment of how such restoration affects the performance of two automatic leukocyte detection methods (pattern matching and local phase symmetry). Precision versus recall analyzes were conducted in order to evaluate the effectiveness of automatic detection in images of good visual quality, images degraded by motion and images restored with each technique. From this study, it was concluded that the method by Shan et al. has the greatest potential for leukocyte restoration and that blind deconvolution techniques indeed have utility as a pre-processing step for MI videos for in vivo analyses, especially when the automatic leukocyte detection method is more sensitive to errors.;;Universidade Federal de São Carlos (UFSCar);pt_BR;Published;22;2014;2014-11-10 10:42:18
51452;Danielle Bernardes;Deconvolução Cega Aplicada à Correção de Artefatos de Movimento em Imagens de Vídeo de Microscopia Intravital para Detecção Automática de Leucócitos;Intravital microscopy (IM) is a powerful tool used in the biomedical field for the in vivo observation and imaging of biological systems, having an important application in the study of leukocyte-endothelium interactions that occur in the microcirculation of various animal tissues under normal and pathological conditions. Given that the control of image acquisition conditions in in vivo analysis is quite limited, the presence of blur and motion artifacts in the acquired video frame sequences, resulting from the animal's inevitable breathing and heartbeat, constitutes one of the main MI obstacles. This significantly compromises the detection and tracking of leukocytes throughout the frames, both in visual analyzes and those carried out by automatic (computational) methods. Thus, this work carried out the study and application of two blind deconvolution techniques (deconvblind algorithm and Shan et al. method) for the correction of motion artifacts in MI video images, making a quantitative assessment of how such restoration affects the performance of two automatic leukocyte detection methods (pattern matching and local phase symmetry). Precision versus recall analyzes were conducted in order to evaluate the effectiveness of automatic detection in images of good visual quality, images degraded by motion and images restored with each technique. From this study, it was concluded that the method by Shan et al. has the greatest potential for leukocyte restoration and that blind deconvolution techniques indeed have utility as a pre-processing step for MI videos for in vivo analyses, especially when the automatic leukocyte detection method is more sensitive to errors.;;Universidade Federal de Minas Gerais (UFMG);pt_BR;Published;22;2014;2014-11-10 10:42:18
51452;Juliana Carvalho Tavares;Deconvolução Cega Aplicada à Correção de Artefatos de Movimento em Imagens de Vídeo de Microscopia Intravital para Detecção Automática de Leucócitos;Intravital microscopy (IM) is a powerful tool used in the biomedical field for the in vivo observation and imaging of biological systems, having an important application in the study of leukocyte-endothelium interactions that occur in the microcirculation of various animal tissues under normal and pathological conditions. Given that the control of image acquisition conditions in in vivo analysis is quite limited, the presence of blur and motion artifacts in the acquired video frame sequences, resulting from the animal's inevitable breathing and heartbeat, constitutes one of the main MI obstacles. This significantly compromises the detection and tracking of leukocytes throughout the frames, both in visual analyzes and those carried out by automatic (computational) methods. Thus, this work carried out the study and application of two blind deconvolution techniques (deconvblind algorithm and Shan et al. method) for the correction of motion artifacts in MI video images, making a quantitative assessment of how such restoration affects the performance of two automatic leukocyte detection methods (pattern matching and local phase symmetry). Precision versus recall analyzes were conducted in order to evaluate the effectiveness of automatic detection in images of good visual quality, images degraded by motion and images restored with each technique. From this study, it was concluded that the method by Shan et al. has the greatest potential for leukocyte restoration and that blind deconvolution techniques indeed have utility as a pre-processing step for MI videos for in vivo analyses, especially when the automatic leukocyte detection method is more sensitive to errors.;;Universidade Federal de Minas Gerais (UFMG);pt_BR;Published;22;2014;2014-11-10 10:42:18
51452;Ricardo José Ferrari;Deconvolução Cega Aplicada à Correção de Artefatos de Movimento em Imagens de Vídeo de Microscopia Intravital para Detecção Automática de Leucócitos;Intravital microscopy (IM) is a powerful tool used in the biomedical field for the in vivo observation and imaging of biological systems, having an important application in the study of leukocyte-endothelium interactions that occur in the microcirculation of various animal tissues under normal and pathological conditions. Given that the control of image acquisition conditions in in vivo analysis is quite limited, the presence of blur and motion artifacts in the acquired video frame sequences, resulting from the animal's inevitable breathing and heartbeat, constitutes one of the main MI obstacles. This significantly compromises the detection and tracking of leukocytes throughout the frames, both in visual analyzes and those carried out by automatic (computational) methods. Thus, this work carried out the study and application of two blind deconvolution techniques (deconvblind algorithm and Shan et al. method) for the correction of motion artifacts in MI video images, making a quantitative assessment of how such restoration affects the performance of two automatic leukocyte detection methods (pattern matching and local phase symmetry). Precision versus recall analyzes were conducted in order to evaluate the effectiveness of automatic detection in images of good visual quality, images degraded by motion and images restored with each technique. From this study, it was concluded that the method by Shan et al. has the greatest potential for leukocyte restoration and that blind deconvolution techniques indeed have utility as a pre-processing step for MI videos for in vivo analyses, especially when the automatic leukocyte detection method is more sensitive to errors.;;Universidade Federal de São Carlos (UFSCar);pt_BR;Published;22;2014;2014-11-10 10:42:18
51756;Guilherme Chagas Kurtz;Software para Auxílio no Processo de Elaboração do Cariótipo;Many genetic abnormalities and diseases can be discovered by analyzing the shape and morphological characteristics of chromosomes. To achieve this objective, a karyotype is generally constructed, based on a photograph obtained through a microscope, through the organization and ordering of the chromosomes of a human cell according to their size. Despite major advances in cell culture, banding, collection and analysis of materials for preparing the karyotype, this process is still largely used manually, as the supply of automatic systems that assist the work of geneticists is still low. By automating this process, it is possible to speed up therapeutic procedures, obtaining results in a shorter period of time. Therefore, this work proposes the development of a tool capable of assisting geneticists in preparing the human karyotype by automating the metaphase segmentation process and identifying chromosomes through images obtained from a microscope.;;Centro Universitário Franciscano;pt_BR;Published;14;2014;2014-11-20 8:34:28
51756;Gustavo Stangherlin Cantarelli;Software para Auxílio no Processo de Elaboração do Cariótipo;Many genetic abnormalities and diseases can be discovered by analyzing the shape and morphological characteristics of chromosomes. To achieve this objective, a karyotype is generally constructed, based on a photograph obtained through a microscope, through the organization and ordering of the chromosomes of a human cell according to their size. Despite major advances in cell culture, banding, collection and analysis of materials for preparing the karyotype, this process is still largely used manually, as the supply of automatic systems that assist the work of geneticists is still low. By automating this process, it is possible to speed up therapeutic procedures, obtaining results in a shorter period of time. Therefore, this work proposes the development of a tool capable of assisting geneticists in preparing the human karyotype by automating the metaphase segmentation process and identifying chromosomes through images obtained from a microscope.;;Centro Universitário Franciscano;pt_BR;Published;14;2014;2014-11-20 8:34:28
51756;Michele Rorato Sagrillo;Software para Auxílio no Processo de Elaboração do Cariótipo;Many genetic abnormalities and diseases can be discovered by analyzing the shape and morphological characteristics of chromosomes. To achieve this objective, a karyotype is generally constructed, based on a photograph obtained through a microscope, through the organization and ordering of the chromosomes of a human cell according to their size. Despite major advances in cell culture, banding, collection and analysis of materials for preparing the karyotype, this process is still largely used manually, as the supply of automatic systems that assist the work of geneticists is still low. By automating this process, it is possible to speed up therapeutic procedures, obtaining results in a shorter period of time. Therefore, this work proposes the development of a tool capable of assisting geneticists in preparing the human karyotype by automating the metaphase segmentation process and identifying chromosomes through images obtained from a microscope.;;Centro Universitário Franciscano;pt_BR;Published;14;2014;2014-11-20 8:34:28
51756;Fernando Gomes Pires;Software para Auxílio no Processo de Elaboração do Cariótipo;Many genetic abnormalities and diseases can be discovered by analyzing the shape and morphological characteristics of chromosomes. To achieve this objective, a karyotype is generally constructed, based on a photograph obtained through a microscope, through the organization and ordering of the chromosomes of a human cell according to their size. Despite major advances in cell culture, banding, collection and analysis of materials for preparing the karyotype, this process is still largely used manually, as the supply of automatic systems that assist the work of geneticists is still low. By automating this process, it is possible to speed up therapeutic procedures, obtaining results in a shorter period of time. Therefore, this work proposes the development of a tool capable of assisting geneticists in preparing the human karyotype by automating the metaphase segmentation process and identifying chromosomes through images obtained from a microscope.;;Centro Universitário Franciscano;pt_BR;Published;14;2014;2014-11-20 8:34:28
52511;Maximiliano Cristiá;Coverage Criteria for Set-Based Specifications;Model-based testing (MBT) studies how test cases are generated from a model of the system under test (SUT). Many MBT methods rely on building an automaton from the model and then they generate test cases by covering the automaton with different path coverage criteria. However, if a model of the SUT is a logical formula over some complex mathematical theories (such as set theory) it may be more natural or intuitive to apply coverage criteria directly over the formula. On the other hand, domain partition, i.e. the partition of the input domain of model operations, is one of the main techniques in MBT. Partitioning is conducted by applying different rules or heuristics. Engineers may find it difficult to decide what, where and how these rules should be applied. In this paper we propose a set of coverage criteria based on domain partition for set-based specifications. We call them testing strategies. Testing strategies play a similar role to path- or data-based coverage criteria in structural testing. Furthermore, we show a partial order of testing strategies as is done in structural testing. We also describe an implementation of testing strategies for the Test Template Framework, which is a MBT method for the Z notation and a scripting language that allows users to implement testing strategies.;;CIFASIS and UNR;en_US;Published;19;2014;2014-12-20 12:07:30
52511;Joaquín Cuenca;Coverage Criteria for Set-Based Specifications;Model-based testing (MBT) studies how test cases are generated from a model of the system under test (SUT). Many MBT methods rely on building an automaton from the model and then they generate test cases by covering the automaton with different path coverage criteria. However, if a model of the SUT is a logical formula over some complex mathematical theories (such as set theory) it may be more natural or intuitive to apply coverage criteria directly over the formula. On the other hand, domain partition, i.e. the partition of the input domain of model operations, is one of the main techniques in MBT. Partitioning is conducted by applying different rules or heuristics. Engineers may find it difficult to decide what, where and how these rules should be applied. In this paper we propose a set of coverage criteria based on domain partition for set-based specifications. We call them testing strategies. Testing strategies play a similar role to path- or data-based coverage criteria in structural testing. Furthermore, we show a partial order of testing strategies as is done in structural testing. We also describe an implementation of testing strategies for the Test Template Framework, which is a MBT method for the Z notation and a scripting language that allows users to implement testing strategies.;;UNR;en_US;Published;19;2014;2014-12-20 12:07:30
52511;Claudia Frydman;Coverage Criteria for Set-Based Specifications;Model-based testing (MBT) studies how test cases are generated from a model of the system under test (SUT). Many MBT methods rely on building an automaton from the model and then they generate test cases by covering the automaton with different path coverage criteria. However, if a model of the SUT is a logical formula over some complex mathematical theories (such as set theory) it may be more natural or intuitive to apply coverage criteria directly over the formula. On the other hand, domain partition, i.e. the partition of the input domain of model operations, is one of the main techniques in MBT. Partitioning is conducted by applying different rules or heuristics. Engineers may find it difficult to decide what, where and how these rules should be applied. In this paper we propose a set of coverage criteria based on domain partition for set-based specifications. We call them testing strategies. Testing strategies play a similar role to path- or data-based coverage criteria in structural testing. Furthermore, we show a partial order of testing strategies as is done in structural testing. We also describe an implementation of testing strategies for the Test Template Framework, which is a MBT method for the Z notation and a scripting language that allows users to implement testing strategies.;;LSIS-CIFASIS;en_US;Published;19;2014;2014-12-20 12:07:30
52609;Junior Silva Souza;Identificação de Viabilidade de Leveduras Com Corante Vital Utilizando Histogramas de Palavras Visuais em Imagens Coloridas;This article presents a proposal to automate the viability classification of yeasts of the species Saccharomyces cerevisae, responsible for the commercial production of ethanol, using as an attribute the color absorbed by the vital dye methylene blue. The methodology is widely used in plants in Brazil and consists of counting the colorless cells that are considered viable, separating them from those colored blue, considered non-viable. The number of viable cells per liter affects industrial yield. As this counting is tiring and results in errors, we present as an alternative the computer vision technique defined as the Bag-of-Word algorithm (histogram of visual words), as well as some extensions that add color information and that can be added to the algorithm. , this is because Bag-of-Word is used for grayscale images. The attributes extracted from this algorithm with its extensions were used for testing and training classifiers extracted from supervised learning techniques. Among the techniques we use we can highlight J48, SMO, Naives Bayes and IBk that are implemented in the WEKA environment. The results were analyzed using ANOVA, which presented a p-value < 2e-16, indicating a statistical difference between the techniques analyzed. The Opponent Color technique showed better results, representing a potential for application in real plant conditions.;;Universidade Federal de Mato Grosso do Sul;pt_BR;Published;21;2014;2014-12-29 14:56:28
52609;Hemerson Pistori;Identificação de Viabilidade de Leveduras Com Corante Vital Utilizando Histogramas de Palavras Visuais em Imagens Coloridas;This article presents a proposal to automate the viability classification of yeasts of the species Saccharomyces cerevisae, responsible for the commercial production of ethanol, using as an attribute the color absorbed by the vital dye methylene blue. The methodology is widely used in plants in Brazil and consists of counting the colorless cells that are considered viable, separating them from those colored blue, considered non-viable. The number of viable cells per liter affects industrial yield. As this counting is tiring and results in errors, we present as an alternative the computer vision technique defined as the Bag-of-Word algorithm (histogram of visual words), as well as some extensions that add color information and that can be added to the algorithm. , this is because Bag-of-Word is used for grayscale images. The attributes extracted from this algorithm with its extensions were used for testing and training classifiers extracted from supervised learning techniques. Among the techniques we use we can highlight J48, SMO, Naives Bayes and IBk that are implemented in the WEKA environment. The results were analyzed using ANOVA, which presented a p-value < 2e-16, indicating a statistical difference between the techniques analyzed. The Opponent Color technique showed better results, representing a potential for application in real plant conditions.;;Universidade Católica de Dom Bosco - UCDB;pt_BR;Published;21;2014;2014-12-29 14:56:28
52609;Marney Pascoli Cereda;Identificação de Viabilidade de Leveduras Com Corante Vital Utilizando Histogramas de Palavras Visuais em Imagens Coloridas;This article presents a proposal to automate the viability classification of yeasts of the species Saccharomyces cerevisae, responsible for the commercial production of ethanol, using as an attribute the color absorbed by the vital dye methylene blue. The methodology is widely used in plants in Brazil and consists of counting the colorless cells that are considered viable, separating them from those colored blue, considered non-viable. The number of viable cells per liter affects industrial yield. As this counting is tiring and results in errors, we present as an alternative the computer vision technique defined as the Bag-of-Word algorithm (histogram of visual words), as well as some extensions that add color information and that can be added to the algorithm. , this is because Bag-of-Word is used for grayscale images. The attributes extracted from this algorithm with its extensions were used for testing and training classifiers extracted from supervised learning techniques. Among the techniques we use we can highlight J48, SMO, Naives Bayes and IBk that are implemented in the WEKA environment. The results were analyzed using ANOVA, which presented a p-value < 2e-16, indicating a statistical difference between the techniques analyzed. The Opponent Color technique showed better results, representing a potential for application in real plant conditions.;;Universidade Católica de Dom Bosco - UCDB;pt_BR;Published;21;2014;2014-12-29 14:56:28
52609;Wesley Nunes Gonçalves;Identificação de Viabilidade de Leveduras Com Corante Vital Utilizando Histogramas de Palavras Visuais em Imagens Coloridas;This article presents a proposal to automate the viability classification of yeasts of the species Saccharomyces cerevisae, responsible for the commercial production of ethanol, using as an attribute the color absorbed by the vital dye methylene blue. The methodology is widely used in plants in Brazil and consists of counting the colorless cells that are considered viable, separating them from those colored blue, considered non-viable. The number of viable cells per liter affects industrial yield. As this counting is tiring and results in errors, we present as an alternative the computer vision technique defined as the Bag-of-Word algorithm (histogram of visual words), as well as some extensions that add color information and that can be added to the algorithm. , this is because Bag-of-Word is used for grayscale images. The attributes extracted from this algorithm with its extensions were used for testing and training classifiers extracted from supervised learning techniques. Among the techniques we use we can highlight J48, SMO, Naives Bayes and IBk that are implemented in the WEKA environment. The results were analyzed using ANOVA, which presented a p-value < 2e-16, indicating a statistical difference between the techniques analyzed. The Opponent Color technique showed better results, representing a potential for application in real plant conditions.;;Universidade Federal de Mato Grosso do Sul;pt_BR;Published;21;2014;2014-12-29 14:56:28
52609;Valguima Victoria Viana Aguiar Odakura;Identificação de Viabilidade de Leveduras Com Corante Vital Utilizando Histogramas de Palavras Visuais em Imagens Coloridas;This article presents a proposal to automate the viability classification of yeasts of the species Saccharomyces cerevisae, responsible for the commercial production of ethanol, using as an attribute the color absorbed by the vital dye methylene blue. The methodology is widely used in plants in Brazil and consists of counting the colorless cells that are considered viable, separating them from those colored blue, considered non-viable. The number of viable cells per liter affects industrial yield. As this counting is tiring and results in errors, we present as an alternative the computer vision technique defined as the Bag-of-Word algorithm (histogram of visual words), as well as some extensions that add color information and that can be added to the algorithm. , this is because Bag-of-Word is used for grayscale images. The attributes extracted from this algorithm with its extensions were used for testing and training classifiers extracted from supervised learning techniques. Among the techniques we use we can highlight J48, SMO, Naives Bayes and IBk that are implemented in the WEKA environment. The results were analyzed using ANOVA, which presented a p-value < 2e-16, indicating a statistical difference between the techniques analyzed. The Opponent Color technique showed better results, representing a potential for application in real plant conditions.;;Universidade Federal da Grande DouradosFaculdade de Ciências Exatas e TecnologiaCurso de Sistemas de Informação;pt_BR;Published;21;2014;2014-12-29 14:56:28
53120;Nécio Lima Veras;Abordagem Proativa para a Gestão Integrada dos Trabalhos de Projeto;Monitoring and control activities are crucial to regulate the progress of project work in order to meet the objectives defined in the management plan. However, changes are inevitable and can arise at any time during development, influencing the execution of the plan originally drawn up and putting the success of the project at risk. This article presents an approach based on intelligent agent technology for project monitoring and control. The approach includes support for the automated management of change requests in order to provide an integrated and consistent view of the project's progress and assist managers in decision-making during the execution of work.;;Instituto Federal de Educação, Ciência e Tecnologia do Ceará;pt_BR;Published;14;2015;2015-01-28 12:58:11
53120;Mariela Inés Cortés;Abordagem Proativa para a Gestão Integrada dos Trabalhos de Projeto;Monitoring and control activities are crucial to regulate the progress of project work in order to meet the objectives defined in the management plan. However, changes are inevitable and can arise at any time during development, influencing the execution of the plan originally drawn up and putting the success of the project at risk. This article presents an approach based on intelligent agent technology for project monitoring and control. The approach includes support for the automated management of change requests in order to provide an integrated and consistent view of the project's progress and assist managers in decision-making during the execution of work.;;Universidade Estadual do Ceará;pt_BR;Published;14;2015;2015-01-28 12:58:11
53120;Anderson Couto Queiroz;Abordagem Proativa para a Gestão Integrada dos Trabalhos de Projeto;Monitoring and control activities are crucial to regulate the progress of project work in order to meet the objectives defined in the management plan. However, changes are inevitable and can arise at any time during development, influencing the execution of the plan originally drawn up and putting the success of the project at risk. This article presents an approach based on intelligent agent technology for project monitoring and control. The approach includes support for the automated management of change requests in order to provide an integrated and consistent view of the project's progress and assist managers in decision-making during the execution of work.;;Universidade Estadual do Ceará;pt_BR;Published;14;2015;2015-01-28 12:58:11
53120;Leandro L. C. de Souza;Abordagem Proativa para a Gestão Integrada dos Trabalhos de Projeto;Monitoring and control activities are crucial to regulate the progress of project work in order to meet the objectives defined in the management plan. However, changes are inevitable and can arise at any time during development, influencing the execution of the plan originally drawn up and putting the success of the project at risk. This article presents an approach based on intelligent agent technology for project monitoring and control. The approach includes support for the automated management of change requests in order to provide an integrated and consistent view of the project's progress and assist managers in decision-making during the execution of work.;;Instituto Federal de Educação do Maranhão;pt_BR;Published;14;2015;2015-01-28 12:58:11
54043;Simone de Oliveira Santos;Linguagens de consulta para bases de dados em grafos: um mapeamento sistemático;The popularization of social networks, associated with the need to analyze and summarize large volumes of data originating from them, has favored the use of graph databases. Query languages ​​for this type of database must, at the same time, have expressiveness sufficient to carry out complex queries and enable the efficient processing of large volumes of data. This article presents a systematic mapping of query languages ​​for graph databases, focusing on their main characteristics such as paradigm or data aggregation capacity . The focus of this mapping is to investigate and quantify publications relating to query languages, characterizing them, identifying possible areas of research, trends and challenges.;;Universidade Federal do Rio Grande do Norte, DIMAp;pt_BR;Published;58;2015;2015-03-09 11:07:26
54043;Martin Musicante;Linguagens de consulta para bases de dados em grafos: um mapeamento sistemático;The popularization of social networks, associated with the need to analyze and summarize large volumes of data originating from them, has favored the use of graph databases. Query languages ​​for this type of database must, at the same time, have expressiveness sufficient to carry out complex queries and enable the efficient processing of large volumes of data. This article presents a systematic mapping of query languages ​​for graph databases, focusing on their main characteristics such as paradigm or data aggregation capacity . The focus of this mapping is to investigate and quantify publications relating to query languages, characterizing them, identifying possible areas of research, trends and challenges.;;Universidade Federal do Rio Grande do Norte, DIMAp;pt_BR;Published;58;2015;2015-03-09 11:07:26
54043;Mirian Halfeld Ferrari Alves;Linguagens de consulta para bases de dados em grafos: um mapeamento sistemático;The popularization of social networks, associated with the need to analyze and summarize large volumes of data originating from them, has favored the use of graph databases. Query languages ​​for this type of database must, at the same time, have expressiveness sufficient to carry out complex queries and enable the efficient processing of large volumes of data. This article presents a systematic mapping of query languages ​​for graph databases, focusing on their main characteristics such as paradigm or data aggregation capacity . The focus of this mapping is to investigate and quantify publications relating to query languages, characterizing them, identifying possible areas of research, trends and challenges.;;Université d'Orléans, LIFO;pt_BR;Published;58;2015;2015-03-09 11:07:26
54772;Gustavo H. Czaikoski;Evolução Diferencial com ensemble de Operadores de Mutação em GPGPUs para o Despacho Econômico de Energia Elétrica;The Economic Electricity Dispatch Problem (PDEE) aims to minimize the cost of energy production from a thermoelectric plant. After analyzing the sequential algorithm, in this work, PDEE will be treated with a parallel algorithm for GPGPUs in CUDA. The proposed algorithm is a Differential Evolution (DE) using the mutation operator ensemble technique. DE is a stochastic population-based optimization technique, developed for the optimization of real values ​​while the ensemble of mutation operators allows various configurations of parameters and strategies to be used at each stage of the algorithm's evolution. Three test instances, considering valve point effects, are adopted to verify the efficiency of the proposed method. The results obtained compare favorably with those described in the literature in terms of the quality of the solutions obtained. The parallel version achieved significant speedups while maintaining the good quality of the solutions found.;;Universidade Estadual do Centro-Oeste - UNICENTRO;pt_BR;Published;22;2015;2015-04-15 16:28:05
54772;Paulo R. Urio;Evolução Diferencial com ensemble de Operadores de Mutação em GPGPUs para o Despacho Econômico de Energia Elétrica;The Economic Electricity Dispatch Problem (PDEE) aims to minimize the cost of energy production from a thermoelectric plant. After analyzing the sequential algorithm, in this work, PDEE will be treated with a parallel algorithm for GPGPUs in CUDA. The proposed algorithm is a Differential Evolution (DE) using the mutation operator ensemble technique. DE is a stochastic population-based optimization technique, developed for the optimization of real values ​​while the ensemble of mutation operators allows various configurations of parameters and strategies to be used at each stage of the algorithm's evolution. Three test instances, considering valve point effects, are adopted to verify the efficiency of the proposed method. The results obtained compare favorably with those described in the literature in terms of the quality of the solutions obtained. The parallel version achieved significant speedups while maintaining the good quality of the solutions found.;;Universidade Estadual do Centro-Oeste - UNICENTRO;pt_BR;Published;22;2015;2015-04-15 16:28:05
54772;Richard A. Gonçalves;Evolução Diferencial com ensemble de Operadores de Mutação em GPGPUs para o Despacho Econômico de Energia Elétrica;The Economic Electricity Dispatch Problem (PDEE) aims to minimize the cost of energy production from a thermoelectric plant. After analyzing the sequential algorithm, in this work, PDEE will be treated with a parallel algorithm for GPGPUs in CUDA. The proposed algorithm is a Differential Evolution (DE) using the mutation operator ensemble technique. DE is a stochastic population-based optimization technique, developed for the optimization of real values ​​while the ensemble of mutation operators allows various configurations of parameters and strategies to be used at each stage of the algorithm's evolution. Three test instances, considering valve point effects, are adopted to verify the efficiency of the proposed method. The results obtained compare favorably with those described in the literature in terms of the quality of the solutions obtained. The parallel version achieved significant speedups while maintaining the good quality of the solutions found.;;Universidade Estadual do Centro-Oeste - UNICENTRO;pt_BR;Published;22;2015;2015-04-15 16:28:05
54772;Carolina P. Almeida;Evolução Diferencial com ensemble de Operadores de Mutação em GPGPUs para o Despacho Econômico de Energia Elétrica;The Economic Electricity Dispatch Problem (PDEE) aims to minimize the cost of energy production from a thermoelectric plant. After analyzing the sequential algorithm, in this work, PDEE will be treated with a parallel algorithm for GPGPUs in CUDA. The proposed algorithm is a Differential Evolution (DE) using the mutation operator ensemble technique. DE is a stochastic population-based optimization technique, developed for the optimization of real values ​​while the ensemble of mutation operators allows various configurations of parameters and strategies to be used at each stage of the algorithm's evolution. Three test instances, considering valve point effects, are adopted to verify the efficiency of the proposed method. The results obtained compare favorably with those described in the literature in terms of the quality of the solutions obtained. The parallel version achieved significant speedups while maintaining the good quality of the solutions found.;;Universidade Estadual do Centro-Oeste - UNICENTRO;pt_BR;Published;22;2015;2015-04-15 16:28:05
54772;Josiel N. Kuk;Evolução Diferencial com ensemble de Operadores de Mutação em GPGPUs para o Despacho Econômico de Energia Elétrica;The Economic Electricity Dispatch Problem (PDEE) aims to minimize the cost of energy production from a thermoelectric plant. After analyzing the sequential algorithm, in this work, PDEE will be treated with a parallel algorithm for GPGPUs in CUDA. The proposed algorithm is a Differential Evolution (DE) using the mutation operator ensemble technique. DE is a stochastic population-based optimization technique, developed for the optimization of real values ​​while the ensemble of mutation operators allows various configurations of parameters and strategies to be used at each stage of the algorithm's evolution. Three test instances, considering valve point effects, are adopted to verify the efficiency of the proposed method. The results obtained compare favorably with those described in the literature in terms of the quality of the solutions obtained. The parallel version achieved significant speedups while maintaining the good quality of the solutions found.;;Universidade Estadual do Centro-Oeste - UNICENTRO;pt_BR;Published;22;2015;2015-04-15 16:28:05
54772;Sandra M. Venske;Evolução Diferencial com ensemble de Operadores de Mutação em GPGPUs para o Despacho Econômico de Energia Elétrica;The Economic Electricity Dispatch Problem (PDEE) aims to minimize the cost of energy production from a thermoelectric plant. After analyzing the sequential algorithm, in this work, PDEE will be treated with a parallel algorithm for GPGPUs in CUDA. The proposed algorithm is a Differential Evolution (DE) using the mutation operator ensemble technique. DE is a stochastic population-based optimization technique, developed for the optimization of real values ​​while the ensemble of mutation operators allows various configurations of parameters and strategies to be used at each stage of the algorithm's evolution. Three test instances, considering valve point effects, are adopted to verify the efficiency of the proposed method. The results obtained compare favorably with those described in the literature in terms of the quality of the solutions obtained. The parallel version achieved significant speedups while maintaining the good quality of the solutions found.;;Universidade Estadual do Centro-Oeste - UNICENTRO;pt_BR;Published;22;2015;2015-04-15 16:28:05
55723;Anderson Faustino da Silva;O Potencial do Uso de Estimativas de Desempenho na Exploração de Conjuntos de Otimizações;Modern compilers have traditionally adopted more generality strategies. On the other hand, to take advantage of the specificities of each program, iterative compilers emerge. These explore different sets of optimizations with the aim of finding the best one for each program, maximizing an objective function. When we're looking for performance improvements, that role is runtime. A practical way to obtain the execution time of a program is to execute it, but execution can be time-consuming, making exploration unfeasible. One solution to this is performance estimation. In this article we present a performance estimation tool to assist in the task of exploring the optimization space by iterative compilers. Different techniques were evaluated, where it was possible to show that even with approximate estimates, good results can be obtained. Furthermore, the estimates reduced exploration time by up to two orders of magnitude.;;Universidade Estadual de Maringá;pt_BR;Published;21;2015;2015-05-17 19:35:44
55723;Vanderson Martins Rosario;O Potencial do Uso de Estimativas de Desempenho na Exploração de Conjuntos de Otimizações;Modern compilers have traditionally adopted more generality strategies. On the other hand, to take advantage of the specificities of each program, iterative compilers emerge. These explore different sets of optimizations with the aim of finding the best one for each program, maximizing an objective function. When we're looking for performance improvements, that role is runtime. A practical way to obtain the execution time of a program is to execute it, but execution can be time-consuming, making exploration unfeasible. One solution to this is performance estimation. In this article we present a performance estimation tool to assist in the task of exploring the optimization space by iterative compilers. Different techniques were evaluated, where it was possible to show that even with approximate estimates, good results can be obtained. Furthermore, the estimates reduced exploration time by up to two orders of magnitude.;;Universidade Estadual de Maringá;pt_BR;Published;21;2015;2015-05-17 19:35:44
56162;Marlon De Oliveira Vaz;Detecção de Tubos em Imagens Radiográficas Digitais;This article presents a methodology for tube detection in double wall double view (PDVD) radiographic images of oil pipes. The main objective of the proposal is to reduce the search region by delimiting the tube area for the automatic extraction of the weld bead, thus helping the subsequent detection of defects in welded joints. The tube detection process presented is fully automatic and based on image processing techniques such as brightness and contrast adjustments, thresholding and analysis of the regions identified for tube segmentation. The process was applied to 167 images from three different radiographic systems, obtaining a result of 90.4% accuracy in detecting the tube. A comparison was made with another approach for tube detection in PDVD type radiographic images and the proposed methodology showed an improvement in relation to previous work. It is concluded, therefore, that the proposed method can be used as a step that precedes the automatic detection of the weld bead.;;Instituto Federal do Paraná - IFPR / Universidade Tecnológica Federal do Paraná - UTFPR;pt_BR;Published;16;2015;2015-06-01 11:08:14
56162;Tania Mezzadri Centeno;Detecção de Tubos em Imagens Radiográficas Digitais;This article presents a methodology for tube detection in double wall double view (PDVD) radiographic images of oil pipes. The main objective of the proposal is to reduce the search region by delimiting the tube area for the automatic extraction of the weld bead, thus helping the subsequent detection of defects in welded joints. The tube detection process presented is fully automatic and based on image processing techniques such as brightness and contrast adjustments, thresholding and analysis of the regions identified for tube segmentation. The process was applied to 167 images from three different radiographic systems, obtaining a result of 90.4% accuracy in detecting the tube. A comparison was made with another approach for tube detection in PDVD type radiographic images and the proposed methodology showed an improvement in relation to previous work. It is concluded, therefore, that the proposed method can be used as a step that precedes the automatic detection of the weld bead.;;Universidade Tecnológica Federal do Paraná - UTFPR;pt_BR;Published;16;2015;2015-06-01 11:08:14
56162;Myrian Regattieri Delgado;Detecção de Tubos em Imagens Radiográficas Digitais;This article presents a methodology for tube detection in double wall double view (PDVD) radiographic images of oil pipes. The main objective of the proposal is to reduce the search region by delimiting the tube area for the automatic extraction of the weld bead, thus helping the subsequent detection of defects in welded joints. The tube detection process presented is fully automatic and based on image processing techniques such as brightness and contrast adjustments, thresholding and analysis of the regions identified for tube segmentation. The process was applied to 167 images from three different radiographic systems, obtaining a result of 90.4% accuracy in detecting the tube. A comparison was made with another approach for tube detection in PDVD type radiographic images and the proposed methodology showed an improvement in relation to previous work. It is concluded, therefore, that the proposed method can be used as a step that precedes the automatic detection of the weld bead.;;Universidade Tecnológica Federal do Paraná - UTFPR;pt_BR;Published;16;2015;2015-06-01 11:08:14
56177;Abilio Parada;Automating mobile application development: UML-based code generation for Android and Windows Phone;This paper proposes a MDD approach for mobile application development, which includes modeling and code generation strategies for An- droid and Windows Phone. UML class and sequence diagrams are employed for modeling mobile applications and code is generated from this model. To support the automatic code generation, GenCode was re-structured and ex- tended to meet the particularities of these two platforms. As result, GenCode’s current version is able to automatically generate Java-Android and C# codes, according to the specified application model and target platform. Finally, case studies are used to demonstrate the proposed approach, as well as to validate the code generation tool.;;UFRGS;en_US;Published;19;2015;2015-06-01 16:22:00
56177;Milena Marques;Automating mobile application development: UML-based code generation for Android and Windows Phone;This paper proposes a MDD approach for mobile application development, which includes modeling and code generation strategies for An- droid and Windows Phone. UML class and sequence diagrams are employed for modeling mobile applications and code is generated from this model. To support the automatic code generation, GenCode was re-structured and ex- tended to meet the particularities of these two platforms. As result, GenCode’s current version is able to automatically generate Java-Android and C# codes, according to the specified application model and target platform. Finally, case studies are used to demonstrate the proposed approach, as well as to validate the code generation tool.;;UFPel;en_US;Published;19;2015;2015-06-01 16:22:00
56177;Lisane B. de Brisolara;Automating mobile application development: UML-based code generation for Android and Windows Phone;This paper proposes a MDD approach for mobile application development, which includes modeling and code generation strategies for An- droid and Windows Phone. UML class and sequence diagrams are employed for modeling mobile applications and code is generated from this model. To support the automatic code generation, GenCode was re-structured and ex- tended to meet the particularities of these two platforms. As result, GenCode’s current version is able to automatically generate Java-Android and C# codes, according to the specified application model and target platform. Finally, case studies are used to demonstrate the proposed approach, as well as to validate the code generation tool.;;Universidade Federal de Pelotas;en_US;Published;19;2015;2015-06-01 16:22:00
56368;André Luiz de Castro Leal;Método Sistêmico com Suporte em GORE para Análise de Conformidade de Requisitos não Funcionais Implementados em Software;Non-functional requirements analysis (RNF) is a challenge and has been explored in scientific literature for a long time. This initiative is due to the existence of the problem of verifying the use of operationalizations of this type of requirement in the software built. In this work we present a method, with support techniques and tools, that check whether software complies with RNF standards established in the catalog as an alternative to the RNF verification problem. Verification is an analysis technique that does not require the execution of the software, that is, the demonstration of its behavior. The term verification is used in contrast to the term validation. The spectrum of verification ranges from testing programs against specifications in a more formal sense, to visual inspection of programs in a less formal sense. The adopted strategy uses autonomous agents to verify software conformity in relation to RNF operationalizations, for this purpose it uses a knowledge base of patterns persisted in a catalog. The partial results are indicative that the proposed solution is applicable. Validity assessment takes place by demonstrating that a partially automated method is effective in identifying conformities. A difference in the proposal is that, in general, the work focused on verification is strongly focused on the functional vision, while the solution presented here is innovative in linking the RNF to its effective implementation. As a proof of concept, an RNF pattern technique based on goal orientation was applied and customized in case studies of examples from everyday software practice. As well as the construction of a framework of agents, which operate under XML notations, to identify software conformities in relation to an RNF catalog.;;Universidade Federal Rural do Rio de Janeiro;pt_BR;Published;42;2015;2015-06-11 13:39:47
56368;Henrique Prado Sousa;Método Sistêmico com Suporte em GORE para Análise de Conformidade de Requisitos não Funcionais Implementados em Software;Non-functional requirements analysis (RNF) is a challenge and has been explored in scientific literature for a long time. This initiative is due to the existence of the problem of verifying the use of operationalizations of this type of requirement in the software built. In this work we present a method, with support techniques and tools, that check whether software complies with RNF standards established in the catalog as an alternative to the RNF verification problem. Verification is an analysis technique that does not require the execution of the software, that is, the demonstration of its behavior. The term verification is used in contrast to the term validation. The spectrum of verification ranges from testing programs against specifications in a more formal sense, to visual inspection of programs in a less formal sense. The adopted strategy uses autonomous agents to verify software conformity in relation to RNF operationalizations, for this purpose it uses a knowledge base of patterns persisted in a catalog. The partial results are indicative that the proposed solution is applicable. Validity assessment takes place by demonstrating that a partially automated method is effective in identifying conformities. A difference in the proposal is that, in general, the work focused on verification is strongly focused on the functional vision, while the solution presented here is innovative in linking the RNF to its effective implementation. As a proof of concept, an RNF pattern technique based on goal orientation was applied and customized in case studies of examples from everyday software practice. As well as the construction of a framework of agents, which operate under XML notations, to identify software conformities in relation to an RNF catalog.;;-;pt_BR;Published;42;2015;2015-06-11 13:39:47
56368;Julio César Sampaio do Prado Leite;Método Sistêmico com Suporte em GORE para Análise de Conformidade de Requisitos não Funcionais Implementados em Software;Non-functional requirements analysis (RNF) is a challenge and has been explored in scientific literature for a long time. This initiative is due to the existence of the problem of verifying the use of operationalizations of this type of requirement in the software built. In this work we present a method, with support techniques and tools, that check whether software complies with RNF standards established in the catalog as an alternative to the RNF verification problem. Verification is an analysis technique that does not require the execution of the software, that is, the demonstration of its behavior. The term verification is used in contrast to the term validation. The spectrum of verification ranges from testing programs against specifications in a more formal sense, to visual inspection of programs in a less formal sense. The adopted strategy uses autonomous agents to verify software conformity in relation to RNF operationalizations, for this purpose it uses a knowledge base of patterns persisted in a catalog. The partial results are indicative that the proposed solution is applicable. Validity assessment takes place by demonstrating that a partially automated method is effective in identifying conformities. A difference in the proposal is that, in general, the work focused on verification is strongly focused on the functional vision, while the solution presented here is innovative in linking the RNF to its effective implementation. As a proof of concept, an RNF pattern technique based on goal orientation was applied and customized in case studies of examples from everyday software practice. As well as the construction of a framework of agents, which operate under XML notations, to identify software conformities in relation to an RNF catalog.;;-;pt_BR;Published;42;2015;2015-06-11 13:39:47
56374;Felipe Cesar Costa Alves;Uma Revisão Sobre as Publicações de Sistemas de Detecção de Intrusão;The growing record of security incidents in computer networks has motivated the development of studies on intrusion detection, the main techniques for identifying an intrusion are based on anomalies and signatures. Currently, the academic community preferably explores research on networks based on anomalies, however, there is no common model for developing these proposals so that many authors describe, implement and validate their systems in a heterogeneous way. In this article, research was carried out that investigated the scientific production of 112 publications related to intrusion detection systems. Some of the criteria used to evaluate these articles were impact factor, detection characteristics used and the database implemented. The results obtained demonstrate that there has been an increase in understanding of this topic, however future studies will be necessary to explore the validity of new evaluation methods in intrusion detection.;;Instituto Federal de educação ciência e tecnologia do estado de Mato Grosso;pt_BR;Published;32;2015;2015-06-12 2:50:40
56374;Ed’ Wilson Tavares Ferreira;Uma Revisão Sobre as Publicações de Sistemas de Detecção de Intrusão;The growing record of security incidents in computer networks has motivated the development of studies on intrusion detection, the main techniques for identifying an intrusion are based on anomalies and signatures. Currently, the academic community preferably explores research on networks based on anomalies, however, there is no common model for developing these proposals so that many authors describe, implement and validate their systems in a heterogeneous way. In this article, research was carried out that investigated the scientific production of 112 publications related to intrusion detection systems. Some of the criteria used to evaluate these articles were impact factor, detection characteristics used and the database implemented. The results obtained demonstrate that there has been an increase in understanding of this topic, however future studies will be necessary to explore the validity of new evaluation methods in intrusion detection.;;Instituto Federal de educação ciência e tecnologia do estado de Mato Grosso;pt_BR;Published;32;2015;2015-06-12 2:50:40
56374;Valtemir Emerencio Nascimento;Uma Revisão Sobre as Publicações de Sistemas de Detecção de Intrusão;The growing record of security incidents in computer networks has motivated the development of studies on intrusion detection, the main techniques for identifying an intrusion are based on anomalies and signatures. Currently, the academic community preferably explores research on networks based on anomalies, however, there is no common model for developing these proposals so that many authors describe, implement and validate their systems in a heterogeneous way. In this article, research was carried out that investigated the scientific production of 112 publications related to intrusion detection systems. Some of the criteria used to evaluate these articles were impact factor, detection characteristics used and the database implemented. The results obtained demonstrate that there has been an increase in understanding of this topic, however future studies will be necessary to explore the validity of new evaluation methods in intrusion detection.;;Instituto Federal de educação ciência e tecnologia do estado de Mato Grosso;pt_BR;Published;32;2015;2015-06-12 2:50:40
56374;Ruy de Oliveira;Uma Revisão Sobre as Publicações de Sistemas de Detecção de Intrusão;The growing record of security incidents in computer networks has motivated the development of studies on intrusion detection, the main techniques for identifying an intrusion are based on anomalies and signatures. Currently, the academic community preferably explores research on networks based on anomalies, however, there is no common model for developing these proposals so that many authors describe, implement and validate their systems in a heterogeneous way. In this article, research was carried out that investigated the scientific production of 112 publications related to intrusion detection systems. Some of the criteria used to evaluate these articles were impact factor, detection characteristics used and the database implemented. The results obtained demonstrate that there has been an increase in understanding of this topic, however future studies will be necessary to explore the validity of new evaluation methods in intrusion detection.;;Instituto Federal de educação ciência e tecnologia do estado de Mato Grosso;pt_BR;Published;32;2015;2015-06-12 2:50:40
56384;Esteban Walter Gonzalez Clua;Programming in CUDA for Kepler and Maxwell Architecture;Since the first version of CUDA was launch, many improvements were made in GPU computing. Every new CUDA version included important novel features, turning this architecture more and more closely related to a typical parallel High Performance Language. This tutorial will present the GPU architecture and CUDA principles, trying to conceptualize novel features included by NVIDIA, such as dynamics parallelism, unified memory and concurrent kernels. This text also includes some optimization remarks for CUDA programs.;;Universidade Federal Fluminense;en_US;Published;24;2015;2015-08-18 7:08:35
56384;Marcelo Panaro Zamith;Programming in CUDA for Kepler and Maxwell Architecture;Since the first version of CUDA was launch, many improvements were made in GPU computing. Every new CUDA version included important novel features, turning this architecture more and more closely related to a typical parallel High Performance Language. This tutorial will present the GPU architecture and CUDA principles, trying to conceptualize novel features included by NVIDIA, such as dynamics parallelism, unified memory and concurrent kernels. This text also includes some optimization remarks for CUDA programs.;;Universidade Federal Rural do Rio de Janeiro;en_US;Published;24;2015;2015-08-18 7:08:35
56437;Gladimir Baranoski;Hyperspectral Modeling of Material Appearance: General Framework, Challenges and Prospects;The main purpose of this tutorial is to address theoretical and practical issues involved in the development of predictive material appearancemodels for interdisciplinary applications within and outside the visible spectral domain. We examine the specific constraints and pitfalls found in each of the key stages of the model development framework, namely data collection, design and evaluation, and discuss alternatives to enhance the effectiveness of the entire process. Although predictive material appearance models developed by computer graphics  researchers are usually aimed at realistic image synthesis applications, they  also provide valuable support for a myriad of advanced investigations in related areas,  such as computer vision, image processing and pattern recognition,  which rely on the accurate analysis and interpretation of material appearance attributes in the hyperspectral domain. In fact, their scope of contributions goes beyond the realm of traditional computer science applications. For example, predictive light transport simulations, which are essential for the development of these models, are also regularly beingused by physical and life science researchers to understand andpredict material appearance changes prompted by mechanisms which cannot be fully studied using standard ``wet'' experimental procedures.For completeness, this tutorial also provides an overview of such synergistic research efforts and in silico investigations, which are illustrated by case studies involving the use of hyperspectral material appearance  models.;;University of Waterloo;en_US;Published;29;2015;2015-06-16 15:48:00
56498;Tácito Trindade de Araújo Tiburtino Neves;Análise Visual utilizando Projeções Multidimensionais;The techniques of projection or positioning of Flat Ano points have aroused great interest in the visualization and data analysis community because they enable human visual capacity to be used in the exploration and interpretation of similarity and neighborhood relationships. Currently, this is one of the most used visualization tools for exploring multidimensional data and its application extends from the analysis of data from sensors and biosensors to the exploration of image and music collections. This article presents examples of classic and state-of-the-art techniques for multidimensional projection, some available tools, as well as examples of applications that use these techniques in different domains, seeking to highlight how projections can help in understanding different sets of data.;;Universidade Federal de Viçosa;pt_BR;Published;30;2015;2015-06-16 23:31:35
56498;Samuel G. Fadel;Análise Visual utilizando Projeções Multidimensionais;The techniques of projection or positioning of Flat Ano points have aroused great interest in the visualization and data analysis community because they enable human visual capacity to be used in the exploration and interpretation of similarity and neighborhood relationships. Currently, this is one of the most used visualization tools for exploring multidimensional data and its application extends from the analysis of data from sensors and biosensors to the exploration of image and music collections. This article presents examples of classic and state-of-the-art techniques for multidimensional projection, some available tools, as well as examples of applications that use these techniques in different domains, seeking to highlight how projections can help in understanding different sets of data.;;Universidade de São Paulo;pt_BR;Published;30;2015;2015-06-16 23:31:35
56498;Danilo Barbosa Coimbra;Análise Visual utilizando Projeções Multidimensionais;The techniques of projection or positioning of Flat Ano points have aroused great interest in the visualization and data analysis community because they enable human visual capacity to be used in the exploration and interpretation of similarity and neighborhood relationships. Currently, this is one of the most used visualization tools for exploring multidimensional data and its application extends from the analysis of data from sensors and biosensors to the exploration of image and music collections. This article presents examples of classic and state-of-the-art techniques for multidimensional projection, some available tools, as well as examples of applications that use these techniques in different domains, seeking to highlight how projections can help in understanding different sets of data.;;Universidade de São Paulo;pt_BR;Published;30;2015;2015-06-16 23:31:35
56498;Fernando Vieira Paulovich;Análise Visual utilizando Projeções Multidimensionais;The techniques of projection or positioning of Flat Ano points have aroused great interest in the visualization and data analysis community because they enable human visual capacity to be used in the exploration and interpretation of similarity and neighborhood relationships. Currently, this is one of the most used visualization tools for exploring multidimensional data and its application extends from the analysis of data from sensors and biosensors to the exploration of image and music collections. This article presents examples of classic and state-of-the-art techniques for multidimensional projection, some available tools, as well as examples of applications that use these techniques in different domains, seeking to highlight how projections can help in understanding different sets of data.;;Universidade de São Paulo;pt_BR;Published;30;2015;2015-06-16 23:31:35
56977;Avelino Ferreira Gomes Filho;O uso de Métodos Ágeis no ensino de Métodos Ágeis;In the Agile Methods Course offered to undergraduate students at the Federal University of Rio de Janeiro (UFRJ), teachers decided that the traditional education model would not be the most suitable for teaching a subject that breaks so many paradigms. This article presents the teachers' four years of experience applying agile concepts in the Agile Methods discipline to undergraduate IT students. Our goal is to demonstrate how using these concepts helped create a stimulating and effective learning environment.;;Universidade Federal do Rio de Janeiro;pt_BR;Published;26;2015;2015-07-08 20:45:53
56977;Carlos Felippe Cardoso Resende;O uso de Métodos Ágeis no ensino de Métodos Ágeis;In the Agile Methods Course offered to undergraduate students at the Federal University of Rio de Janeiro (UFRJ), teachers decided that the traditional education model would not be the most suitable for teaching a subject that breaks so many paradigms. This article presents the teachers' four years of experience applying agile concepts in the Agile Methods discipline to undergraduate IT students. Our goal is to demonstrate how using these concepts helped create a stimulating and effective learning environment.;;Universidade Federal do Rio de Janeiro;pt_BR;Published;26;2015;2015-07-08 20:45:53
56977;Rodrigo Toledo;O uso de Métodos Ágeis no ensino de Métodos Ágeis;In the Agile Methods Course offered to undergraduate students at the Federal University of Rio de Janeiro (UFRJ), teachers decided that the traditional education model would not be the most suitable for teaching a subject that breaks so many paradigms. This article presents the teachers' four years of experience applying agile concepts in the Agile Methods discipline to undergraduate IT students. Our goal is to demonstrate how using these concepts helped create a stimulating and effective learning environment.;;Departamento de Ciência da Computação, Universidade Federal do Rio de Janeiro;pt_BR;Published;26;2015;2015-07-08 20:45:53
58022;Sandro Loiola Menezes;Mineração em Grandes Massas de Dados Utilizando Hadoop MapReduce e Algoritmos Bio-inspirados: Uma Revisão Sistemática;The Data Mining Area has been used in several application areas and aims to extract knowledge through data analysis. In recent decades, numerous databases are tending to have large volume, high growth speed and great variety. This phenomenon is known as BigData and represents new challenges for classic technologies such as Relational Database Management System as it has not offered satisfactory performance and scalability for Big Data type applications. Unlike these technologies, Hadoop MapReduce is a framework that, in addition to providing parallel processing, also provides fault tolerance and easy scalability on a distributed storage system compatible with Big Data scenarios. One of the techniques that has been used in the Big Data context are bio-inspired algorithms. These algorithms are good solution options for complex multidimensional, multiobjective and large-scale problems. The combination of Hadoop MapReduce-based systems and bio-inspired algorithms has proven advantageous in Big Data applications. This article presents a systematic review of works in this context, aiming to analyze criteria such as: data mining tasks addressed, bio-inspired algorithms used, availability of the databases used and which Big Data characteristics are addressed in the works. As a result, this article discusses the analyzed criteria and identifies some parallelization models, in addition to suggesting a direction for future work.;;UNIVERSIDADE DO ESTADO DE SANTA CATARINA;pt_BR;Published;32;2015;2015-08-24 11:10:32
58022;Rebeca Schroeder Freitas;Mineração em Grandes Massas de Dados Utilizando Hadoop MapReduce e Algoritmos Bio-inspirados: Uma Revisão Sistemática;The Data Mining Area has been used in several application areas and aims to extract knowledge through data analysis. In recent decades, numerous databases are tending to have large volume, high growth speed and great variety. This phenomenon is known as BigData and represents new challenges for classic technologies such as Relational Database Management System as it has not offered satisfactory performance and scalability for Big Data type applications. Unlike these technologies, Hadoop MapReduce is a framework that, in addition to providing parallel processing, also provides fault tolerance and easy scalability on a distributed storage system compatible with Big Data scenarios. One of the techniques that has been used in the Big Data context are bio-inspired algorithms. These algorithms are good solution options for complex multidimensional, multiobjective and large-scale problems. The combination of Hadoop MapReduce-based systems and bio-inspired algorithms has proven advantageous in Big Data applications. This article presents a systematic review of works in this context, aiming to analyze criteria such as: data mining tasks addressed, bio-inspired algorithms used, availability of the databases used and which Big Data characteristics are addressed in the works. As a result, this article discusses the analyzed criteria and identifies some parallelization models, in addition to suggesting a direction for future work.;;UNIVERSIDADE DO ESTADO DE SANTA CATARINA;pt_BR;Published;32;2015;2015-08-24 11:10:32
58022;Rafael Stubs Parpinelli;Mineração em Grandes Massas de Dados Utilizando Hadoop MapReduce e Algoritmos Bio-inspirados: Uma Revisão Sistemática;The Data Mining Area has been used in several application areas and aims to extract knowledge through data analysis. In recent decades, numerous databases are tending to have large volume, high growth speed and great variety. This phenomenon is known as BigData and represents new challenges for classic technologies such as Relational Database Management System as it has not offered satisfactory performance and scalability for Big Data type applications. Unlike these technologies, Hadoop MapReduce is a framework that, in addition to providing parallel processing, also provides fault tolerance and easy scalability on a distributed storage system compatible with Big Data scenarios. One of the techniques that has been used in the Big Data context are bio-inspired algorithms. These algorithms are good solution options for complex multidimensional, multiobjective and large-scale problems. The combination of Hadoop MapReduce-based systems and bio-inspired algorithms has proven advantageous in Big Data applications. This article presents a systematic review of works in this context, aiming to analyze criteria such as: data mining tasks addressed, bio-inspired algorithms used, availability of the databases used and which Big Data characteristics are addressed in the works. As a result, this article discusses the analyzed criteria and identifies some parallelization models, in addition to suggesting a direction for future work.;;UNIVERSIDADE DO ESTADO DE SANTA CATARINA;pt_BR;Published;32;2015;2015-08-24 11:10:32
58551;Ítalo de Pontes Oliveira;Relação Entre o Gênero de Espectadores e o Conteúdo de Vídeos;Digital signage is a type of advertising transmitted through panels placed in public places, in which the business problem is identified as the advertiser's need to know the profile of their viewers. In this context, this work aims to conduct an experiment that evaluates the attention paid by men and women to sports and news programs, using a monocular camera to automatically detect the faces in front of the monitor and correlate it with the content being transmitted. Furthermore, the impact of the size of the training set on the processing time and accuracy of the gender classifier was investigated. The conclusions of this work indicate that men and women vary their attention depending on the type of content. Additionally, the increase in the size of the training set (from a certain value) has no influence on the accuracy of the gender classifier, but it does have an influence on the increase in processing time.;;Universidade Federal de Campina Grande (UFCG)/Laboratório de Visão Computacional (LVC);pt_BR;Published;18;2015;2015-09-15 17:46:26
58551;Eanes Torres Pereira;Relação Entre o Gênero de Espectadores e o Conteúdo de Vídeos;Digital signage is a type of advertising transmitted through panels placed in public places, in which the business problem is identified as the advertiser's need to know the profile of their viewers. In this context, this work aims to conduct an experiment that evaluates the attention paid by men and women to sports and news programs, using a monocular camera to automatically detect the faces in front of the monitor and correlate it with the content being transmitted. Furthermore, the impact of the size of the training set on the processing time and accuracy of the gender classifier was investigated. The conclusions of this work indicate that men and women vary their attention depending on the type of content. Additionally, the increase in the size of the training set (from a certain value) has no influence on the accuracy of the gender classifier, but it does have an influence on the increase in processing time.;;Universidade Federal de Campina Grande (UFCG)/Laboratório de Visão Computacional (LVC);pt_BR;Published;18;2015;2015-09-15 17:46:26
58551;Herman Martins Gomes;Relação Entre o Gênero de Espectadores e o Conteúdo de Vídeos;Digital signage is a type of advertising transmitted through panels placed in public places, in which the business problem is identified as the advertiser's need to know the profile of their viewers. In this context, this work aims to conduct an experiment that evaluates the attention paid by men and women to sports and news programs, using a monocular camera to automatically detect the faces in front of the monitor and correlate it with the content being transmitted. Furthermore, the impact of the size of the training set on the processing time and accuracy of the gender classifier was investigated. The conclusions of this work indicate that men and women vary their attention depending on the type of content. Additionally, the increase in the size of the training set (from a certain value) has no influence on the accuracy of the gender classifier, but it does have an influence on the increase in processing time.;;Universidade Federal de Campina Grande (UFCG)/Laboratório de Visão Computacional (LVC);pt_BR;Published;18;2015;2015-09-15 17:46:26
59104;Rafael Ribaldo;Exploring the subtopic-based relationship map strategy for multi-document summarization;In this paper we adapt and explore strategies for generating multi-document summaries based on relationship maps, which represent texts as graphs (maps) of interrelated segments and apply different traversing techniques for producing the summaries. In particular, we focus on the Segmented Bushy Path, a sophisticated method which tries to represent in a summary the main subtopics from source texts while keeping its informativeness. In addition, we also investigate some well-known subtopic segmentation and clustering techniques in order to correctly select the most relevant information to compose the final summary. We show that this subtopic-based method outperforms other methods for multi-document summarization and that achieves state of the art results, competing with the most sophisticated deep summarization methods in the area.;;Universidade de São Paulo;en_US;Published;28;2015;2015-10-04 22:29:39
59104;Paula Christina Figueira Cardoso;Exploring the subtopic-based relationship map strategy for multi-document summarization;In this paper we adapt and explore strategies for generating multi-document summaries based on relationship maps, which represent texts as graphs (maps) of interrelated segments and apply different traversing techniques for producing the summaries. In particular, we focus on the Segmented Bushy Path, a sophisticated method which tries to represent in a summary the main subtopics from source texts while keeping its informativeness. In addition, we also investigate some well-known subtopic segmentation and clustering techniques in order to correctly select the most relevant information to compose the final summary. We show that this subtopic-based method outperforms other methods for multi-document summarization and that achieves state of the art results, competing with the most sophisticated deep summarization methods in the area.;;-;en_US;Published;28;2015;2015-10-04 22:29:39
59104;Thiago Alexandre Salgueiro Pardo;Exploring the subtopic-based relationship map strategy for multi-document summarization;In this paper we adapt and explore strategies for generating multi-document summaries based on relationship maps, which represent texts as graphs (maps) of interrelated segments and apply different traversing techniques for producing the summaries. In particular, we focus on the Segmented Bushy Path, a sophisticated method which tries to represent in a summary the main subtopics from source texts while keeping its informativeness. In addition, we also investigate some well-known subtopic segmentation and clustering techniques in order to correctly select the most relevant information to compose the final summary. We show that this subtopic-based method outperforms other methods for multi-document summarization and that achieves state of the art results, competing with the most sophisticated deep summarization methods in the area.;;-;en_US;Published;28;2015;2015-10-04 22:29:39
59336;Diego Nunes Molinos;ZipfTool: Uma ferramenta bibliométrica para auxílio na pesquisa teórica;Due to the volume of works published in scientific dissemination vehicles, textual data analysis tools become important for several areas of knowledge. Utilities of this nature offer the user functionality on both a quantitative and qualitative level. From a quantitative point of view, they make it possible to identify the frequency of occurrence of words in the text and differentiate verbs, nouns and definite articles. Qualitative analyzes deal with the survey of words with greater semantic content, the identification of descriptors and key words in the text and the correlation between terms with greater semantic content. This work aims to present the development of a data analysis tool that does not it only has quantitative but also qualitative analysis primitives. Quantitatively, the tool provides the frequency of the main terms in the text, while qualitatively it identifies the words with the highest semantic content. Using techniques derived from bibliometrics, the tool presented, called ZipfTool, implements both Zipf's 1st and 2nd Laws. The case study in the area of ​​computer architecture shows a reduction in the universe of articles to be analyzed from 46785 to 1508 together with bibliometric techniques. It also allows a quick and consistent form of the main descriptors of a field of knowledge under analysis, contributing to qualifying its understanding.;;Universidade Federal de Uberlândia - UFU;pt_BR;Published;24;2015;2015-10-14 14:17:19
59336;Daniel Gomes Mesquita;ZipfTool: Uma ferramenta bibliométrica para auxílio na pesquisa teórica;Due to the volume of works published in scientific dissemination vehicles, textual data analysis tools become important for several areas of knowledge. Utilities of this nature offer the user functionality on both a quantitative and qualitative level. From a quantitative point of view, they make it possible to identify the frequency of occurrence of words in the text and differentiate verbs, nouns and definite articles. Qualitative analyzes deal with the survey of words with greater semantic content, the identification of descriptors and key words in the text and the correlation between terms with greater semantic content. This work aims to present the development of a data analysis tool that does not it only has quantitative but also qualitative analysis primitives. Quantitatively, the tool provides the frequency of the main terms in the text, while qualitatively it identifies the words with the highest semantic content. Using techniques derived from bibliometrics, the tool presented, called ZipfTool, implements both Zipf's 1st and 2nd Laws. The case study in the area of ​​computer architecture shows a reduction in the universe of articles to be analyzed from 46785 to 1508 together with bibliometric techniques. It also allows a quick and consistent form of the main descriptors of a field of knowledge under analysis, contributing to qualifying its understanding.;;Universidade Federal do Pampa - UNIPAMPA;pt_BR;Published;24;2015;2015-10-14 14:17:19
59336;Debora Nayar Hoff;ZipfTool: Uma ferramenta bibliométrica para auxílio na pesquisa teórica;Due to the volume of works published in scientific dissemination vehicles, textual data analysis tools become important for several areas of knowledge. Utilities of this nature offer the user functionality on both a quantitative and qualitative level. From a quantitative point of view, they make it possible to identify the frequency of occurrence of words in the text and differentiate verbs, nouns and definite articles. Qualitative analyzes deal with the survey of words with greater semantic content, the identification of descriptors and key words in the text and the correlation between terms with greater semantic content. This work aims to present the development of a data analysis tool that does not it only has quantitative but also qualitative analysis primitives. Quantitatively, the tool provides the frequency of the main terms in the text, while qualitatively it identifies the words with the highest semantic content. Using techniques derived from bibliometrics, the tool presented, called ZipfTool, implements both Zipf's 1st and 2nd Laws. The case study in the area of ​​computer architecture shows a reduction in the universe of articles to be analyzed from 46785 to 1508 together with bibliometric techniques. It also allows a quick and consistent form of the main descriptors of a field of knowledge under analysis, contributing to qualifying its understanding.;;Universidade Federal do Pampa - UNIPAMPA;pt_BR;Published;24;2015;2015-10-14 14:17:19
59614;João Fabrício Filho;Paralelismo em Prolog: Conceitos e Sistemas;Parallelism is an area of ​​study that grows every day, due to the reduction in cost and popularization of machines with parallel architectures. In this context, logical languages, especially PROLOG, present a viable and practical alternative for parallelism. Exploring this parallelism can be carried out in different ways, and there are numerous challenges in this task. This tutorial aims to present the main concepts of parallelism in PROLOG, the challenges faced when seeking parallelization in this language and the state-of-the-art development of systems that support parallelization in logical languages. Systems based on implicit parallelism implemented on different platforms are presented. At the end, a comparison is made between the systems presented and the models implemented in them.;;Universidade Tecnológica Federal do ParanáUniversidade Estadual de Maringá;pt_BR;Published;20;2015;2015-10-27 23:58:53
59614;Anderson Faustino da Silva;Paralelismo em Prolog: Conceitos e Sistemas;Parallelism is an area of ​​study that grows every day, due to the reduction in cost and popularization of machines with parallel architectures. In this context, logical languages, especially PROLOG, present a viable and practical alternative for parallelism. Exploring this parallelism can be carried out in different ways, and there are numerous challenges in this task. This tutorial aims to present the main concepts of parallelism in PROLOG, the challenges faced when seeking parallelization in this language and the state-of-the-art development of systems that support parallelization in logical languages. Systems based on implicit parallelism implemented on different platforms are presented. At the end, a comparison is made between the systems presented and the models implemented in them.;;-;pt_BR;Published;20;2015;2015-10-27 23:58:53
60234;Samuel Portela Carvalho;Uma contribuição ao auxíılio do diagnóstico do autismo a partir do processamento de imagens para extração de medidas antropométricas;Autism Spectrum Disorder (ASD) is a syndrome characterized by difficulty in social interaction and qualitative deviations in communication and the use of imagination. Although the diagnosis of this syndrome basically consists of clinical assessment, computerized tools can be used as an adjunct in this task. From clinical assessments, there is evidence that children with ASD have different anthropometric measurements than children without the syndrome. The present work aims to define and validate techniques for processing facial images and measuring anthropometric distances in order to assist in the diagnosis of ASD. The defined techniques culminated in the construction of a computational tool capable of analyzing patient photographs and calculating facial anthropometric measurements. The tool was validated with an image bank of individuals with and without the syndrome, and similarities and differences were found between the anthropometric measurements extracted by the tool and measurements cited in previous studies. The tool proved capable of processing frontal images of patients and extracting their components and anthropometric measurements accurately, using image processing techniques adapted for these purposes, and can effectively contribute to aiding the diagnosis of ASD.;;Escola de Artes, Ciências e Humanidades – Universidade de São Paulo (EACH-USP);pt_BR;Published;23;2015;2015-11-23 0:45:26
60234;Ariane Machado Lima;Uma contribuição ao auxíılio do diagnóstico do autismo a partir do processamento de imagens para extração de medidas antropométricas;Autism Spectrum Disorder (ASD) is a syndrome characterized by difficulty in social interaction and qualitative deviations in communication and the use of imagination. Although the diagnosis of this syndrome basically consists of clinical assessment, computerized tools can be used as an adjunct in this task. From clinical assessments, there is evidence that children with ASD have different anthropometric measurements than children without the syndrome. The present work aims to define and validate techniques for processing facial images and measuring anthropometric distances in order to assist in the diagnosis of ASD. The defined techniques culminated in the construction of a computational tool capable of analyzing patient photographs and calculating facial anthropometric measurements. The tool was validated with an image bank of individuals with and without the syndrome, and similarities and differences were found between the anthropometric measurements extracted by the tool and measurements cited in previous studies. The tool proved capable of processing frontal images of patients and extracting their components and anthropometric measurements accurately, using image processing techniques adapted for these purposes, and can effectively contribute to aiding the diagnosis of ASD.;;USP;pt_BR;Published;23;2015;2015-11-23 0:45:26
60234;Helena Paula Brentani;Uma contribuição ao auxíılio do diagnóstico do autismo a partir do processamento de imagens para extração de medidas antropométricas;Autism Spectrum Disorder (ASD) is a syndrome characterized by difficulty in social interaction and qualitative deviations in communication and the use of imagination. Although the diagnosis of this syndrome basically consists of clinical assessment, computerized tools can be used as an adjunct in this task. From clinical assessments, there is evidence that children with ASD have different anthropometric measurements than children without the syndrome. The present work aims to define and validate techniques for processing facial images and measuring anthropometric distances in order to assist in the diagnosis of ASD. The defined techniques culminated in the construction of a computational tool capable of analyzing patient photographs and calculating facial anthropometric measurements. The tool was validated with an image bank of individuals with and without the syndrome, and similarities and differences were found between the anthropometric measurements extracted by the tool and measurements cited in previous studies. The tool proved capable of processing frontal images of patients and extracting their components and anthropometric measurements accurately, using image processing techniques adapted for these purposes, and can effectively contribute to aiding the diagnosis of ASD.;;Instituto de Psiquiatria da FMUSP;pt_BR;Published;23;2015;2015-11-23 0:45:26
60234;Décio Brunoni;Uma contribuição ao auxíılio do diagnóstico do autismo a partir do processamento de imagens para extração de medidas antropométricas;Autism Spectrum Disorder (ASD) is a syndrome characterized by difficulty in social interaction and qualitative deviations in communication and the use of imagination. Although the diagnosis of this syndrome basically consists of clinical assessment, computerized tools can be used as an adjunct in this task. From clinical assessments, there is evidence that children with ASD have different anthropometric measurements than children without the syndrome. The present work aims to define and validate techniques for processing facial images and measuring anthropometric distances in order to assist in the diagnosis of ASD. The defined techniques culminated in the construction of a computational tool capable of analyzing patient photographs and calculating facial anthropometric measurements. The tool was validated with an image bank of individuals with and without the syndrome, and similarities and differences were found between the anthropometric measurements extracted by the tool and measurements cited in previous studies. The tool proved capable of processing frontal images of patients and extracting their components and anthropometric measurements accurately, using image processing techniques adapted for these purposes, and can effectively contribute to aiding the diagnosis of ASD.;;Universidade Federal de São Paulo – Unifesp;pt_BR;Published;23;2015;2015-11-23 0:45:26
60234;Rodrigo Ambrósio Fock;Uma contribuição ao auxíılio do diagnóstico do autismo a partir do processamento de imagens para extração de medidas antropométricas;Autism Spectrum Disorder (ASD) is a syndrome characterized by difficulty in social interaction and qualitative deviations in communication and the use of imagination. Although the diagnosis of this syndrome basically consists of clinical assessment, computerized tools can be used as an adjunct in this task. From clinical assessments, there is evidence that children with ASD have different anthropometric measurements than children without the syndrome. The present work aims to define and validate techniques for processing facial images and measuring anthropometric distances in order to assist in the diagnosis of ASD. The defined techniques culminated in the construction of a computational tool capable of analyzing patient photographs and calculating facial anthropometric measurements. The tool was validated with an image bank of individuals with and without the syndrome, and similarities and differences were found between the anthropometric measurements extracted by the tool and measurements cited in previous studies. The tool proved capable of processing frontal images of patients and extracting their components and anthropometric measurements accurately, using image processing techniques adapted for these purposes, and can effectively contribute to aiding the diagnosis of ASD.;;Instituto de Psiquiatria da FMUSP;pt_BR;Published;23;2015;2015-11-23 0:45:26
60234;Fátima Lourdes Santos Nunes;Uma contribuição ao auxíılio do diagnóstico do autismo a partir do processamento de imagens para extração de medidas antropométricas;Autism Spectrum Disorder (ASD) is a syndrome characterized by difficulty in social interaction and qualitative deviations in communication and the use of imagination. Although the diagnosis of this syndrome basically consists of clinical assessment, computerized tools can be used as an adjunct in this task. From clinical assessments, there is evidence that children with ASD have different anthropometric measurements than children without the syndrome. The present work aims to define and validate techniques for processing facial images and measuring anthropometric distances in order to assist in the diagnosis of ASD. The defined techniques culminated in the construction of a computational tool capable of analyzing patient photographs and calculating facial anthropometric measurements. The tool was validated with an image bank of individuals with and without the syndrome, and similarities and differences were found between the anthropometric measurements extracted by the tool and measurements cited in previous studies. The tool proved capable of processing frontal images of patients and extracting their components and anthropometric measurements accurately, using image processing techniques adapted for these purposes, and can effectively contribute to aiding the diagnosis of ASD.;;Escola de Artes, Ciências e Humanidades – Universidade de São Paulo (EACH-USP);pt_BR;Published;23;2015;2015-11-23 0:45:26
60989;Ives Fernando Martins Santos de Moura;Avaliação de Incisão Cirúrgica em Simuladores com Realidade Virtual;Surgical procedure simulators with Virtual Reality have become increasingly popular. An important component of this type of software for the medical field is the ability to simulate incisions. In the computational context, incisions involve three basic steps: definition of geometry and topology, collision and deformation detection. Furthermore, in simulators aimed at training students, the ability to evaluate user performance is essential. In this work, two experiments are carried out, one with randomly generated data and the other with data generated from user interaction with an application via mouse, to evaluate the quality of the cut with regard to its trajectory. Both experiments were performed using the Support Vector Machine (SVM) method. Satisfactory results were obtained with SVM and five kernels, with accuracy rates above 70%. The main conclusion, then, is that SVM is a method capable of evaluating problems related to the incision trajectory. The contributions of the work include the analysis of the incision components, the survey of evaluation methods applicable to this problem and the use of SVM in the two experiments conducted, which showed the applicability of this method in this context.;;Departamento de EstatísticaUniversidade Federal da Paraíba;pt_BR;Published;18;2015;2015-12-17 18:46:46
60989;Ronei Marcos Moraes;Avaliação de Incisão Cirúrgica em Simuladores com Realidade Virtual;Surgical procedure simulators with Virtual Reality have become increasingly popular. An important component of this type of software for the medical field is the ability to simulate incisions. In the computational context, incisions involve three basic steps: definition of geometry and topology, collision and deformation detection. Furthermore, in simulators aimed at training students, the ability to evaluate user performance is essential. In this work, two experiments are carried out, one with randomly generated data and the other with data generated from user interaction with an application via mouse, to evaluate the quality of the cut with regard to its trajectory. Both experiments were performed using the Support Vector Machine (SVM) method. Satisfactory results were obtained with SVM and five kernels, with accuracy rates above 70%. The main conclusion, then, is that SVM is a method capable of evaluating problems related to the incision trajectory. The contributions of the work include the analysis of the incision components, the survey of evaluation methods applicable to this problem and the use of SVM in the two experiments conducted, which showed the applicability of this method in this context.;;Universidade Federal da Paraíba;pt_BR;Published;18;2015;2015-12-17 18:46:46
60989;Liliane Santos Machado;Avaliação de Incisão Cirúrgica em Simuladores com Realidade Virtual;Surgical procedure simulators with Virtual Reality have become increasingly popular. An important component of this type of software for the medical field is the ability to simulate incisions. In the computational context, incisions involve three basic steps: definition of geometry and topology, collision and deformation detection. Furthermore, in simulators aimed at training students, the ability to evaluate user performance is essential. In this work, two experiments are carried out, one with randomly generated data and the other with data generated from user interaction with an application via mouse, to evaluate the quality of the cut with regard to its trajectory. Both experiments were performed using the Support Vector Machine (SVM) method. Satisfactory results were obtained with SVM and five kernels, with accuracy rates above 70%. The main conclusion, then, is that SVM is a method capable of evaluating problems related to the incision trajectory. The contributions of the work include the analysis of the incision components, the survey of evaluation methods applicable to this problem and the use of SVM in the two experiments conducted, which showed the applicability of this method in this context.;;Universidade Federal da Paraíba;pt_BR;Published;18;2015;2015-12-17 18:46:46
61220;José do Nascimento Linhares;Método Computacional para o Diagnóstico Precoce da Granulomatose de Wegener;In this work, a proteomic pattern recognition system is presented with the aim of assisting in the early diagnosis of Wegener's Granulomatosis (WG), a rare idiopathic vasculitis that is difficult to detect and has a high mortality rate for untreated individuals. The method consists of extracting characteristics from proteomic signals and classifying them as being from individuals with or without GW. To this end, Independent Component Analysis is used to extract characteristics from the signals, a Maximum Relevance and Minimum Redundancy Algorithm to reduce the number of characteristics and computational costs and a Support Vector Machine to classify. The quality of the method was evaluated using a database with 335 proteomic signals, consisting of 75 active cases, 101 negative cases and 159 in remission. The best result obtained was for a vector of twenty features whose accuracy, specificity and sensitivity were, respectively, 98.24%, 99.73% and 99.50%.;;Universidade Estadual do Maranhão;pt_BR;Published;15;2015;2015-12-29 14:07:46
61220;Lúcio Flávio A. Campos;Método Computacional para o Diagnóstico Precoce da Granulomatose de Wegener;In this work, a proteomic pattern recognition system is presented with the aim of assisting in the early diagnosis of Wegener's Granulomatosis (WG), a rare idiopathic vasculitis that is difficult to detect and has a high mortality rate for untreated individuals. The method consists of extracting characteristics from proteomic signals and classifying them as being from individuals with or without GW. To this end, Independent Component Analysis is used to extract characteristics from the signals, a Maximum Relevance and Minimum Redundancy Algorithm to reduce the number of characteristics and computational costs and a Support Vector Machine to classify. The quality of the method was evaluated using a database with 335 proteomic signals, consisting of 75 active cases, 101 negative cases and 159 in remission. The best result obtained was for a vector of twenty features whose accuracy, specificity and sensitivity were, respectively, 98.24%, 99.73% and 99.50%.;;-;pt_BR;Published;15;2015;2015-12-29 14:07:46
61220;Ewaldo Eder Carvalho Santana;Método Computacional para o Diagnóstico Precoce da Granulomatose de Wegener;In this work, a proteomic pattern recognition system is presented with the aim of assisting in the early diagnosis of Wegener's Granulomatosis (WG), a rare idiopathic vasculitis that is difficult to detect and has a high mortality rate for untreated individuals. The method consists of extracting characteristics from proteomic signals and classifying them as being from individuals with or without GW. To this end, Independent Component Analysis is used to extract characteristics from the signals, a Maximum Relevance and Minimum Redundancy Algorithm to reduce the number of characteristics and computational costs and a Support Vector Machine to classify. The quality of the method was evaluated using a database with 335 proteomic signals, consisting of 75 active cases, 101 negative cases and 159 in remission. The best result obtained was for a vector of twenty features whose accuracy, specificity and sensitivity were, respectively, 98.24%, 99.73% and 99.50%.;;-;pt_BR;Published;15;2015;2015-12-29 14:07:46
61220;Jardiel Nunes Almeida;Método Computacional para o Diagnóstico Precoce da Granulomatose de Wegener;In this work, a proteomic pattern recognition system is presented with the aim of assisting in the early diagnosis of Wegener's Granulomatosis (WG), a rare idiopathic vasculitis that is difficult to detect and has a high mortality rate for untreated individuals. The method consists of extracting characteristics from proteomic signals and classifying them as being from individuals with or without GW. To this end, Independent Component Analysis is used to extract characteristics from the signals, a Maximum Relevance and Minimum Redundancy Algorithm to reduce the number of characteristics and computational costs and a Support Vector Machine to classify. The quality of the method was evaluated using a database with 335 proteomic signals, consisting of 75 active cases, 101 negative cases and 159 in remission. The best result obtained was for a vector of twenty features whose accuracy, specificity and sensitivity were, respectively, 98.24%, 99.73% and 99.50%.;;-;pt_BR;Published;15;2015;2015-12-29 14:07:46
61220;Flávia Larisse da Silva Fernandes;Método Computacional para o Diagnóstico Precoce da Granulomatose de Wegener;In this work, a proteomic pattern recognition system is presented with the aim of assisting in the early diagnosis of Wegener's Granulomatosis (WG), a rare idiopathic vasculitis that is difficult to detect and has a high mortality rate for untreated individuals. The method consists of extracting characteristics from proteomic signals and classifying them as being from individuals with or without GW. To this end, Independent Component Analysis is used to extract characteristics from the signals, a Maximum Relevance and Minimum Redundancy Algorithm to reduce the number of characteristics and computational costs and a Support Vector Machine to classify. The quality of the method was evaluated using a database with 335 proteomic signals, consisting of 75 active cases, 101 negative cases and 159 in remission. The best result obtained was for a vector of twenty features whose accuracy, specificity and sensitivity were, respectively, 98.24%, 99.73% and 99.50%.;;-;pt_BR;Published;15;2015;2015-12-29 14:07:46
62211;Douglas Detoni;Learning to Identify At-Risk Students in Distance Education Using Interaction Counts;Student dropout is one of the main problems faced by distance learning courses. One of the major challenges for researchers is to develop methods to predict the behavior of students so that teachers and tutors are able to identify at-risk students as early as possible and provide assistance before they drop out or fail in their courses. Machine Learning models have been used to predict or classify students in these settings. However, while these models have shown promising results in several settings, they usually attain these results using attributes that are not immediately transferable to other courses or platforms. In this paper, we provide a methodology to classify students using only interaction counts from each student. We evaluate this methodology on a data set from two majors based on the Moodle platform. We run experiments consisting of training and evaluating three machine learning models (Support Vector Machines, Naive Bayes and Adaboost decision trees) under different scenarios. We provide evidences that patterns from interaction counts can provide useful information for classifying at-risk students. This classification allows the customization of the activities presented to at-risk students (automatically or through tutors) as an attempt to avoid students drop out.;;UFPel;pt_BR;Published;16;2016;2016-02-15 15:07:27
62211;Cristian Cechinel;Learning to Identify At-Risk Students in Distance Education Using Interaction Counts;Student dropout is one of the main problems faced by distance learning courses. One of the major challenges for researchers is to develop methods to predict the behavior of students so that teachers and tutors are able to identify at-risk students as early as possible and provide assistance before they drop out or fail in their courses. Machine Learning models have been used to predict or classify students in these settings. However, while these models have shown promising results in several settings, they usually attain these results using attributes that are not immediately transferable to other courses or platforms. In this paper, we provide a methodology to classify students using only interaction counts from each student. We evaluate this methodology on a data set from two majors based on the Moodle platform. We run experiments consisting of training and evaluating three machine learning models (Support Vector Machines, Naive Bayes and Adaboost decision trees) under different scenarios. We provide evidences that patterns from interaction counts can provide useful information for classifying at-risk students. This classification allows the customization of the activities presented to at-risk students (automatically or through tutors) as an attempt to avoid students drop out.;;UFPel;pt_BR;Published;16;2016;2016-02-15 15:07:27
62211;Ricardo Araujo Matsumura;Learning to Identify At-Risk Students in Distance Education Using Interaction Counts;Student dropout is one of the main problems faced by distance learning courses. One of the major challenges for researchers is to develop methods to predict the behavior of students so that teachers and tutors are able to identify at-risk students as early as possible and provide assistance before they drop out or fail in their courses. Machine Learning models have been used to predict or classify students in these settings. However, while these models have shown promising results in several settings, they usually attain these results using attributes that are not immediately transferable to other courses or platforms. In this paper, we provide a methodology to classify students using only interaction counts from each student. We evaluate this methodology on a data set from two majors based on the Moodle platform. We run experiments consisting of training and evaluating three machine learning models (Support Vector Machines, Naive Bayes and Adaboost decision trees) under different scenarios. We provide evidences that patterns from interaction counts can provide useful information for classifying at-risk students. This classification allows the customization of the activities presented to at-risk students (automatically or through tutors) as an attempt to avoid students drop out.;;UFPel;pt_BR;Published;16;2016;2016-02-15 15:07:27
62211;Daniela Francisco Brauner;Learning to Identify At-Risk Students in Distance Education Using Interaction Counts;Student dropout is one of the main problems faced by distance learning courses. One of the major challenges for researchers is to develop methods to predict the behavior of students so that teachers and tutors are able to identify at-risk students as early as possible and provide assistance before they drop out or fail in their courses. Machine Learning models have been used to predict or classify students in these settings. However, while these models have shown promising results in several settings, they usually attain these results using attributes that are not immediately transferable to other courses or platforms. In this paper, we provide a methodology to classify students using only interaction counts from each student. We evaluate this methodology on a data set from two majors based on the Moodle platform. We run experiments consisting of training and evaluating three machine learning models (Support Vector Machines, Naive Bayes and Adaboost decision trees) under different scenarios. We provide evidences that patterns from interaction counts can provide useful information for classifying at-risk students. This classification allows the customization of the activities presented to at-risk students (automatically or through tutors) as an attempt to avoid students drop out.;;EA/UFRGS;pt_BR;Published;16;2016;2016-02-15 15:07:27
62657;Eduardo Cassiano Silva;Autômatos celulares unidimensionais caóticos com borda fixa aplicados à modelagem de um sistema criptográfico para imagens digitais;The main objective of data encryption is to enable two entities to communicate over an insecure channel, in such a way that no opponent can decipher the message that is sent. Many classical encryption methods have already been investigated to minimize this problem. A new approach to this topic is Cellular Automata (CAs), which are being studied for their ability to process large volumes of data in parallel. This work investigates a new cellular automaton model for image encryption, which features the use of pre-image calculation. Results showed that the model has great potential for encrypting large volumes of data.;;Instituto Federal do Triângulo Mineiro;pt_BR;Published;26;2016;2016-03-01 15:28:21
62657;Jaqueline A. J. P. Soares;Autômatos celulares unidimensionais caóticos com borda fixa aplicados à modelagem de um sistema criptográfico para imagens digitais;The main objective of data encryption is to enable two entities to communicate over an insecure channel, in such a way that no opponent can decipher the message that is sent. Many classical encryption methods have already been investigated to minimize this problem. A new approach to this topic is Cellular Automata (CAs), which are being studied for their ability to process large volumes of data in parallel. This work investigates a new cellular automaton model for image encryption, which features the use of pre-image calculation. Results showed that the model has great potential for encrypting large volumes of data.;;Instituto Federal do Triângulo Mineiro;pt_BR;Published;26;2016;2016-03-01 15:28:21
62657;Danielli Araújo Lima;Autômatos celulares unidimensionais caóticos com borda fixa aplicados à modelagem de um sistema criptográfico para imagens digitais;The main objective of data encryption is to enable two entities to communicate over an insecure channel, in such a way that no opponent can decipher the message that is sent. Many classical encryption methods have already been investigated to minimize this problem. A new approach to this topic is Cellular Automata (CAs), which are being studied for their ability to process large volumes of data in parallel. This work investigates a new cellular automaton model for image encryption, which features the use of pre-image calculation. Results showed that the model has great potential for encrypting large volumes of data.;;-;pt_BR;Published;26;2016;2016-03-01 15:28:21
64206;Yule Vaz;Emprego de Banco de Filtros e do Teorema de Imersão de Takens em Padrões Espaciais para a Classificação de Imagética Motora em Interfaces Cérebro-Computador;Brain-Computer Interfaces (BCI) are systems that provide an alternative for people with severe or total loss of motor control to interact with the external environment. To map individual intentions into machine operations, BCI systems employ a set of steps that involve capturing and preprocessing brain signals, extracting and selecting their most relevant features, and classifying intentions. In this work, different approaches for extracting features from brain signals were evaluated, including: i) Common Spectro-Spatial Patterns (CSSP) ii) Common Sparse Spectro-Spatial Patterns (CSSSP) iii) CSSP with filter bank (FBCSSP) and, finally, iv) CSSSP with filter bank (FBCSSSP). In common, these techniques use frequency band filtering and space reconstruction to highlight similarities between signals. The Mutual Information-based Feature Selection (MIFS) technique was adopted to reduce the dimensionality of the extracted features and, then, Support Vector Machines (SVM) were used to classify the example space. The experiments considered the BCI Competition IV-2b dataset, which has signals produced by electrodes in positions C3, Cz and C4 in order to identify the intentions of moving the right and left hands. It is concluded, based on the kappa indices obtained, that the adopted feature extractors can present results comparable to the state of the art.;;Universidade de São Paulo - Instituto de Ciências Matemáticas e de Computação;pt_BR;Published;27;2016;2016-04-20 12:53:54
64206;Rodrigo Fernandes de Mello;Emprego de Banco de Filtros e do Teorema de Imersão de Takens em Padrões Espaciais para a Classificação de Imagética Motora em Interfaces Cérebro-Computador;Brain-Computer Interfaces (BCI) are systems that provide an alternative for people with severe or total loss of motor control to interact with the external environment. To map individual intentions into machine operations, BCI systems employ a set of steps that involve capturing and preprocessing brain signals, extracting and selecting their most relevant features, and classifying intentions. In this work, different approaches for extracting features from brain signals were evaluated, including: i) Common Spectro-Spatial Patterns (CSSP) ii) Common Sparse Spectro-Spatial Patterns (CSSSP) iii) CSSP with filter bank (FBCSSP) and, finally, iv) CSSSP with filter bank (FBCSSSP). In common, these techniques use frequency band filtering and space reconstruction to highlight similarities between signals. The Mutual Information-based Feature Selection (MIFS) technique was adopted to reduce the dimensionality of the extracted features and, then, Support Vector Machines (SVM) were used to classify the example space. The experiments considered the BCI Competition IV-2b dataset, which has signals produced by electrodes in positions C3, Cz and C4 in order to identify the intentions of moving the right and left hands. It is concluded, based on the kappa indices obtained, that the adopted feature extractors can present results comparable to the state of the art.;;Universidade de São Paulo - Instituto de Ciências Matemáticas e de Computação;pt_BR;Published;27;2016;2016-04-20 12:53:54
64413;Rodrigo Medeiros Duarte;Comparando o Desempenho de Implementações de Tabelas Hash Concorrentes em Haskell;Implementing a concurrent hash table algorithm that extracts performance is far from a trivial computational task. In this article we present seven different implementations of hash tables in Haskell, exploring everything from low-level synchronization models to those with higher abstraction such as transactional memories. In the tests carried out, the implementation using the STM Haskell transactional memory library was the one that presented the best performance.;;Universidade Federal de Pelotas;pt_BR;Published;16;2016;2016-04-29 13:07:37
64413;André Rauber Du Bois;Comparando o Desempenho de Implementações de Tabelas Hash Concorrentes em Haskell;Implementing a concurrent hash table algorithm that extracts performance is far from a trivial computational task. In this article we present seven different implementations of hash tables in Haskell, exploring everything from low-level synchronization models to those with higher abstraction such as transactional memories. In the tests carried out, the implementation using the STM Haskell transactional memory library was the one that presented the best performance.;;Universidade Federal de Pelotas;pt_BR;Published;16;2016;2016-04-29 13:07:37
64413;Maurício Lima Pilla;Comparando o Desempenho de Implementações de Tabelas Hash Concorrentes em Haskell;Implementing a concurrent hash table algorithm that extracts performance is far from a trivial computational task. In this article we present seven different implementations of hash tables in Haskell, exploring everything from low-level synchronization models to those with higher abstraction such as transactional memories. In the tests carried out, the implementation using the STM Haskell transactional memory library was the one that presented the best performance.;;Universidade Federal de Pelotas;pt_BR;Published;16;2016;2016-04-29 13:07:37
64413;Renata Hax Sander Reiser;Comparando o Desempenho de Implementações de Tabelas Hash Concorrentes em Haskell;Implementing a concurrent hash table algorithm that extracts performance is far from a trivial computational task. In this article we present seven different implementations of hash tables in Haskell, exploring everything from low-level synchronization models to those with higher abstraction such as transactional memories. In the tests carried out, the implementation using the STM Haskell transactional memory library was the one that presented the best performance.;;Universidade Federal de Pelotas;pt_BR;Published;16;2016;2016-04-29 13:07:37
64431;Adriana Bordini;Computação na Educação Básica no Brasil: o Estado da Arte;This article presents a survey of projects in the area of ​​Computational Thinking, focusing on elementary and secondary education, which had results published in the main Information Technology in Education vehicles in Brazil. In this study it was identified that different strategies have been adopted to introduce Computing in Basic Education, highlighting: Algorithms and Programming, Robotics, Games, Unplugged Computing, among others. Each of the strategies was described considering the following aspects: the computer science concepts that have been addressed, the tools used, the way in which the strategy has approached interdisciplinarity, collaboration and/or communication. Furthermore, a statistical description of the work is carried out, highlighting the institutions involved in the projects, the target audience, the approaches and tools used. The objective of this report is to provide an overview of what has already been achieved in the area in Brazil.;;UFPEL;pt_PT;Published;28;2016;2016-04-29 21:26:43
64431;Christiano Martino Otero Avila;Computação na Educação Básica no Brasil: o Estado da Arte;This article presents a survey of projects in the area of ​​Computational Thinking, focusing on elementary and secondary education, which had results published in the main Information Technology in Education vehicles in Brazil. In this study it was identified that different strategies have been adopted to introduce Computing in Basic Education, highlighting: Algorithms and Programming, Robotics, Games, Unplugged Computing, among others. Each of the strategies was described considering the following aspects: the computer science concepts that have been addressed, the tools used, the way in which the strategy has approached interdisciplinarity, collaboration and/or communication. Furthermore, a statistical description of the work is carried out, highlighting the institutions involved in the projects, the target audience, the approaches and tools used. The objective of this report is to provide an overview of what has already been achieved in the area in Brazil.;;UFPEL;pt_PT;Published;28;2016;2016-04-29 21:26:43
64431;Yuri Weisshahn;Computação na Educação Básica no Brasil: o Estado da Arte;This article presents a survey of projects in the area of ​​Computational Thinking, focusing on elementary and secondary education, which had results published in the main Information Technology in Education vehicles in Brazil. In this study it was identified that different strategies have been adopted to introduce Computing in Basic Education, highlighting: Algorithms and Programming, Robotics, Games, Unplugged Computing, among others. Each of the strategies was described considering the following aspects: the computer science concepts that have been addressed, the tools used, the way in which the strategy has approached interdisciplinarity, collaboration and/or communication. Furthermore, a statistical description of the work is carried out, highlighting the institutions involved in the projects, the target audience, the approaches and tools used. The objective of this report is to provide an overview of what has already been achieved in the area in Brazil.;;UFPEL;pt_PT;Published;28;2016;2016-04-29 21:26:43
64431;Mônica Marques da Cunha;Computação na Educação Básica no Brasil: o Estado da Arte;This article presents a survey of projects in the area of ​​Computational Thinking, focusing on elementary and secondary education, which had results published in the main Information Technology in Education vehicles in Brazil. In this study it was identified that different strategies have been adopted to introduce Computing in Basic Education, highlighting: Algorithms and Programming, Robotics, Games, Unplugged Computing, among others. Each of the strategies was described considering the following aspects: the computer science concepts that have been addressed, the tools used, the way in which the strategy has approached interdisciplinarity, collaboration and/or communication. Furthermore, a statistical description of the work is carried out, highlighting the institutions involved in the projects, the target audience, the approaches and tools used. The objective of this report is to provide an overview of what has already been achieved in the area in Brazil.;;UFPEL;pt_PT;Published;28;2016;2016-04-29 21:26:43
64431;Simone André da Costa Cavalheiro;Computação na Educação Básica no Brasil: o Estado da Arte;This article presents a survey of projects in the area of ​​Computational Thinking, focusing on elementary and secondary education, which had results published in the main Information Technology in Education vehicles in Brazil. In this study it was identified that different strategies have been adopted to introduce Computing in Basic Education, highlighting: Algorithms and Programming, Robotics, Games, Unplugged Computing, among others. Each of the strategies was described considering the following aspects: the computer science concepts that have been addressed, the tools used, the way in which the strategy has approached interdisciplinarity, collaboration and/or communication. Furthermore, a statistical description of the work is carried out, highlighting the institutions involved in the projects, the target audience, the approaches and tools used. The objective of this report is to provide an overview of what has already been achieved in the area in Brazil.;;UFPEL;pt_PT;Published;28;2016;2016-04-29 21:26:43
64431;Luciana Foss;Computação na Educação Básica no Brasil: o Estado da Arte;This article presents a survey of projects in the area of ​​Computational Thinking, focusing on elementary and secondary education, which had results published in the main Information Technology in Education vehicles in Brazil. In this study it was identified that different strategies have been adopted to introduce Computing in Basic Education, highlighting: Algorithms and Programming, Robotics, Games, Unplugged Computing, among others. Each of the strategies was described considering the following aspects: the computer science concepts that have been addressed, the tools used, the way in which the strategy has approached interdisciplinarity, collaboration and/or communication. Furthermore, a statistical description of the work is carried out, highlighting the institutions involved in the projects, the target audience, the approaches and tools used. The objective of this report is to provide an overview of what has already been achieved in the area in Brazil.;;UFPEL;pt_PT;Published;28;2016;2016-04-29 21:26:43
64431;Marilton Sanchotene Aguiar;Computação na Educação Básica no Brasil: o Estado da Arte;This article presents a survey of projects in the area of ​​Computational Thinking, focusing on elementary and secondary education, which had results published in the main Information Technology in Education vehicles in Brazil. In this study it was identified that different strategies have been adopted to introduce Computing in Basic Education, highlighting: Algorithms and Programming, Robotics, Games, Unplugged Computing, among others. Each of the strategies was described considering the following aspects: the computer science concepts that have been addressed, the tools used, the way in which the strategy has approached interdisciplinarity, collaboration and/or communication. Furthermore, a statistical description of the work is carried out, highlighting the institutions involved in the projects, the target audience, the approaches and tools used. The objective of this report is to provide an overview of what has already been achieved in the area in Brazil.;;UFPEL;pt_PT;Published;28;2016;2016-04-29 21:26:43
64431;Renata Hax Sander Reiser;Computação na Educação Básica no Brasil: o Estado da Arte;This article presents a survey of projects in the area of ​​Computational Thinking, focusing on elementary and secondary education, which had results published in the main Information Technology in Education vehicles in Brazil. In this study it was identified that different strategies have been adopted to introduce Computing in Basic Education, highlighting: Algorithms and Programming, Robotics, Games, Unplugged Computing, among others. Each of the strategies was described considering the following aspects: the computer science concepts that have been addressed, the tools used, the way in which the strategy has approached interdisciplinarity, collaboration and/or communication. Furthermore, a statistical description of the work is carried out, highlighting the institutions involved in the projects, the target audience, the approaches and tools used. The objective of this report is to provide an overview of what has already been achieved in the area in Brazil.;;-;pt_PT;Published;28;2016;2016-04-29 21:26:43
65372;Alexandre Santiago de Abreu;QEDS: Um Simulador Clássico para Distinção de Elementos Quântico;The problem of deciding whether all N elements in a list are distinct requires Omega(N) queries in the classical model. A quantum algorithm based on quantum walking on a Johnson graph improves this limit to O(N2/3) queries. The quantum algorithm for distinguishing elements performs several calculations, each involving non-trivial superpositions of states. For this reason, it is difficult to study the algorithm without appropriate tools. In this work, we present a numerical simulator for the element distinction algorithm and analyze its performance. The main purpose of our simulator is to serve as an educational tool. However, as free open source software, it can be easily extended for professional use.;;Universidade Federal do Rio de Janeiro;en_US;Published;15;2016;2016-06-11 18:08:14
65372;Matheus Manzoli Ferreira;QEDS: Um Simulador Clássico para Distinção de Elementos Quântico;The problem of deciding whether all N elements in a list are distinct requires Omega(N) queries in the classical model. A quantum algorithm based on quantum walking on a Johnson graph improves this limit to O(N2/3) queries. The quantum algorithm for distinguishing elements performs several calculations, each involving non-trivial superpositions of states. For this reason, it is difficult to study the algorithm without appropriate tools. In this work, we present a numerical simulator for the element distinction algorithm and analyze its performance. The main purpose of our simulator is to serve as an educational tool. However, as free open source software, it can be easily extended for professional use.;;UFF;en_US;Published;15;2016;2016-06-11 18:08:14
65372;Luis Antonio Brasil Kowada;QEDS: Um Simulador Clássico para Distinção de Elementos Quântico;The problem of deciding whether all N elements in a list are distinct requires Omega(N) queries in the classical model. A quantum algorithm based on quantum walking on a Johnson graph improves this limit to O(N2/3) queries. The quantum algorithm for distinguishing elements performs several calculations, each involving non-trivial superpositions of states. For this reason, it is difficult to study the algorithm without appropriate tools. In this work, we present a numerical simulator for the element distinction algorithm and analyze its performance. The main purpose of our simulator is to serve as an educational tool. However, as free open source software, it can be easily extended for professional use.;;UFF;en_US;Published;15;2016;2016-06-11 18:08:14
65372;Franklin de Lima Marquezino;QEDS: Um Simulador Clássico para Distinção de Elementos Quântico;The problem of deciding whether all N elements in a list are distinct requires Omega(N) queries in the classical model. A quantum algorithm based on quantum walking on a Johnson graph improves this limit to O(N2/3) queries. The quantum algorithm for distinguishing elements performs several calculations, each involving non-trivial superpositions of states. For this reason, it is difficult to study the algorithm without appropriate tools. In this work, we present a numerical simulator for the element distinction algorithm and analyze its performance. The main purpose of our simulator is to serve as an educational tool. However, as free open source software, it can be easily extended for professional use.;;UFRJ;en_US;Published;15;2016;2016-06-11 18:08:14
65538;Rogério Luis Rizzi;Autômatos Celulares e Sistema Multiagente para Simulação da Propagação de Doenças;This work presents a study on epidemiological computational models focusing on the Susceptible-Infected-Removed type, SIR, and different solution strategies for computer simulation of the spread of communicable diseases. It is shown that models based on cellular automata like Lattice Gas Cellular Automata, LGCA, have solutions similar to those obtained by multi-agent systems, unlike diffusive cellular automata. The results obtained from the literature, as well as those resulting from this work, indicate that such methodologies have the potential to simulate the dynamics of ecological, biological and physical phenomena that can thus be modeled. They indicate, however, that results more consistent with real data depend on the development and parameterization of models that add essential characteristics to the phenomenon, such as the interaction between individuals and the environment, and the heterogeneity of the relational system of contacts.;;Universidade Estadual do Oeste do Paraná - UNIOESTE;pt_BR;Published;23;2016;2016-06-16 9:08:58
65538;Claudia Brandelero Rizzi;Autômatos Celulares e Sistema Multiagente para Simulação da Propagação de Doenças;This work presents a study on epidemiological computational models focusing on the Susceptible-Infected-Removed type, SIR, and different solution strategies for computer simulation of the spread of communicable diseases. It is shown that models based on cellular automata like Lattice Gas Cellular Automata, LGCA, have solutions similar to those obtained by multi-agent systems, unlike diffusive cellular automata. The results obtained from the literature, as well as those resulting from this work, indicate that such methodologies have the potential to simulate the dynamics of ecological, biological and physical phenomena that can thus be modeled. They indicate, however, that results more consistent with real data depend on the development and parameterization of models that add essential characteristics to the phenomenon, such as the interaction between individuals and the environment, and the heterogeneity of the relational system of contacts.;;Universidade Estadual do Oeste do Paraná - UNIOESTE;pt_BR;Published;23;2016;2016-06-16 9:08:58
65538;Wesley Luciano Kaiser;Autômatos Celulares e Sistema Multiagente para Simulação da Propagação de Doenças;This work presents a study on epidemiological computational models focusing on the Susceptible-Infected-Removed type, SIR, and different solution strategies for computer simulation of the spread of communicable diseases. It is shown that models based on cellular automata like Lattice Gas Cellular Automata, LGCA, have solutions similar to those obtained by multi-agent systems, unlike diffusive cellular automata. The results obtained from the literature, as well as those resulting from this work, indicate that such methodologies have the potential to simulate the dynamics of ecological, biological and physical phenomena that can thus be modeled. They indicate, however, that results more consistent with real data depend on the development and parameterization of models that add essential characteristics to the phenomenon, such as the interaction between individuals and the environment, and the heterogeneity of the relational system of contacts.;;Universidade Estadual do Oeste do Paraná - UNIOESTE;pt_BR;Published;23;2016;2016-06-16 9:08:58
65538;Petterson Vinícius Pramiu;Autômatos Celulares e Sistema Multiagente para Simulação da Propagação de Doenças;This work presents a study on epidemiological computational models focusing on the Susceptible-Infected-Removed type, SIR, and different solution strategies for computer simulation of the spread of communicable diseases. It is shown that models based on cellular automata like Lattice Gas Cellular Automata, LGCA, have solutions similar to those obtained by multi-agent systems, unlike diffusive cellular automata. The results obtained from the literature, as well as those resulting from this work, indicate that such methodologies have the potential to simulate the dynamics of ecological, biological and physical phenomena that can thus be modeled. They indicate, however, that results more consistent with real data depend on the development and parameterization of models that add essential characteristics to the phenomenon, such as the interaction between individuals and the environment, and the heterogeneity of the relational system of contacts.;;Universidade de São Paulo - USP;pt_BR;Published;23;2016;2016-06-16 9:08:58
65827;Gustavo de Paula Avelar;Comparação entre abordagens escaláveis para o processamento de conjuntos de dados textuais;DataAnalytics is a concept aimed at analyzing large amounts of data in search of patterns and relevant information. Manipulating this data is complex and requires automatic methods capable of processing large volumes of data requiring computational power to obtain information in a timely manner. The MapReduce programming model emerged to help distribute these problems among several machines, improving processing efficiency. The Apache Hadoop and Spark platforms enable the use of this paradigm in commodity hardware environments. Data grouping aims to determine a finite set of categories to describe a data set according to the similar characteristics of the objects in the data set. Different strategies for preprocessing influence the results of the data grouping stage. Therefore, this work deals with the study of different methods of pre-processing textual documents, aiming to achieve representations that provide good results in the grouping stage. In it, we propose an approach for attribute selection based on the Latent Dirichlet Allocation (LDA) algorithm.;;Universidade Federal de Viçosa - Campus Rio Paranaíba;pt_BR;Published;28;2016;2016-06-28 20:32:00
65827;Murilo Coelho Naldi;Comparação entre abordagens escaláveis para o processamento de conjuntos de dados textuais;DataAnalytics is a concept aimed at analyzing large amounts of data in search of patterns and relevant information. Manipulating this data is complex and requires automatic methods capable of processing large volumes of data requiring computational power to obtain information in a timely manner. The MapReduce programming model emerged to help distribute these problems among several machines, improving processing efficiency. The Apache Hadoop and Spark platforms enable the use of this paradigm in commodity hardware environments. Data grouping aims to determine a finite set of categories to describe a data set according to the similar characteristics of the objects in the data set. Different strategies for preprocessing influence the results of the data grouping stage. Therefore, this work deals with the study of different methods of pre-processing textual documents, aiming to achieve representations that provide good results in the grouping stage. In it, we propose an approach for attribute selection based on the Latent Dirichlet Allocation (LDA) algorithm.;;Departamento de Computação, Universidade Federal de São Carlos, Rod. Washington Luís - Km 235 Caixa Postal 676, CEP 13565-905, São Carlos-SP;pt_BR;Published;28;2016;2016-06-28 20:32:00
69309;Luís Felipe Bilecki;Uma Revisão Sistemática sobre Abordagens em Organizações Virtuais no Contexto de Confiança, Reputação e Computação em Nuvem;Virtual Organizations represent a prominent form of collaboration in which a set of companies share skills, competencies and risks to meet a common objective. Some concerns arise during collaboration, such as issues relating to trust, reputation and the use of technological resources. Trust and reputation in OVs aim to minimize risk when dealing with unknown entities. The use of technological resources, such as cloud computing, provides support for collaboration opportunities, but presents some problems related to authentication, security, privacy and trust. Therefore, this article presents a systematic review whose objective is to verify the approaches that are being carried out in the context of Virtual Organizations and that are related to at least one of the concepts of trust, reputation or cloud computing. To this end, the search in specialized literature resulted in 75 articles, 24 of which were selected for analysis. The criteria used for selection were relevant articles in the last 5 years, complete, in the English language and that related Virtual Organization with at least one of the concepts of trust, reputation and cloud computing. Therefore, it was identified how each concept is approached and it was found that the same concept presents itself in different ways and none of the works sought to present the integration of these concepts.;;Programa de Pós-Graduação em Computação Aplicada, Centro de Ciências Tecnológicas, Universidade do Estado de Santa Catarina,Caixa Postal 631 - Joinville, Santa Catarina, Brasil;pt_BR;Published;29;2016;2016-11-08 19:48:06
69309;Marcelo da Silva Hounsell;Uma Revisão Sistemática sobre Abordagens em Organizações Virtuais no Contexto de Confiança, Reputação e Computação em Nuvem;Virtual Organizations represent a prominent form of collaboration in which a set of companies share skills, competencies and risks to meet a common objective. Some concerns arise during collaboration, such as issues relating to trust, reputation and the use of technological resources. Trust and reputation in OVs aim to minimize risk when dealing with unknown entities. The use of technological resources, such as cloud computing, provides support for collaboration opportunities, but presents some problems related to authentication, security, privacy and trust. Therefore, this article presents a systematic review whose objective is to verify the approaches that are being carried out in the context of Virtual Organizations and that are related to at least one of the concepts of trust, reputation or cloud computing. To this end, the search in specialized literature resulted in 75 articles, 24 of which were selected for analysis. The criteria used for selection were relevant articles in the last 5 years, complete, in the English language and that related Virtual Organization with at least one of the concepts of trust, reputation and cloud computing. Therefore, it was identified how each concept is approached and it was found that the same concept presents itself in different ways and none of the works sought to present the integration of these concepts.;;Departamento de Ciência da Computação (DCC), Programa de Pós-Graduação em Computação Aplicada, Centrode Ciências Tecnológicas, Universidade do Estado de Santa Catarina, Caixa Postal 631 - Joinville, Santa Catarina,Brasil;pt_BR;Published;29;2016;2016-11-08 19:48:06
69309;Adriano Fiorese;Uma Revisão Sistemática sobre Abordagens em Organizações Virtuais no Contexto de Confiança, Reputação e Computação em Nuvem;Virtual Organizations represent a prominent form of collaboration in which a set of companies share skills, competencies and risks to meet a common objective. Some concerns arise during collaboration, such as issues relating to trust, reputation and the use of technological resources. Trust and reputation in OVs aim to minimize risk when dealing with unknown entities. The use of technological resources, such as cloud computing, provides support for collaboration opportunities, but presents some problems related to authentication, security, privacy and trust. Therefore, this article presents a systematic review whose objective is to verify the approaches that are being carried out in the context of Virtual Organizations and that are related to at least one of the concepts of trust, reputation or cloud computing. To this end, the search in specialized literature resulted in 75 articles, 24 of which were selected for analysis. The criteria used for selection were relevant articles in the last 5 years, complete, in the English language and that related Virtual Organization with at least one of the concepts of trust, reputation and cloud computing. Therefore, it was identified how each concept is approached and it was found that the same concept presents itself in different ways and none of the works sought to present the integration of these concepts.;;Departamento de Ciência da Computação (DCC), Programa de Pós-Graduação em Computação Aplicada, Centrode Ciências Tecnológicas, Universidade do Estado de Santa Catarina, Caixa Postal 631 - Joinville, Santa Catarina,Brasil;pt_BR;Published;29;2016;2016-11-08 19:48:06
71429;Félix Carvalho Rodrigues;Non-Cooperative Facility Location Games: a Survey;The Facility Location problem is a well-know NP-Hard combinatorial optimization problem. It models a diverse set of situations where one aims to provide a set of goods or services via a set of facilities F to a set of clients T, also called terminals. There are opening costs for each facility in F and connection costs for each pair of facility and client, if such facility attends this client. A central authority wants to determine the solution with minimum cost, considering both opening and connection costs, in such a way that all clients are attended by one facility. In this survey we are interested in the non-cooperative game version of this problem, where instead of having a central authority, each client is a player and decides where to con- nect himself. In doing so, he aims to minimize his own costs, given by the connection costs and opening costs of the facility, which may be shared among clients using the same facility. This problem has several applications as well, specially in distributed scenarios where a central authority is too expensive or even infeasible to exist. In this paper we present a survey describing different variants of this problem and reviewing several results about it, as well as adapting results from existing literature concerning the existence of equilibria, Price of Anarchy and Price of Stability. We also point out open problems that remain to be addressed. ;;Universidade Estadual de Campinas;en_US;Published;31;2017;2017-02-24 7:47:37
71429;Eduardo Xavier;Non-Cooperative Facility Location Games: a Survey;The Facility Location problem is a well-know NP-Hard combinatorial optimization problem. It models a diverse set of situations where one aims to provide a set of goods or services via a set of facilities F to a set of clients T, also called terminals. There are opening costs for each facility in F and connection costs for each pair of facility and client, if such facility attends this client. A central authority wants to determine the solution with minimum cost, considering both opening and connection costs, in such a way that all clients are attended by one facility. In this survey we are interested in the non-cooperative game version of this problem, where instead of having a central authority, each client is a player and decides where to con- nect himself. In doing so, he aims to minimize his own costs, given by the connection costs and opening costs of the facility, which may be shared among clients using the same facility. This problem has several applications as well, specially in distributed scenarios where a central authority is too expensive or even infeasible to exist. In this paper we present a survey describing different variants of this problem and reviewing several results about it, as well as adapting results from existing literature concerning the existence of equilibria, Price of Anarchy and Price of Stability. We also point out open problems that remain to be addressed. ;;-;en_US;Published;31;2017;2017-02-24 7:47:37
71775;Juccelino Rodrigues Alves Barros;Análise de desempenho de Banco de Dados Relacionais e Não Relacionais em dados genômicos;Storing genomic data is a major challenge today, as with the advancement of molecular technology the amount of genomic data generated is increasing, so that the sequencing of a single organism can generate files with gigabytes of information. In general, genomic data manipulation processes use simple files as the main means of storing such data. However, modern databases present themselves as an alternative for managing this data by offering better organization, fault tolerance, better use of available storage space and performance. Furthermore, databases make it possible to add meta-information about the stored DNA sequences to the raw sequencing data. Given this scenario, this work presents and evaluates the performance of different storage strategies in three databases belonging to two different paradigms, MySQL (representative of Relational databases), Cassandra and MongoDB (representative of Non-Relational databases). The results demonstrated that relational databases have limitations when they are inserted in an environment with large masses of data.;;Universidade Federal Rural de Pernambuco - UFRPE;pt_BR;Published;16;2017;2017-03-10 9:54:23
71775;Gustavo Almeida Callou;Análise de desempenho de Banco de Dados Relacionais e Não Relacionais em dados genômicos;Storing genomic data is a major challenge today, as with the advancement of molecular technology the amount of genomic data generated is increasing, so that the sequencing of a single organism can generate files with gigabytes of information. In general, genomic data manipulation processes use simple files as the main means of storing such data. However, modern databases present themselves as an alternative for managing this data by offering better organization, fault tolerance, better use of available storage space and performance. Furthermore, databases make it possible to add meta-information about the stored DNA sequences to the raw sequencing data. Given this scenario, this work presents and evaluates the performance of different storage strategies in three databases belonging to two different paradigms, MySQL (representative of Relational databases), Cassandra and MongoDB (representative of Non-Relational databases). The results demonstrated that relational databases have limitations when they are inserted in an environment with large masses of data.;;Universidade Federal Rural de Pernambuco - UFRPE;pt_BR;Published;16;2017;2017-03-10 9:54:23
71775;Glauco Gonçalves;Análise de desempenho de Banco de Dados Relacionais e Não Relacionais em dados genômicos;Storing genomic data is a major challenge today, as with the advancement of molecular technology the amount of genomic data generated is increasing, so that the sequencing of a single organism can generate files with gigabytes of information. In general, genomic data manipulation processes use simple files as the main means of storing such data. However, modern databases present themselves as an alternative for managing this data by offering better organization, fault tolerance, better use of available storage space and performance. Furthermore, databases make it possible to add meta-information about the stored DNA sequences to the raw sequencing data. Given this scenario, this work presents and evaluates the performance of different storage strategies in three databases belonging to two different paradigms, MySQL (representative of Relational databases), Cassandra and MongoDB (representative of Non-Relational databases). The results demonstrated that relational databases have limitations when they are inserted in an environment with large masses of data.;;Universidade Federal Rural de Pernambuco - UFRPE;pt_BR;Published;16;2017;2017-03-10 9:54:23
71775;Victor Wanderley;Análise de desempenho de Banco de Dados Relacionais e Não Relacionais em dados genômicos;Storing genomic data is a major challenge today, as with the advancement of molecular technology the amount of genomic data generated is increasing, so that the sequencing of a single organism can generate files with gigabytes of information. In general, genomic data manipulation processes use simple files as the main means of storing such data. However, modern databases present themselves as an alternative for managing this data by offering better organization, fault tolerance, better use of available storage space and performance. Furthermore, databases make it possible to add meta-information about the stored DNA sequences to the raw sequencing data. Given this scenario, this work presents and evaluates the performance of different storage strategies in three databases belonging to two different paradigms, MySQL (representative of Relational databases), Cassandra and MongoDB (representative of Non-Relational databases). The results demonstrated that relational databases have limitations when they are inserted in an environment with large masses of data.;;Universidade Federal Rural de Pernambuco - UFRPE;pt_BR;Published;16;2017;2017-03-10 9:54:23
71775;Henrique Casteletti;Análise de desempenho de Banco de Dados Relacionais e Não Relacionais em dados genômicos;Storing genomic data is a major challenge today, as with the advancement of molecular technology the amount of genomic data generated is increasing, so that the sequencing of a single organism can generate files with gigabytes of information. In general, genomic data manipulation processes use simple files as the main means of storing such data. However, modern databases present themselves as an alternative for managing this data by offering better organization, fault tolerance, better use of available storage space and performance. Furthermore, databases make it possible to add meta-information about the stored DNA sequences to the raw sequencing data. Given this scenario, this work presents and evaluates the performance of different storage strategies in three databases belonging to two different paradigms, MySQL (representative of Relational databases), Cassandra and MongoDB (representative of Non-Relational databases). The results demonstrated that relational databases have limitations when they are inserted in an environment with large masses of data.;;Universidade Federal de Pernambuco - UFPE;pt_BR;Published;16;2017;2017-03-10 9:54:23
73173;Davi Padilha Mesquita;Estudo e Modelagem dos Efeitos da Forma e do Crescimento em Processos de Formação de Padrões de Pelagem via Reação-Difusão;Reaction-diffusion models are widely used in simulating pattern formation in living beings, being applied both to explain these phenomena and to generate textures in computer graphics. However, few studies take into account the influence of surface shape on the resulting pattern. In this work, we performed simulations with Turing's nonlinear reaction-diffusion model on two-dimensional surfaces using different parameters and surface shapes. Our results show that the shape has a relevant effect on the resulting final pattern, allowing the transition from spots to stripes, the change stability and robustness of the system, and obtaining geometrically regular patterns.;;Instituto de Informática - UFRGS;pt_BR;Published;18;2017;2017-05-06 5:44:41
73173;Marcelo Walter;Estudo e Modelagem dos Efeitos da Forma e do Crescimento em Processos de Formação de Padrões de Pelagem via Reação-Difusão;Reaction-diffusion models are widely used in simulating pattern formation in living beings, being applied both to explain these phenomena and to generate textures in computer graphics. However, few studies take into account the influence of surface shape on the resulting pattern. In this work, we performed simulations with Turing's nonlinear reaction-diffusion model on two-dimensional surfaces using different parameters and surface shapes. Our results show that the shape has a relevant effect on the resulting final pattern, allowing the transition from spots to stripes, the change stability and robustness of the system, and obtaining geometrically regular patterns.;;Instituto de Informática - UFRGS;pt_BR;Published;18;2017;2017-05-06 5:44:41
74030;Eduardo Tiago Braun;Sleep Stages Classification Using Spectral Based Statistical Moments as Features;In the pursuit of highly effective and efficient portable sleep classification systems, researchers have been testing a massive number of combinations of EEG features and classifiers.  State of art sleep classification ensembles achieve accuracy in the order of 90%.  However, there is presently no consensus regarding the best setof features for sleep staging with single channel EEG, leading researchers to modify feature selection according to the number of classification stages. This paper introduces a reduced set of frequency-domain features capable of yielding high classification accuracy (90.9%, 91.8%, 92.4%, 94.3% and 97.1%) for all 6- to 2-state sleep stages.  The proposed system uses fast Fourier transform (FFT) to convert data from Pz-Oz EEG channel into the frequency domain. Afterwards, eight statistical features are extracted from specific frequency ranges and fed into a random forest classifier.;sleep stage classification, frequency domain, single EEG channel, random forest;Universidade Federal de Santa Maria;en_US;Published;11;2017;2017-06-07 13:51:58
74030;Alice de Jesus Kozakevicius;Sleep Stages Classification Using Spectral Based Statistical Moments as Features;In the pursuit of highly effective and efficient portable sleep classification systems, researchers have been testing a massive number of combinations of EEG features and classifiers.  State of art sleep classification ensembles achieve accuracy in the order of 90%.  However, there is presently no consensus regarding the best setof features for sleep staging with single channel EEG, leading researchers to modify feature selection according to the number of classification stages. This paper introduces a reduced set of frequency-domain features capable of yielding high classification accuracy (90.9%, 91.8%, 92.4%, 94.3% and 97.1%) for all 6- to 2-state sleep stages.  The proposed system uses fast Fourier transform (FFT) to convert data from Pz-Oz EEG channel into the frequency domain. Afterwards, eight statistical features are extracted from specific frequency ranges and fed into a random forest classifier.;sleep stage classification, frequency domain, single EEG channel, random forest;Universidade Federal de Santa Maria;en_US;Published;11;2017;2017-06-07 13:51:58
74030;Thiago Lopes Trugillo da Silveira;Sleep Stages Classification Using Spectral Based Statistical Moments as Features;In the pursuit of highly effective and efficient portable sleep classification systems, researchers have been testing a massive number of combinations of EEG features and classifiers.  State of art sleep classification ensembles achieve accuracy in the order of 90%.  However, there is presently no consensus regarding the best setof features for sleep staging with single channel EEG, leading researchers to modify feature selection according to the number of classification stages. This paper introduces a reduced set of frequency-domain features capable of yielding high classification accuracy (90.9%, 91.8%, 92.4%, 94.3% and 97.1%) for all 6- to 2-state sleep stages.  The proposed system uses fast Fourier transform (FFT) to convert data from Pz-Oz EEG channel into the frequency domain. Afterwards, eight statistical features are extracted from specific frequency ranges and fed into a random forest classifier.;sleep stage classification, frequency domain, single EEG channel, random forest;Universidade Federal do Rio Grande do Sul;en_US;Published;11;2017;2017-06-07 13:51:58
74030;Cesar Ramos Rodrigues;Sleep Stages Classification Using Spectral Based Statistical Moments as Features;In the pursuit of highly effective and efficient portable sleep classification systems, researchers have been testing a massive number of combinations of EEG features and classifiers.  State of art sleep classification ensembles achieve accuracy in the order of 90%.  However, there is presently no consensus regarding the best setof features for sleep staging with single channel EEG, leading researchers to modify feature selection according to the number of classification stages. This paper introduces a reduced set of frequency-domain features capable of yielding high classification accuracy (90.9%, 91.8%, 92.4%, 94.3% and 97.1%) for all 6- to 2-state sleep stages.  The proposed system uses fast Fourier transform (FFT) to convert data from Pz-Oz EEG channel into the frequency domain. Afterwards, eight statistical features are extracted from specific frequency ranges and fed into a random forest classifier.;sleep stage classification, frequency domain, single EEG channel, random forest;Universidade Federal de Santa Maria;en_US;Published;11;2017;2017-06-07 13:51:58
74030;Giovani Baratto;Sleep Stages Classification Using Spectral Based Statistical Moments as Features;In the pursuit of highly effective and efficient portable sleep classification systems, researchers have been testing a massive number of combinations of EEG features and classifiers.  State of art sleep classification ensembles achieve accuracy in the order of 90%.  However, there is presently no consensus regarding the best setof features for sleep staging with single channel EEG, leading researchers to modify feature selection according to the number of classification stages. This paper introduces a reduced set of frequency-domain features capable of yielding high classification accuracy (90.9%, 91.8%, 92.4%, 94.3% and 97.1%) for all 6- to 2-state sleep stages.  The proposed system uses fast Fourier transform (FFT) to convert data from Pz-Oz EEG channel into the frequency domain. Afterwards, eight statistical features are extracted from specific frequency ranges and fed into a random forest classifier.;sleep stage classification, frequency domain, single EEG channel, random forest;Universidade Federal de Santa Maria;en_US;Published;11;2017;2017-06-07 13:51:58
74992;Paulo André Lima de Castro;Análise Autônoma de Investimento: Uma Abordagem Multiagente Discreta;Since the early days of computer science, researchers have wondered where the line is between tasks that machines can do and those that only humans can perform. Several tasks were identified as impossible for machines and later conquered by new advances in Artificial Intelligence. Nowadays, it seems that we are not far from the day when driving cars will be included in the tasks that machines can do efficiently. Certainly, even more complex activities will be dominated by machines in the future. In this article, we argue that investment analysis, the process of evaluating and selecting investments in terms of risk and return, may be among the tasks performed efficiently by machines in the perhaps not distant future. In fact, there are significant research efforts to create algorithms and quantitative methods for analyzing investments. We present a brief review about them. Through this review, we can see that there are many challenges and complexities to be faced in the pursuit of autonomous investment analysis (AAI). In this article, we propose an approach to simplify the problem of autonomous investment analysis capable of dealing with the identified complexities (nature of assets, multiple analysis algorithms per asset, non-stationarity and multiple investment horizons). This approach is based on the simultaneous use of several autonomous agents and the discretization of the AAI problem and its modeling as a classification problem. This approach breaks down the complexity faced by AAI into problems that can be addressed by a group of agents working together to provide intelligent, personalized investment advice to individuals. We present an implementation of this approach and results obtained through its use with historical data from the Brazilian capital market. We believe that such an approach can contribute to the development of AAI. Furthermore, this approach allows the incorporation of already known algorithms and techniques that can help solve part of the problem.;Autonomous analysis, technical analysis, artificial intelligence, online learning;Intituto Tecnológico de Aeronáutica (ITA);pt_BR;Published;15;2017;2017-07-14 15:01:20
74992;Ronald Annoni Junior;Análise Autônoma de Investimento: Uma Abordagem Multiagente Discreta;Since the early days of computer science, researchers have wondered where the line is between tasks that machines can do and those that only humans can perform. Several tasks were identified as impossible for machines and later conquered by new advances in Artificial Intelligence. Nowadays, it seems that we are not far from the day when driving cars will be included in the tasks that machines can do efficiently. Certainly, even more complex activities will be dominated by machines in the future. In this article, we argue that investment analysis, the process of evaluating and selecting investments in terms of risk and return, may be among the tasks performed efficiently by machines in the perhaps not distant future. In fact, there are significant research efforts to create algorithms and quantitative methods for analyzing investments. We present a brief review about them. Through this review, we can see that there are many challenges and complexities to be faced in the pursuit of autonomous investment analysis (AAI). In this article, we propose an approach to simplify the problem of autonomous investment analysis capable of dealing with the identified complexities (nature of assets, multiple analysis algorithms per asset, non-stationarity and multiple investment horizons). This approach is based on the simultaneous use of several autonomous agents and the discretization of the AAI problem and its modeling as a classification problem. This approach breaks down the complexity faced by AAI into problems that can be addressed by a group of agents working together to provide intelligent, personalized investment advice to individuals. We present an implementation of this approach and results obtained through its use with historical data from the Brazilian capital market. We believe that such an approach can contribute to the development of AAI. Furthermore, this approach allows the incorporation of already known algorithms and techniques that can help solve part of the problem.;Autonomous analysis, technical analysis, artificial intelligence, online learning;Raifa Sistems InteligentesSão José dos Campos, SP, Brasil;pt_BR;Published;15;2017;2017-07-14 15:01:20
74992;Jaime Simão Sichman;Análise Autônoma de Investimento: Uma Abordagem Multiagente Discreta;Since the early days of computer science, researchers have wondered where the line is between tasks that machines can do and those that only humans can perform. Several tasks were identified as impossible for machines and later conquered by new advances in Artificial Intelligence. Nowadays, it seems that we are not far from the day when driving cars will be included in the tasks that machines can do efficiently. Certainly, even more complex activities will be dominated by machines in the future. In this article, we argue that investment analysis, the process of evaluating and selecting investments in terms of risk and return, may be among the tasks performed efficiently by machines in the perhaps not distant future. In fact, there are significant research efforts to create algorithms and quantitative methods for analyzing investments. We present a brief review about them. Through this review, we can see that there are many challenges and complexities to be faced in the pursuit of autonomous investment analysis (AAI). In this article, we propose an approach to simplify the problem of autonomous investment analysis capable of dealing with the identified complexities (nature of assets, multiple analysis algorithms per asset, non-stationarity and multiple investment horizons). This approach is based on the simultaneous use of several autonomous agents and the discretization of the AAI problem and its modeling as a classification problem. This approach breaks down the complexity faced by AAI into problems that can be addressed by a group of agents working together to provide intelligent, personalized investment advice to individuals. We present an implementation of this approach and results obtained through its use with historical data from the Brazilian capital market. We believe that such an approach can contribute to the development of AAI. Furthermore, this approach allows the incorporation of already known algorithms and techniques that can help solve part of the problem.;Autonomous analysis, technical analysis, artificial intelligence, online learning;Laboratório de Técnicas Inteligentes (LTI)3 Universidade de São Paulo (USP)São Paulo, SP, Brasil;pt_BR;Published;15;2017;2017-07-14 15:01:20
75048;Adilson Lopes Khouri;Combining Artificial Intelligence, Ontology, and Frequency-based Approaches to Recommend Activities in Scientific Workflows;The number of activities provided by scientific workflow management systems is large, which requires scientists to know many of them to take advantage of the reusability of these systems. To minimize this problem, the literature presents some techniques to recommend activities during the scientific workflow construction. In this paper we specified and developed a hybrid activity recommendation system considering information on frequency, input and outputs of activities and ontological annotations. Additionally, this paper presents a modeling of activities recommendation as a classification problem, tested using 5 classifiers 5 regressors and a composite approach which uses a Support Vector Machine (SVM) classifier, combining the results of other classifiers and regressors to recommend and Rotation Forest, an ensemble of classifiers. The proposed technique was compared to related techniques and to classifiers and regressors, using 10-fold-cross-validation, achieving a Mean Reciprocal Rank (MRR) at least 70% greater than those obtained by classical techniques.;recommendation system, scientific workflows, artificial intelligence, ontology;Universidade de São Paulo;en_US;Published;8;2017;2017-07-17 19:16:32
75048;Luciano Antonio Digiampietri;Combining Artificial Intelligence, Ontology, and Frequency-based Approaches to Recommend Activities in Scientific Workflows;The number of activities provided by scientific workflow management systems is large, which requires scientists to know many of them to take advantage of the reusability of these systems. To minimize this problem, the literature presents some techniques to recommend activities during the scientific workflow construction. In this paper we specified and developed a hybrid activity recommendation system considering information on frequency, input and outputs of activities and ontological annotations. Additionally, this paper presents a modeling of activities recommendation as a classification problem, tested using 5 classifiers 5 regressors and a composite approach which uses a Support Vector Machine (SVM) classifier, combining the results of other classifiers and regressors to recommend and Rotation Forest, an ensemble of classifiers. The proposed technique was compared to related techniques and to classifiers and regressors, using 10-fold-cross-validation, achieving a Mean Reciprocal Rank (MRR) at least 70% greater than those obtained by classical techniques.;recommendation system, scientific workflows, artificial intelligence, ontology;Universidade de São Paulo;en_US;Published;8;2017;2017-07-17 19:16:32
75063;Antônio Carlos Rocha Costa;Energy Systems in Material Agent Societies;This paper concerns material agent societies, that is, agent societies inhabited by material agents. The paper introduces the concept of energy system of a material agent society, as the organizational sub-system of the society that is responsible for the coordination of the production, distribution and consumption of energy in that society. Two models (labor-based and work-based) for energy systems, the conditions of their equilibrated functioning are formulated, and the concept of energy autonomous material agent society are formally defined. The possible relation between economic and political systems, and energy systems, in material agent societies, is indicated.;;UFRGS-FURG;en_US;Published;14;2017;2017-07-18 10:34:07
75255;Wilamis Kleiton Nunes da Silva;Estratégias de Construções de Comitês de Classificadores Multirrótulos no Aprendizado Semissupervisionado Multidescrição;Multi-label classification is a supervised learning problem in which an object can be associated with multiple classes. Among the different multi-label classification methods, the BR (Binary Relevance), LP (Label Powerset) and RAkEL (RAndom k-labELsets) methods stand out. The work carried out a study on the construction of multi-label classifier committees built through the application of semi-supervised multi-description learning techniques. The classifier committees used in the experiments were Bagging, Boosting and Stacking as methods of transforming the problem, we used the BR, LP and Rakel methods in multi-label semi-supervised multi-description classification, Co-Training was used, five different algorithms were applied as base classifiers: k- NN (k Nearest Neighbors), J48 (Decision Tree Induction Algorithm), SVM (Support Vector Machines), NB (Naive Bayes) and JRip (Extended Repeated Incremental Pruning). All experiments used the 10-fold Cross-Validation methodology and the MULAN framework, which is implemented using WEKA. For the sizes of the classifier committees, we adopted the values ​​3, 5, 7 and 9. To analyze the results, the Wilcoxon statistical test was used. At the end of the experimental analyses, it was found that the semi-supervised approach presented competitive results in relation to supervised learning, since the two approaches used presented statistically similar results.;;Universidade Federal Rural do Semi-Árido;pt_BR;Published;29;2017;2017-07-26 13:14:27
75255;Araken de Medeiros Santos;Estratégias de Construções de Comitês de Classificadores Multirrótulos no Aprendizado Semissupervisionado Multidescrição;Multi-label classification is a supervised learning problem in which an object can be associated with multiple classes. Among the different multi-label classification methods, the BR (Binary Relevance), LP (Label Powerset) and RAkEL (RAndom k-labELsets) methods stand out. The work carried out a study on the construction of multi-label classifier committees built through the application of semi-supervised multi-description learning techniques. The classifier committees used in the experiments were Bagging, Boosting and Stacking as methods of transforming the problem, we used the BR, LP and Rakel methods in multi-label semi-supervised multi-description classification, Co-Training was used, five different algorithms were applied as base classifiers: k- NN (k Nearest Neighbors), J48 (Decision Tree Induction Algorithm), SVM (Support Vector Machines), NB (Naive Bayes) and JRip (Extended Repeated Incremental Pruning). All experiments used the 10-fold Cross-Validation methodology and the MULAN framework, which is implemented using WEKA. For the sizes of the classifier committees, we adopted the values ​​3, 5, 7 and 9. To analyze the results, the Wilcoxon statistical test was used. At the end of the experimental analyses, it was found that the semi-supervised approach presented competitive results in relation to supervised learning, since the two approaches used presented statistically similar results.;;Universidade Federal Rural do Semi-Árido;pt_BR;Published;29;2017;2017-07-26 13:14:27
75300;Vinicius Montenegro Silva;Simulação Multiagente da Evacuação da Boate Kiss: A Importância da NBR 9.077 e sua relação com o Pânico;This article uses the scenario of the evacuation of a tragedy involving the Kiss nightclub in 2013 in southern Brazil. This event was marked by recklessness and neglect of safety standards, resulting in several victims. The simulations were modeled using the NetLogo tool in the Multiagent context. The parameters used in both scenarios are based on the known data of the nightclub in the event and in the 'ideal' context, respecting the indications of NBR 9.077. The environment was designed based on the layout of the Kiss nightclub. The emotion panic was modeled after its psychological definition in the literature. The results demonstrated the potential and importance of the safety standard instructions from the Brazilian Association of Technical Standards (ABNT), in guaranteeing safe evacuations with a low level of panic.;;Universidade Federal do RIo Grande - FURG;pt_BR;Published;10;2017;2017-08-04 17:12:25
75300;Marcos Vinicius Scholl;Simulação Multiagente da Evacuação da Boate Kiss: A Importância da NBR 9.077 e sua relação com o Pânico;This article uses the scenario of the evacuation of a tragedy involving the Kiss nightclub in 2013 in southern Brazil. This event was marked by recklessness and neglect of safety standards, resulting in several victims. The simulations were modeled using the NetLogo tool in the Multiagent context. The parameters used in both scenarios are based on the known data of the nightclub in the event and in the 'ideal' context, respecting the indications of NBR 9.077. The environment was designed based on the layout of the Kiss nightclub. The emotion panic was modeled after its psychological definition in the literature. The results demonstrated the potential and importance of the safety standard instructions from the Brazilian Association of Technical Standards (ABNT), in guaranteeing safe evacuations with a low level of panic.;;Centro de Ciências Computacionais - C3 - FURG;pt_BR;Published;10;2017;2017-08-04 17:12:25
75300;Diana Francisca Adamatti;Simulação Multiagente da Evacuação da Boate Kiss: A Importância da NBR 9.077 e sua relação com o Pânico;This article uses the scenario of the evacuation of a tragedy involving the Kiss nightclub in 2013 in southern Brazil. This event was marked by recklessness and neglect of safety standards, resulting in several victims. The simulations were modeled using the NetLogo tool in the Multiagent context. The parameters used in both scenarios are based on the known data of the nightclub in the event and in the 'ideal' context, respecting the indications of NBR 9.077. The environment was designed based on the layout of the Kiss nightclub. The emotion panic was modeled after its psychological definition in the literature. The results demonstrated the potential and importance of the safety standard instructions from the Brazilian Association of Technical Standards (ABNT), in guaranteeing safe evacuations with a low level of panic.;;Universidade Federal do RIo Grande - FURG;pt_BR;Published;10;2017;2017-08-04 17:12:25
75327;Diego Couto;Composição de Agentes EBDI: Integração WASABI-Jason;Emotions play a very important role in the decision-making process of human beings and it is natural that their use becomes a very fertile field in the area of ​​Artificial Intelligence, whether with the aim of bringing the behavior of computational agents closer to that of humans, or simply to improve the human-computer relationship. This work presents the integration of the WASABI emotion simulator with the BDI JASON agent framework, to provide emotional agents. The results obtained suggest that the performance of agents in the environment, as well as in human societies, is susceptible not only to their programmed behavior, but to their emotional personality and environmental conditions.;;Departamento de Informática e Estatística - INEUniversidade Federal de Santa Catarina - UFSC;pt_BR;Published;17;2017;2017-07-30 12:01:18
75327;Jerusa Marchi;Composição de Agentes EBDI: Integração WASABI-Jason;Emotions play a very important role in the decision-making process of human beings and it is natural that their use becomes a very fertile field in the area of ​​Artificial Intelligence, whether with the aim of bringing the behavior of computational agents closer to that of humans, or simply to improve the human-computer relationship. This work presents the integration of the WASABI emotion simulator with the BDI JASON agent framework, to provide emotional agents. The results obtained suggest that the performance of agents in the environment, as well as in human societies, is susceptible not only to their programmed behavior, but to their emotional personality and environmental conditions.;;Departamento de Informática e Estatística - INEPrograma de Pós-Graduação em Ciência da Computação - PPGCCUniversidade Federal de Santa Catarina - UFSC;pt_BR;Published;17;2017;2017-07-30 12:01:18
75327;Thiago Ângelo Gelaim;Composição de Agentes EBDI: Integração WASABI-Jason;Emotions play a very important role in the decision-making process of human beings and it is natural that their use becomes a very fertile field in the area of ​​Artificial Intelligence, whether with the aim of bringing the behavior of computational agents closer to that of humans, or simply to improve the human-computer relationship. This work presents the integration of the WASABI emotion simulator with the BDI JASON agent framework, to provide emotional agents. The results obtained suggest that the performance of agents in the environment, as well as in human societies, is susceptible not only to their programmed behavior, but to their emotional personality and environmental conditions.;;Programa de Pós-Graduação em Ciência da Computação - PPGCCUniversidade Federal de Santa Catarina - UFSC;pt_BR;Published;17;2017;2017-07-30 12:01:18
76387;Maíla Lima Claro;Diagnóstico de Glaucoma Utilizando Atributos de Textura e CNN’s Pré-treinadas;Glaucoma is an eye disease that damages the optic nerve causing vision loss. It is the second leading cause of blindness in the world. Several automatic glaucoma diagnosis systems have been proposed, however it is possible to make improvements in these techniques, as current systems do not deal with a large diversity of images. Therefore, this work aims to perform the automatic detection of glaucoma in retinal images, through the use of texture descriptors and Convolutional Neural Networks (CNNs). The results showed that the combination of GLCM and CNNs descriptors and the use of the Random Forest classifier are promising in detecting this pathology, obtaining an accuracy of 91.06% in 873 images from 4 public databases.;medical images, glaucoma diagnosis, texture features, transfer learning;Universidade Federal do Piauí;pt_BR;Published;7;2017;2017-09-10 19:32:08
76387;Rodrigo de Melo Souza Veras;Diagnóstico de Glaucoma Utilizando Atributos de Textura e CNN’s Pré-treinadas;Glaucoma is an eye disease that damages the optic nerve causing vision loss. It is the second leading cause of blindness in the world. Several automatic glaucoma diagnosis systems have been proposed, however it is possible to make improvements in these techniques, as current systems do not deal with a large diversity of images. Therefore, this work aims to perform the automatic detection of glaucoma in retinal images, through the use of texture descriptors and Convolutional Neural Networks (CNNs). The results showed that the combination of GLCM and CNNs descriptors and the use of the Random Forest classifier are promising in detecting this pathology, obtaining an accuracy of 91.06% in 873 images from 4 public databases.;medical images, glaucoma diagnosis, texture features, transfer learning;Universidade Federal do Piauí;pt_BR;Published;7;2017;2017-09-10 19:32:08
76387;André Macedo Santana;Diagnóstico de Glaucoma Utilizando Atributos de Textura e CNN’s Pré-treinadas;Glaucoma is an eye disease that damages the optic nerve causing vision loss. It is the second leading cause of blindness in the world. Several automatic glaucoma diagnosis systems have been proposed, however it is possible to make improvements in these techniques, as current systems do not deal with a large diversity of images. Therefore, this work aims to perform the automatic detection of glaucoma in retinal images, through the use of texture descriptors and Convolutional Neural Networks (CNNs). The results showed that the combination of GLCM and CNNs descriptors and the use of the Random Forest classifier are promising in detecting this pathology, obtaining an accuracy of 91.06% in 873 images from 4 public databases.;medical images, glaucoma diagnosis, texture features, transfer learning;Universidade Federal do Piauí;pt_BR;Published;7;2017;2017-09-10 19:32:08
76387;Luis Henrique Silva Vogado;Diagnóstico de Glaucoma Utilizando Atributos de Textura e CNN’s Pré-treinadas;Glaucoma is an eye disease that damages the optic nerve causing vision loss. It is the second leading cause of blindness in the world. Several automatic glaucoma diagnosis systems have been proposed, however it is possible to make improvements in these techniques, as current systems do not deal with a large diversity of images. Therefore, this work aims to perform the automatic detection of glaucoma in retinal images, through the use of texture descriptors and Convolutional Neural Networks (CNNs). The results showed that the combination of GLCM and CNNs descriptors and the use of the Random Forest classifier are promising in detecting this pathology, obtaining an accuracy of 91.06% in 873 images from 4 public databases.;medical images, glaucoma diagnosis, texture features, transfer learning;Universidade Federal do Piauí;pt_BR;Published;7;2017;2017-09-10 19:32:08
76387;Leonardo Pereira Sousa;Diagnóstico de Glaucoma Utilizando Atributos de Textura e CNN’s Pré-treinadas;Glaucoma is an eye disease that damages the optic nerve causing vision loss. It is the second leading cause of blindness in the world. Several automatic glaucoma diagnosis systems have been proposed, however it is possible to make improvements in these techniques, as current systems do not deal with a large diversity of images. Therefore, this work aims to perform the automatic detection of glaucoma in retinal images, through the use of texture descriptors and Convolutional Neural Networks (CNNs). The results showed that the combination of GLCM and CNNs descriptors and the use of the Random Forest classifier are promising in detecting this pathology, obtaining an accuracy of 91.06% in 873 images from 4 public databases.;medical images, glaucoma diagnosis, texture features, transfer learning;Universidade Federal do Piauí;pt_BR;Published;7;2017;2017-09-10 19:32:08
76452;Emerson Bezerra de Carvalho;A Multi-objective Version of the Lin-Kernighan Heuristic for the Traveling Salesman Problem;The Lin and Kernighan’s algorithm for the single objective Traveling Salesman Problem (TSP) is one of the most efficient heuristics for the symmetric case. Although many algorithms for the TSP were extended to the multi-objective version of the problem (MTSP), the Lin and Kernighan’s algorithm was still not fully explored. Works that applied the Lin and Kernighan’s algorithm for the MTSP were driven to weighted sum versions of the problem. We investigate the LK from a Pareto dominance perspective. The multi-objective LK was implemented within two local search schemes and applied to 2 to 4-objective instances. The results  showed that the proposed algorithmic variants obtained better results than a state-of-the-art algorithm.;multi-objective traveling salesman, multi-objective lin and kernighan, local search;Universidade Federal do Rio Grande do Norte;en_US;Published;18;2017;2017-09-13 12:01:53
76452;Elizabeth Ferreira Gouvêa Goldbarg;A Multi-objective Version of the Lin-Kernighan Heuristic for the Traveling Salesman Problem;The Lin and Kernighan’s algorithm for the single objective Traveling Salesman Problem (TSP) is one of the most efficient heuristics for the symmetric case. Although many algorithms for the TSP were extended to the multi-objective version of the problem (MTSP), the Lin and Kernighan’s algorithm was still not fully explored. Works that applied the Lin and Kernighan’s algorithm for the MTSP were driven to weighted sum versions of the problem. We investigate the LK from a Pareto dominance perspective. The multi-objective LK was implemented within two local search schemes and applied to 2 to 4-objective instances. The results  showed that the proposed algorithmic variants obtained better results than a state-of-the-art algorithm.;multi-objective traveling salesman, multi-objective lin and kernighan, local search;Departamento de Informática e Matemática Aplicada, Universidade Federal do Rio Grande do Norte;en_US;Published;18;2017;2017-09-13 12:01:53
76452;Marco Cesar Goldbarg;A Multi-objective Version of the Lin-Kernighan Heuristic for the Traveling Salesman Problem;The Lin and Kernighan’s algorithm for the single objective Traveling Salesman Problem (TSP) is one of the most efficient heuristics for the symmetric case. Although many algorithms for the TSP were extended to the multi-objective version of the problem (MTSP), the Lin and Kernighan’s algorithm was still not fully explored. Works that applied the Lin and Kernighan’s algorithm for the MTSP were driven to weighted sum versions of the problem. We investigate the LK from a Pareto dominance perspective. The multi-objective LK was implemented within two local search schemes and applied to 2 to 4-objective instances. The results  showed that the proposed algorithmic variants obtained better results than a state-of-the-art algorithm.;multi-objective traveling salesman, multi-objective lin and kernighan, local search;Universidade Federal do Rio Grande do Norte;en_US;Published;18;2017;2017-09-13 12:01:53
76460;Daniel Scheidemantel Camargo;MeHarCEn: Um Método de Harmonização do Consumo de Energia em Data Centers;Controlled and efficient energy consumption is a challenge faced daily by managers of small, medium and large Data Centers. Specialized and technical literature defines various guides, equipment and mechanisms for this purpose. However, the combined application of these solutions is a complex task, which, in many cases, requires a high financial investment. In this context, this work proposes MeHarCEn, an initiative for combined and harmonious management of energy consumption in Data Centers. MeHarCEn does not depend on proprietary solutions and can be adapted to Data Centers with different configurations. Applied to a case study, a small DC, MeHarCEn results in savings of 52.7% in energy consumption with air conditioning.;;Universidade do Estado de Santa Catarina;pt_BR;Published;23;2017;2017-09-13 15:53:11
76460;Charles Christian Miers;MeHarCEn: Um Método de Harmonização do Consumo de Energia em Data Centers;Controlled and efficient energy consumption is a challenge faced daily by managers of small, medium and large Data Centers. Specialized and technical literature defines various guides, equipment and mechanisms for this purpose. However, the combined application of these solutions is a complex task, which, in many cases, requires a high financial investment. In this context, this work proposes MeHarCEn, an initiative for combined and harmonious management of energy consumption in Data Centers. MeHarCEn does not depend on proprietary solutions and can be adapted to Data Centers with different configurations. Applied to a case study, a small DC, MeHarCEn results in savings of 52.7% in energy consumption with air conditioning.;;Universidade do Estado de Santa Catarina;pt_BR;Published;23;2017;2017-09-13 15:53:11
76460;Maurício Aronne Pillon;MeHarCEn: Um Método de Harmonização do Consumo de Energia em Data Centers;Controlled and efficient energy consumption is a challenge faced daily by managers of small, medium and large Data Centers. Specialized and technical literature defines various guides, equipment and mechanisms for this purpose. However, the combined application of these solutions is a complex task, which, in many cases, requires a high financial investment. In this context, this work proposes MeHarCEn, an initiative for combined and harmonious management of energy consumption in Data Centers. MeHarCEn does not depend on proprietary solutions and can be adapted to Data Centers with different configurations. Applied to a case study, a small DC, MeHarCEn results in savings of 52.7% in energy consumption with air conditioning.;;Universidade do Estado de Santa Catarina;pt_BR;Published;23;2017;2017-09-13 15:53:11
76460;Guilherme Piêgas Koslovski;MeHarCEn: Um Método de Harmonização do Consumo de Energia em Data Centers;Controlled and efficient energy consumption is a challenge faced daily by managers of small, medium and large Data Centers. Specialized and technical literature defines various guides, equipment and mechanisms for this purpose. However, the combined application of these solutions is a complex task, which, in many cases, requires a high financial investment. In this context, this work proposes MeHarCEn, an initiative for combined and harmonious management of energy consumption in Data Centers. MeHarCEn does not depend on proprietary solutions and can be adapted to Data Centers with different configurations. Applied to a case study, a small DC, MeHarCEn results in savings of 52.7% in energy consumption with air conditioning.;;Universidade do Estado de Santa Catarina;pt_BR;Published;23;2017;2017-09-13 15:53:11
77654;Marco Aurélio Spohn;Avaliação do Intel Software Guard Extensions via Emulação;By allowing the execution of applications in a fully protected context (i.e., within enclaves), the possibilities for new generations of Intel processors in the x86 family are expanded with the Software Guard Extensions (SGX). As it is a recent technology, machines that rely on this technology are still a minority. In order to evaluate SGX, an emulator of this technology called OpenSGX was used, which implements and reproduces the main functionalities and structures used in SGX. The approach consisted of evaluating the overhead, in terms of processing, resulting from executing an application in an environment with SGX emulated. For the evaluation, benchmark applications from the MiBench platform were used, modifying them to make execution in enclaves compatible with OpenSGX. As performance metrics, the total number of instructions and the total number of CPU cycles for the complete execution of each application with and without OpenSGX were collected.;trusted execution environment, software guard extensions, emulator;Universidade Federal da Fronteira Sul;pt_BR;Published;8;2017;2017-10-25 19:29:05
77654;Mateus Trebien;Avaliação do Intel Software Guard Extensions via Emulação;By allowing the execution of applications in a fully protected context (i.e., within enclaves), the possibilities for new generations of Intel processors in the x86 family are expanded with the Software Guard Extensions (SGX). As it is a recent technology, machines that rely on this technology are still a minority. In order to evaluate SGX, an emulator of this technology called OpenSGX was used, which implements and reproduces the main functionalities and structures used in SGX. The approach consisted of evaluating the overhead, in terms of processing, resulting from executing an application in an environment with SGX emulated. For the evaluation, benchmark applications from the MiBench platform were used, modifying them to make execution in enclaves compatible with OpenSGX. As performance metrics, the total number of instructions and the total number of CPU cycles for the complete execution of each application with and without OpenSGX were collected.;trusted execution environment, software guard extensions, emulator;Universidade Federal da Fronteira Sul;pt_BR;Published;8;2017;2017-10-25 19:29:05
77994;Rian Dutra da Cunha;Virtual Reality as a Support Tool for the Treatment of People with Intellectual and Multiple Disabilities: A Systematic Literature Review;Since the emergence of virtual reality (VR) technologies, many researchers have argued on the benefits of their use for people with intellectual and multiple disabilities. However, up to this date there is not a single study that presents a detailed overview of the state of the art in virtual reality as a support tool for the treatment of people with intellectual and multiple disabilities, as well as Autism and Down Syndrome. The aim of this study is to provide a detailed overview of the state of the art in the virtual reality area focusing on people with multiple disabilities, that encompasses intellectual and physical disabilities. There is still no consensus on the effectiveness of VR-based treatments. Virtual reality can offer rich environment and features, but most of the researches focuses only in the experience to be inside a virtual place without taking advantage of what benefits VR provide us. Furthermore, most of our selected studies used non-immersive VR and AR. Thus, immersive VR is an open field with many opportunities to be explored. We believe VR has great potential to be effective in the treatment of people with intellectual and multiple disabilities.;virtual reality, augmented reality, intellectual disability, multiple disability;Universidade Federal de Juiz de Fora;en_US;Published;14;2017;2017-11-13 18:39:17
77994;Frâncila Weidt Neiva;Virtual Reality as a Support Tool for the Treatment of People with Intellectual and Multiple Disabilities: A Systematic Literature Review;Since the emergence of virtual reality (VR) technologies, many researchers have argued on the benefits of their use for people with intellectual and multiple disabilities. However, up to this date there is not a single study that presents a detailed overview of the state of the art in virtual reality as a support tool for the treatment of people with intellectual and multiple disabilities, as well as Autism and Down Syndrome. The aim of this study is to provide a detailed overview of the state of the art in the virtual reality area focusing on people with multiple disabilities, that encompasses intellectual and physical disabilities. There is still no consensus on the effectiveness of VR-based treatments. Virtual reality can offer rich environment and features, but most of the researches focuses only in the experience to be inside a virtual place without taking advantage of what benefits VR provide us. Furthermore, most of our selected studies used non-immersive VR and AR. Thus, immersive VR is an open field with many opportunities to be explored. We believe VR has great potential to be effective in the treatment of people with intellectual and multiple disabilities.;virtual reality, augmented reality, intellectual disability, multiple disability;Universidade Federal do Rio de Janeiro;en_US;Published;14;2017;2017-11-13 18:39:17
77994;Rodrigo Luis de Souza da Silva;Virtual Reality as a Support Tool for the Treatment of People with Intellectual and Multiple Disabilities: A Systematic Literature Review;Since the emergence of virtual reality (VR) technologies, many researchers have argued on the benefits of their use for people with intellectual and multiple disabilities. However, up to this date there is not a single study that presents a detailed overview of the state of the art in virtual reality as a support tool for the treatment of people with intellectual and multiple disabilities, as well as Autism and Down Syndrome. The aim of this study is to provide a detailed overview of the state of the art in the virtual reality area focusing on people with multiple disabilities, that encompasses intellectual and physical disabilities. There is still no consensus on the effectiveness of VR-based treatments. Virtual reality can offer rich environment and features, but most of the researches focuses only in the experience to be inside a virtual place without taking advantage of what benefits VR provide us. Furthermore, most of our selected studies used non-immersive VR and AR. Thus, immersive VR is an open field with many opportunities to be explored. We believe VR has great potential to be effective in the treatment of people with intellectual and multiple disabilities.;virtual reality, augmented reality, intellectual disability, multiple disability;Universidade Federal de Juiz de Fora;en_US;Published;14;2017;2017-11-13 18:39:17
78391;Mário Andrade Vieira Melo Neto;IndoLoR -- Uma Plataforma Adaptável para Localização em Ambientes Internos;Location systems have become an increasingly integral part of people's lives. In external environments, GPS is a standard technology, widely disseminated and used. However, people tend to spend most of their time indoors, such as: hospitals, universities, factories, buildings, among others. In these environments, the GPS's functioning is compromised and it does not obtain accurate positioning. Currently, to locate people or objects indoors there is no technology that can achieve the same results obtained by GPS in outdoor environments. Because of this, it is necessary to consider the use of information from different sources using different technologies. Therefore, this work has the general objective of building an adaptable platform for localization in indoor environments. Based on this objective, the IndoLoR platform is proposed. This platform aims to allow the receipt of information from different sources, in addition to processing, merging, storing and making this information available in the context of indoor location. The proposed platform is evaluated through a case study verifying aspects related to ease of adaptation and performance.;Indoor, Environment, Location, Adaptability, IndoLoR;Instituto Federal do Rio Grande do Norte (IFRN);pt_BR;Published;18;2017;2017-11-30 13:23:52
78391;Gibeon Soares Aquino Junior;IndoLoR -- Uma Plataforma Adaptável para Localização em Ambientes Internos;Location systems have become an increasingly integral part of people's lives. In external environments, GPS is a standard technology, widely disseminated and used. However, people tend to spend most of their time indoors, such as: hospitals, universities, factories, buildings, among others. In these environments, the GPS's functioning is compromised and it does not obtain accurate positioning. Currently, to locate people or objects indoors there is no technology that can achieve the same results obtained by GPS in outdoor environments. Because of this, it is necessary to consider the use of information from different sources using different technologies. Therefore, this work has the general objective of building an adaptable platform for localization in indoor environments. Based on this objective, the IndoLoR platform is proposed. This platform aims to allow the receipt of information from different sources, in addition to processing, merging, storing and making this information available in the context of indoor location. The proposed platform is evaluated through a case study verifying aspects related to ease of adaptation and performance.;Indoor, Environment, Location, Adaptability, IndoLoR;Universidade Federal do Rio Grande do Norte (UFRN);pt_BR;Published;18;2017;2017-11-30 13:23:52
79152;Charles Tim Batista Garrocho;CoWPar: A D2D Communication Approach Without Pairing for Mobile Social Network in Proximity;Mobile devices and the cellular network have been popularized and evolving in recent years. The increase of these devices may promote Device-to-Device (D2D) communications. However, current D2D communication technologies such as Wi-Fi Direct, Wi-Fi Ad Hoc, and Bluetooth are not available on devices or require human interaction in the pairing process. In addition, the cellular network is not available in many places and has partial or total communication infrastructure failures. To overcome this failure and lack of connectivity, and to allow D2D communication between devices in disturbing scenarios, we presented CoWPar. Based on the Wi-Fi infrastructure mode, CoWPar establishes the connection and performs data exchange without human interaction between the devices. Results of experiments performed in a proof of concept showed in practice that CoWPar allowed D2D communications with no pairing and also without the need to change the operating system (OS) of the devices, surpassing all the works available so far and thus contributing to the process of viabilization the paradigm of Pervasive Computing.;Device-to-Device Communication, Wi-Fi Tethering, Mobile Social Network, Pervasive Computing;Instituto Federal de Minas Gerais, Ponte Nova, Minas Gerais, Brasil;pt_BR;Published;11;2017;2017-12-23 17:25:30
79152;Jessé Pires Barbosa Rocha;CoWPar: A D2D Communication Approach Without Pairing for Mobile Social Network in Proximity;Mobile devices and the cellular network have been popularized and evolving in recent years. The increase of these devices may promote Device-to-Device (D2D) communications. However, current D2D communication technologies such as Wi-Fi Direct, Wi-Fi Ad Hoc, and Bluetooth are not available on devices or require human interaction in the pairing process. In addition, the cellular network is not available in many places and has partial or total communication infrastructure failures. To overcome this failure and lack of connectivity, and to allow D2D communication between devices in disturbing scenarios, we presented CoWPar. Based on the Wi-Fi infrastructure mode, CoWPar establishes the connection and performs data exchange without human interaction between the devices. Results of experiments performed in a proof of concept showed in practice that CoWPar allowed D2D communications with no pairing and also without the need to change the operating system (OS) of the devices, surpassing all the works available so far and thus contributing to the process of viabilization the paradigm of Pervasive Computing.;Device-to-Device Communication, Wi-Fi Tethering, Mobile Social Network, Pervasive Computing;Instituto Federal do Paraná, Goioerê, Paraná, Brasil;pt_BR;Published;11;2017;2017-12-23 17:25:30
79152;José Eduardo de Souza;CoWPar: A D2D Communication Approach Without Pairing for Mobile Social Network in Proximity;Mobile devices and the cellular network have been popularized and evolving in recent years. The increase of these devices may promote Device-to-Device (D2D) communications. However, current D2D communication technologies such as Wi-Fi Direct, Wi-Fi Ad Hoc, and Bluetooth are not available on devices or require human interaction in the pairing process. In addition, the cellular network is not available in many places and has partial or total communication infrastructure failures. To overcome this failure and lack of connectivity, and to allow D2D communication between devices in disturbing scenarios, we presented CoWPar. Based on the Wi-Fi infrastructure mode, CoWPar establishes the connection and performs data exchange without human interaction between the devices. Results of experiments performed in a proof of concept showed in practice that CoWPar allowed D2D communications with no pairing and also without the need to change the operating system (OS) of the devices, surpassing all the works available so far and thus contributing to the process of viabilization the paradigm of Pervasive Computing.;Device-to-Device Communication, Wi-Fi Tethering, Mobile Social Network, Pervasive Computing;Instituto Federal do Paraná, Goioerê, Paraná, Brasil;pt_BR;Published;11;2017;2017-12-23 17:25:30
79158;Matheus D'Eça Torquato;Modelos para avaliação de disponibilidade orientada a capacidade de uma nuvem privada;High availability is one of the main requirements of applications that use cloud computing. It is possible to apply redundancies in hardware and software to achieve better levels of system availability. However, in addition to concerns about service availability, it is necessary to measure the system's ability to handle the workload presented. A metric that can be used to measure this capacity is capacity-oriented availability. Using this metric, it is possible to obtain estimates of the computational resources available for use when the system is running. This work presents a set of analytical models for capacity-oriented availability assessment considering private cloud environments. To verify different situations, this work presents six different private cloud architectures. The fundamental components of each architecture are Front-End, PM and VMs. The set of results presented comprises availability assessment, capacity-oriented availability assessment and sensitivity analysis of the parameters used in the models. From the results it is possible to infer which components are most important for each of the metrics studied.;Computer Science;Instituto Federal de Alagoas (IFAL), Campus Arapiraca;pt_BR;Published;11;2017;2017-12-24 1:02:53
79158;Lucas Torquato;Modelos para avaliação de disponibilidade orientada a capacidade de uma nuvem privada;High availability is one of the main requirements of applications that use cloud computing. It is possible to apply redundancies in hardware and software to achieve better levels of system availability. However, in addition to concerns about service availability, it is necessary to measure the system's ability to handle the workload presented. A metric that can be used to measure this capacity is capacity-oriented availability. Using this metric, it is possible to obtain estimates of the computational resources available for use when the system is running. This work presents a set of analytical models for capacity-oriented availability assessment considering private cloud environments. To verify different situations, this work presents six different private cloud architectures. The fundamental components of each architecture are Front-End, PM and VMs. The set of results presented comprises availability assessment, capacity-oriented availability assessment and sensitivity analysis of the parameters used in the models. From the results it is possible to infer which components are most important for each of the metrics studied.;Computer Science;-;pt_BR;Published;11;2017;2017-12-24 1:02:53
79158;Paulo Maciel;Modelos para avaliação de disponibilidade orientada a capacidade de uma nuvem privada;High availability is one of the main requirements of applications that use cloud computing. It is possible to apply redundancies in hardware and software to achieve better levels of system availability. However, in addition to concerns about service availability, it is necessary to measure the system's ability to handle the workload presented. A metric that can be used to measure this capacity is capacity-oriented availability. Using this metric, it is possible to obtain estimates of the computational resources available for use when the system is running. This work presents a set of analytical models for capacity-oriented availability assessment considering private cloud environments. To verify different situations, this work presents six different private cloud architectures. The fundamental components of each architecture are Front-End, PM and VMs. The set of results presented comprises availability assessment, capacity-oriented availability assessment and sensitivity analysis of the parameters used in the models. From the results it is possible to infer which components are most important for each of the metrics studied.;Computer Science;-;pt_BR;Published;11;2017;2017-12-24 1:02:53
79310;Francisco Oliveira;Video Conferencing Evaluation Considering Scalable Video Coding and SDN Network;Video conferencing is very common nowadays, and it may contemplate heterogenous devices (e.g., smartphones, notebooks, game consoles) and networks in the same session. Developing video conferencing systems for this myriad of devices with different capabilities requires special attention from system designer. Scalable video coding (SVC) is a prominent option to mitigate this heterogeneity issue, but traditional Internet protocol (IP) networks may not fully benefit from such a technology. In contrast, software-defined networking (SDN) may allow better utilization of SVC and improvements on video conferencing components. This paper evaluates the performance of video conferencing systems adopting SVC, SDN and ordinary IP networks, taking into account throughput, delay and peak signal-to-noise ratio (PSNR) as the metrics of interest. The experiments are based on Mininet framework and distinct network infrastructures are also considered. Results indicate SDN with SVC may deliver better video quality with reduced delay and increased throughput.;Availability, Capacity Oriented Availability, Cloud Computing, Analytical modeling;Universidade Federal de Pernambuco;en_US;Published;8;2018;2018-01-01 19:19:47
79310;Eduardo Tavares;Video Conferencing Evaluation Considering Scalable Video Coding and SDN Network;Video conferencing is very common nowadays, and it may contemplate heterogenous devices (e.g., smartphones, notebooks, game consoles) and networks in the same session. Developing video conferencing systems for this myriad of devices with different capabilities requires special attention from system designer. Scalable video coding (SVC) is a prominent option to mitigate this heterogeneity issue, but traditional Internet protocol (IP) networks may not fully benefit from such a technology. In contrast, software-defined networking (SDN) may allow better utilization of SVC and improvements on video conferencing components. This paper evaluates the performance of video conferencing systems adopting SVC, SDN and ordinary IP networks, taking into account throughput, delay and peak signal-to-noise ratio (PSNR) as the metrics of interest. The experiments are based on Mininet framework and distinct network infrastructures are also considered. Results indicate SDN with SVC may deliver better video quality with reduced delay and increased throughput.;Availability, Capacity Oriented Availability, Cloud Computing, Analytical modeling;Universidade Federal de Pernambuco;en_US;Published;8;2018;2018-01-01 19:19:47
79310;Erica Sousa;Video Conferencing Evaluation Considering Scalable Video Coding and SDN Network;Video conferencing is very common nowadays, and it may contemplate heterogenous devices (e.g., smartphones, notebooks, game consoles) and networks in the same session. Developing video conferencing systems for this myriad of devices with different capabilities requires special attention from system designer. Scalable video coding (SVC) is a prominent option to mitigate this heterogeneity issue, but traditional Internet protocol (IP) networks may not fully benefit from such a technology. In contrast, software-defined networking (SDN) may allow better utilization of SVC and improvements on video conferencing components. This paper evaluates the performance of video conferencing systems adopting SVC, SDN and ordinary IP networks, taking into account throughput, delay and peak signal-to-noise ratio (PSNR) as the metrics of interest. The experiments are based on Mininet framework and distinct network infrastructures are also considered. Results indicate SDN with SVC may deliver better video quality with reduced delay and increased throughput.;Availability, Capacity Oriented Availability, Cloud Computing, Analytical modeling;Universidade Federal Rural de Pernambuco;en_US;Published;8;2018;2018-01-01 19:19:47
79310;Bruno Nogueira;Video Conferencing Evaluation Considering Scalable Video Coding and SDN Network;Video conferencing is very common nowadays, and it may contemplate heterogenous devices (e.g., smartphones, notebooks, game consoles) and networks in the same session. Developing video conferencing systems for this myriad of devices with different capabilities requires special attention from system designer. Scalable video coding (SVC) is a prominent option to mitigate this heterogeneity issue, but traditional Internet protocol (IP) networks may not fully benefit from such a technology. In contrast, software-defined networking (SDN) may allow better utilization of SVC and improvements on video conferencing components. This paper evaluates the performance of video conferencing systems adopting SVC, SDN and ordinary IP networks, taking into account throughput, delay and peak signal-to-noise ratio (PSNR) as the metrics of interest. The experiments are based on Mininet framework and distinct network infrastructures are also considered. Results indicate SDN with SVC may deliver better video quality with reduced delay and increased throughput.;Availability, Capacity Oriented Availability, Cloud Computing, Analytical modeling;Universidade Federal Rural de Pernambuco;en_US;Published;8;2018;2018-01-01 19:19:47
79333;Igor Magalhães Ribeiro;A Genetic Programming Model for Association Studies to Detect Epistasis in Low Heritability Data;The genome-wide associations studies (GWAS) aims to identify the most influential markers in relation to the phenotype values. One of the substantial challenges is to find a non-linear mapping between genotype and phenotype, also known as epistasis, that usually becomes the process of searching and identifying functional SNPs more complex. Some diseases such as cervical cancer, leukemia and type 2 diabetes have low heritability. The heritability of the sample is directly related to the explanation defined by the genotype, so the lower the heritability the greater the influence of the environmental factors and the less the genotypic explanation. In this work, an algorithm capable of identifying epistatic associations at different levels of heritability is proposed. The developing model is a aplication of genetic programming with a specialized initialization for the initial population consisting of a random forest strategy. The initialization process aims to rank the most important SNPs increasing the probability of their insertion in the initial population of the genetic programming model. The expected behavior of the presented model for the obtainment of the causal markers intends to be robust in relation to the heritability level. The simulated experiments are case-control type with heritability level of 0.4, 0.3, 0.2 and 0.1 considering scenarios with 100 and 1000 markers. Our approach was compared with the GPAS software and a genetic programming algorithm without the initialization step. The results show that the use of an efficient population initialization method based on ranking strategy is very promising compared to other models.;Bioinformatics, GWAS, SNP, Genetic Programming, Random Forest, Computational Modeling, Mathematical Modeling;Universidade Federal de Juiz de Fora (UFJF);en_US;Published;7;2018;2018-01-02 19:23:50
79333;Carlos Cristiano Hasenclever Borges;A Genetic Programming Model for Association Studies to Detect Epistasis in Low Heritability Data;The genome-wide associations studies (GWAS) aims to identify the most influential markers in relation to the phenotype values. One of the substantial challenges is to find a non-linear mapping between genotype and phenotype, also known as epistasis, that usually becomes the process of searching and identifying functional SNPs more complex. Some diseases such as cervical cancer, leukemia and type 2 diabetes have low heritability. The heritability of the sample is directly related to the explanation defined by the genotype, so the lower the heritability the greater the influence of the environmental factors and the less the genotypic explanation. In this work, an algorithm capable of identifying epistatic associations at different levels of heritability is proposed. The developing model is a aplication of genetic programming with a specialized initialization for the initial population consisting of a random forest strategy. The initialization process aims to rank the most important SNPs increasing the probability of their insertion in the initial population of the genetic programming model. The expected behavior of the presented model for the obtainment of the causal markers intends to be robust in relation to the heritability level. The simulated experiments are case-control type with heritability level of 0.4, 0.3, 0.2 and 0.1 considering scenarios with 100 and 1000 markers. Our approach was compared with the GPAS software and a genetic programming algorithm without the initialization step. The results show that the use of an efficient population initialization method based on ranking strategy is very promising compared to other models.;Bioinformatics, GWAS, SNP, Genetic Programming, Random Forest, Computational Modeling, Mathematical Modeling;Universidade Federal de Juiz de Fora (UFJF);en_US;Published;7;2018;2018-01-02 19:23:50
79333;Bruno Zonovelli Silva;A Genetic Programming Model for Association Studies to Detect Epistasis in Low Heritability Data;The genome-wide associations studies (GWAS) aims to identify the most influential markers in relation to the phenotype values. One of the substantial challenges is to find a non-linear mapping between genotype and phenotype, also known as epistasis, that usually becomes the process of searching and identifying functional SNPs more complex. Some diseases such as cervical cancer, leukemia and type 2 diabetes have low heritability. The heritability of the sample is directly related to the explanation defined by the genotype, so the lower the heritability the greater the influence of the environmental factors and the less the genotypic explanation. In this work, an algorithm capable of identifying epistatic associations at different levels of heritability is proposed. The developing model is a aplication of genetic programming with a specialized initialization for the initial population consisting of a random forest strategy. The initialization process aims to rank the most important SNPs increasing the probability of their insertion in the initial population of the genetic programming model. The expected behavior of the presented model for the obtainment of the causal markers intends to be robust in relation to the heritability level. The simulated experiments are case-control type with heritability level of 0.4, 0.3, 0.2 and 0.1 considering scenarios with 100 and 1000 markers. Our approach was compared with the GPAS software and a genetic programming algorithm without the initialization step. The results show that the use of an efficient population initialization method based on ranking strategy is very promising compared to other models.;Bioinformatics, GWAS, SNP, Genetic Programming, Random Forest, Computational Modeling, Mathematical Modeling;Universidade Federal de Juiz de Fora (UFJF);en_US;Published;7;2018;2018-01-02 19:23:50
79333;Wagner Arbex;A Genetic Programming Model for Association Studies to Detect Epistasis in Low Heritability Data;The genome-wide associations studies (GWAS) aims to identify the most influential markers in relation to the phenotype values. One of the substantial challenges is to find a non-linear mapping between genotype and phenotype, also known as epistasis, that usually becomes the process of searching and identifying functional SNPs more complex. Some diseases such as cervical cancer, leukemia and type 2 diabetes have low heritability. The heritability of the sample is directly related to the explanation defined by the genotype, so the lower the heritability the greater the influence of the environmental factors and the less the genotypic explanation. In this work, an algorithm capable of identifying epistatic associations at different levels of heritability is proposed. The developing model is a aplication of genetic programming with a specialized initialization for the initial population consisting of a random forest strategy. The initialization process aims to rank the most important SNPs increasing the probability of their insertion in the initial population of the genetic programming model. The expected behavior of the presented model for the obtainment of the causal markers intends to be robust in relation to the heritability level. The simulated experiments are case-control type with heritability level of 0.4, 0.3, 0.2 and 0.1 considering scenarios with 100 and 1000 markers. Our approach was compared with the GPAS software and a genetic programming algorithm without the initialization step. The results show that the use of an efficient population initialization method based on ranking strategy is very promising compared to other models.;Bioinformatics, GWAS, SNP, Genetic Programming, Random Forest, Computational Modeling, Mathematical Modeling;Universidade Federal de Juiz de Fora (UFJF) e Empresa Brasileira de Pesquisa Agropecuária (Embrapa);en_US;Published;7;2018;2018-01-02 19:23:50
79334;Arthur Lorenzi Almeida;Relative Scalability of NoSQL Databases for Genotype Data Manipulation;Genotype data manipulation is one of the greatest challenges in bioinformatics and genomics mainly because of high dimensionality and unbalancing characteristics. These peculiarities explains why Relational Database Management Systems (RDBMSs), the "de facto" standard storage solution, have not been presented as the best tools for this kind of data. However, Big Data has been pushing the development of modern database systems that might be able to overcome RDBMSs deficiencies. In this context, we extended our previous works on the evaluation of relative performance among NoSQLs engines from different families, adapting the schema design in order to achieve better performance based on its conclusions, thus being able to store more SNP markers for each individual. Using Yahoo! Cloud Serving Benchmark (YCSB) benchmark framework, we assessed each database system over hypothetical SNP sequences. Results indicate that although Tarantool has the best overall throughput, MongoDB is less impacted by the increase of SNP markers per individual.;Database, NoSQL, Bionformatics, Data Science, SNP, Genotype;Departamento de Ciência da Computação da Universidade Federal de Juiz de Fora (DCC/UFJF).;pt_BR;Published;7;2018;2018-01-02 22:58:40
79334;Vinícius Junqueira Schettino;Relative Scalability of NoSQL Databases for Genotype Data Manipulation;Genotype data manipulation is one of the greatest challenges in bioinformatics and genomics mainly because of high dimensionality and unbalancing characteristics. These peculiarities explains why Relational Database Management Systems (RDBMSs), the "de facto" standard storage solution, have not been presented as the best tools for this kind of data. However, Big Data has been pushing the development of modern database systems that might be able to overcome RDBMSs deficiencies. In this context, we extended our previous works on the evaluation of relative performance among NoSQLs engines from different families, adapting the schema design in order to achieve better performance based on its conclusions, thus being able to store more SNP markers for each individual. Using Yahoo! Cloud Serving Benchmark (YCSB) benchmark framework, we assessed each database system over hypothetical SNP sequences. Results indicate that although Tarantool has the best overall throughput, MongoDB is less impacted by the increase of SNP markers per individual.;Database, NoSQL, Bionformatics, Data Science, SNP, Genotype;Programa de Pós-graduação em Ciência da Computação da Universidade Federal de Juiz de Fora (PPGCC/UFJF).;pt_BR;Published;7;2018;2018-01-02 22:58:40
79334;Thiago Jesus Rodrigues Barbosa;Relative Scalability of NoSQL Databases for Genotype Data Manipulation;Genotype data manipulation is one of the greatest challenges in bioinformatics and genomics mainly because of high dimensionality and unbalancing characteristics. These peculiarities explains why Relational Database Management Systems (RDBMSs), the "de facto" standard storage solution, have not been presented as the best tools for this kind of data. However, Big Data has been pushing the development of modern database systems that might be able to overcome RDBMSs deficiencies. In this context, we extended our previous works on the evaluation of relative performance among NoSQLs engines from different families, adapting the schema design in order to achieve better performance based on its conclusions, thus being able to store more SNP markers for each individual. Using Yahoo! Cloud Serving Benchmark (YCSB) benchmark framework, we assessed each database system over hypothetical SNP sequences. Results indicate that although Tarantool has the best overall throughput, MongoDB is less impacted by the increase of SNP markers per individual.;Database, NoSQL, Bionformatics, Data Science, SNP, Genotype;Programa de Pós-graduação em Ciência da Computação da Universidade Federal de Juiz de Fora (PPGCC/UFJF);pt_BR;Published;7;2018;2018-01-02 22:58:40
79334;Pedro Fernandes Freitas;Relative Scalability of NoSQL Databases for Genotype Data Manipulation;Genotype data manipulation is one of the greatest challenges in bioinformatics and genomics mainly because of high dimensionality and unbalancing characteristics. These peculiarities explains why Relational Database Management Systems (RDBMSs), the "de facto" standard storage solution, have not been presented as the best tools for this kind of data. However, Big Data has been pushing the development of modern database systems that might be able to overcome RDBMSs deficiencies. In this context, we extended our previous works on the evaluation of relative performance among NoSQLs engines from different families, adapting the schema design in order to achieve better performance based on its conclusions, thus being able to store more SNP markers for each individual. Using Yahoo! Cloud Serving Benchmark (YCSB) benchmark framework, we assessed each database system over hypothetical SNP sequences. Results indicate that although Tarantool has the best overall throughput, MongoDB is less impacted by the increase of SNP markers per individual.;Database, NoSQL, Bionformatics, Data Science, SNP, Genotype;Departamento de Ciência da Computação da Universidade Federal de Juiz de Fora (DCC/UFJF).;pt_BR;Published;7;2018;2018-01-02 22:58:40
79334;Pedro Gabriel Silva Guimarães;Relative Scalability of NoSQL Databases for Genotype Data Manipulation;Genotype data manipulation is one of the greatest challenges in bioinformatics and genomics mainly because of high dimensionality and unbalancing characteristics. These peculiarities explains why Relational Database Management Systems (RDBMSs), the "de facto" standard storage solution, have not been presented as the best tools for this kind of data. However, Big Data has been pushing the development of modern database systems that might be able to overcome RDBMSs deficiencies. In this context, we extended our previous works on the evaluation of relative performance among NoSQLs engines from different families, adapting the schema design in order to achieve better performance based on its conclusions, thus being able to store more SNP markers for each individual. Using Yahoo! Cloud Serving Benchmark (YCSB) benchmark framework, we assessed each database system over hypothetical SNP sequences. Results indicate that although Tarantool has the best overall throughput, MongoDB is less impacted by the increase of SNP markers per individual.;Database, NoSQL, Bionformatics, Data Science, SNP, Genotype;Programa de Pós-graduação em Ciência da Computação da Universidade Federal de Juiz de Fora (PPGCC/UFJF).;pt_BR;Published;7;2018;2018-01-02 22:58:40
79334;Wagner Arbex;Relative Scalability of NoSQL Databases for Genotype Data Manipulation;Genotype data manipulation is one of the greatest challenges in bioinformatics and genomics mainly because of high dimensionality and unbalancing characteristics. These peculiarities explains why Relational Database Management Systems (RDBMSs), the "de facto" standard storage solution, have not been presented as the best tools for this kind of data. However, Big Data has been pushing the development of modern database systems that might be able to overcome RDBMSs deficiencies. In this context, we extended our previous works on the evaluation of relative performance among NoSQLs engines from different families, adapting the schema design in order to achieve better performance based on its conclusions, thus being able to store more SNP markers for each individual. Using Yahoo! Cloud Serving Benchmark (YCSB) benchmark framework, we assessed each database system over hypothetical SNP sequences. Results indicate that although Tarantool has the best overall throughput, MongoDB is less impacted by the increase of SNP markers per individual.;Database, NoSQL, Bionformatics, Data Science, SNP, Genotype;Empresa Brasileira de Pesquisa Agropecuária (Embrapa) e Universidade Federal de Juiz de Fora (UFJF).;pt_BR;Published;7;2018;2018-01-02 22:58:40
79885;Luís Eduardo Ramos de Carvalho;Can the Use of nonlinear Color Metrics systematically improve Segmentation?;Image segmentation is a procedure where an image is split into its constituent parts, according to some criterion. In the literature, there are different well-known approaches for segmentation, such as clustering, thresholding, graph theory and region growing. Such approaches, additionally, can be combined with color distance metrics, playing an important role for color similarity computation. Aiming to investigate general approaches able to enhance the performance of segmentation methods, this work presents an empirical study of the effect of a nonlinear color metric on segmentation procedures. For this purpose, three algorithms were  chosen: Mumford-Shah, Color Structure Code and Felzenszwalb and Huttenlocher Segmentation. The color similarity metric employed by these algorithms (L2-norm) was replaced by the Polynomial Mahalanobis Distance. This metric is an extension of the statistical Mahalanobis Distance used to measure the distance between coordinates and distribution centers. An evaluation based upon automated comparison of segmentation results against ground truths from the Berkeley Dataset was performed. All three segmentation approaches were compared to their traditional implementations, against each other and also to a large set of other segmentation methods. The statistical analysis performed has indicated a systematic improvement of segmentation results for all three segmentation approaches when the nonlinear metric was employed.;Image Segmentation, Nonlinear Color Metrics, Polynomial Mahalanobis Distance, Split and Merge, Variational Methods, Graph-Based Segmentation.;Graduate Program in Computer Science - Federal University of Santa Catarina, Florianópolis, Santa Catarina, Brazil.National Brazilian Institute for Digital Convergence - Federal University of Santa Catarina,  Florianópolis, Santa Catarina, Brazil.;en_US;Published;15;2018;2018-01-23 15:58:36
79885;Sylvio Luiz Mantelli Neto;Can the Use of nonlinear Color Metrics systematically improve Segmentation?;Image segmentation is a procedure where an image is split into its constituent parts, according to some criterion. In the literature, there are different well-known approaches for segmentation, such as clustering, thresholding, graph theory and region growing. Such approaches, additionally, can be combined with color distance metrics, playing an important role for color similarity computation. Aiming to investigate general approaches able to enhance the performance of segmentation methods, this work presents an empirical study of the effect of a nonlinear color metric on segmentation procedures. For this purpose, three algorithms were  chosen: Mumford-Shah, Color Structure Code and Felzenszwalb and Huttenlocher Segmentation. The color similarity metric employed by these algorithms (L2-norm) was replaced by the Polynomial Mahalanobis Distance. This metric is an extension of the statistical Mahalanobis Distance used to measure the distance between coordinates and distribution centers. An evaluation based upon automated comparison of segmentation results against ground truths from the Berkeley Dataset was performed. All three segmentation approaches were compared to their traditional implementations, against each other and also to a large set of other segmentation methods. The statistical analysis performed has indicated a systematic improvement of segmentation results for all three segmentation approaches when the nonlinear metric was employed.;Image Segmentation, Nonlinear Color Metrics, Polynomial Mahalanobis Distance, Split and Merge, Variational Methods, Graph-Based Segmentation.;Brazilian Institute for Space Research - INPE,   São José dos Campos, São Paulo, Brazil.National Brazilian Institute for Digital Convergence - Federal University of Santa Catarina,  Florianópolis, Santa Catarina, Brazil.;en_US;Published;15;2018;2018-01-23 15:58:36
79885;Eros Comunello;Can the Use of nonlinear Color Metrics systematically improve Segmentation?;Image segmentation is a procedure where an image is split into its constituent parts, according to some criterion. In the literature, there are different well-known approaches for segmentation, such as clustering, thresholding, graph theory and region growing. Such approaches, additionally, can be combined with color distance metrics, playing an important role for color similarity computation. Aiming to investigate general approaches able to enhance the performance of segmentation methods, this work presents an empirical study of the effect of a nonlinear color metric on segmentation procedures. For this purpose, three algorithms were  chosen: Mumford-Shah, Color Structure Code and Felzenszwalb and Huttenlocher Segmentation. The color similarity metric employed by these algorithms (L2-norm) was replaced by the Polynomial Mahalanobis Distance. This metric is an extension of the statistical Mahalanobis Distance used to measure the distance between coordinates and distribution centers. An evaluation based upon automated comparison of segmentation results against ground truths from the Berkeley Dataset was performed. All three segmentation approaches were compared to their traditional implementations, against each other and also to a large set of other segmentation methods. The statistical analysis performed has indicated a systematic improvement of segmentation results for all three segmentation approaches when the nonlinear metric was employed.;Image Segmentation, Nonlinear Color Metrics, Polynomial Mahalanobis Distance, Split and Merge, Variational Methods, Graph-Based Segmentation.;University of Itajai Valley - Univali, Itajaí, Santa Catarina, Brazil.National Brazilian Institute for Digital Convergence - Federal University of Santa Catarina,  Florianópolis, Santa Catarina, Brazil.;en_US;Published;15;2018;2018-01-23 15:58:36
79885;Antonio Carlos Sobieranski;Can the Use of nonlinear Color Metrics systematically improve Segmentation?;Image segmentation is a procedure where an image is split into its constituent parts, according to some criterion. In the literature, there are different well-known approaches for segmentation, such as clustering, thresholding, graph theory and region growing. Such approaches, additionally, can be combined with color distance metrics, playing an important role for color similarity computation. Aiming to investigate general approaches able to enhance the performance of segmentation methods, this work presents an empirical study of the effect of a nonlinear color metric on segmentation procedures. For this purpose, three algorithms were  chosen: Mumford-Shah, Color Structure Code and Felzenszwalb and Huttenlocher Segmentation. The color similarity metric employed by these algorithms (L2-norm) was replaced by the Polynomial Mahalanobis Distance. This metric is an extension of the statistical Mahalanobis Distance used to measure the distance between coordinates and distribution centers. An evaluation based upon automated comparison of segmentation results against ground truths from the Berkeley Dataset was performed. All three segmentation approaches were compared to their traditional implementations, against each other and also to a large set of other segmentation methods. The statistical analysis performed has indicated a systematic improvement of segmentation results for all three segmentation approaches when the nonlinear metric was employed.;Image Segmentation, Nonlinear Color Metrics, Polynomial Mahalanobis Distance, Split and Merge, Variational Methods, Graph-Based Segmentation.;Department of Computing - Federal University of Santa Catarina, Ararangua, Brazil.National Brazilian Institute for Digital Convergence - Federal University of Santa Catarina,  Florianópolis, Santa Catarina, Brazil.;en_US;Published;15;2018;2018-01-23 15:58:36
79885;Aldo von Wangenheim;Can the Use of nonlinear Color Metrics systematically improve Segmentation?;Image segmentation is a procedure where an image is split into its constituent parts, according to some criterion. In the literature, there are different well-known approaches for segmentation, such as clustering, thresholding, graph theory and region growing. Such approaches, additionally, can be combined with color distance metrics, playing an important role for color similarity computation. Aiming to investigate general approaches able to enhance the performance of segmentation methods, this work presents an empirical study of the effect of a nonlinear color metric on segmentation procedures. For this purpose, three algorithms were  chosen: Mumford-Shah, Color Structure Code and Felzenszwalb and Huttenlocher Segmentation. The color similarity metric employed by these algorithms (L2-norm) was replaced by the Polynomial Mahalanobis Distance. This metric is an extension of the statistical Mahalanobis Distance used to measure the distance between coordinates and distribution centers. An evaluation based upon automated comparison of segmentation results against ground truths from the Berkeley Dataset was performed. All three segmentation approaches were compared to their traditional implementations, against each other and also to a large set of other segmentation methods. The statistical analysis performed has indicated a systematic improvement of segmentation results for all three segmentation approaches when the nonlinear metric was employed.;Image Segmentation, Nonlinear Color Metrics, Polynomial Mahalanobis Distance, Split and Merge, Variational Methods, Graph-Based Segmentation.;Graduate Program in Computer Science - Federal University of Santa Catarina, Florianópolis, Santa Catarina, Brazil.National Brazilian Institute for Digital Convergence - Federal University of Santa Catarina,  Florianópolis, Santa Catarina, Brazil.;en_US;Published;15;2018;2018-01-23 15:58:36
79994;Francisca Aparecida Prado Pinto;Algoritmos de Escalonamento Gangue com Estratégias de Migracão em um Ambiente MCMCA;Over the last decade, the rapid advancement of networking, hardware and middleware technologies, as well as the sophistication of software resources, have contributed to the emergence of new computational models. Consequently, there has been a growing capacity for the efficient and effective use of distributed resources in order to integrate them, in order to provide a widely distributed environment, whose computational capacity can be used to solve complex problems. The two most challenging aspects of distributed systems are resource management and task scheduling. This work contributes to minimizing such problems: (i) through the use of migration techniques (ii) the implementation of a multicore multicluster simulation environment with a load balancing mechanism, in order to analyze the system in different contexts (iii) implementation and analysis of gang escalators through metrics to measure performance in different situations. Thus, the results showed a better use of resources, resulting in a reduction in operational costs;Distributed systems, Parallel jobs, Migration techniques, Gang scheduling;Universidade Estadual do Rio Grande do Norte e a Universidade Federal Rural do Semi-árido.;pt_BR;Published;16;2018;2018-01-30 18:41:17
79994;Henrique Jorge Amorim Holanda;Algoritmos de Escalonamento Gangue com Estratégias de Migracão em um Ambiente MCMCA;Over the last decade, the rapid advancement of networking, hardware and middleware technologies, as well as the sophistication of software resources, have contributed to the emergence of new computational models. Consequently, there has been a growing capacity for the efficient and effective use of distributed resources in order to integrate them, in order to provide a widely distributed environment, whose computational capacity can be used to solve complex problems. The two most challenging aspects of distributed systems are resource management and task scheduling. This work contributes to minimizing such problems: (i) through the use of migration techniques (ii) the implementation of a multicore multicluster simulation environment with a load balancing mechanism, in order to analyze the system in different contexts (iii) implementation and analysis of gang escalators through metrics to measure performance in different situations. Thus, the results showed a better use of resources, resulting in a reduction in operational costs;Distributed systems, Parallel jobs, Migration techniques, Gang scheduling;Universidade Estadual do Rio Grande do Norte;pt_BR;Published;16;2018;2018-01-30 18:41:17
79994;Giovanni Cordeiro Barroso;Algoritmos de Escalonamento Gangue com Estratégias de Migracão em um Ambiente MCMCA;Over the last decade, the rapid advancement of networking, hardware and middleware technologies, as well as the sophistication of software resources, have contributed to the emergence of new computational models. Consequently, there has been a growing capacity for the efficient and effective use of distributed resources in order to integrate them, in order to provide a widely distributed environment, whose computational capacity can be used to solve complex problems. The two most challenging aspects of distributed systems are resource management and task scheduling. This work contributes to minimizing such problems: (i) through the use of migration techniques (ii) the implementation of a multicore multicluster simulation environment with a load balancing mechanism, in order to analyze the system in different contexts (iii) implementation and analysis of gang escalators through metrics to measure performance in different situations. Thus, the results showed a better use of resources, resulting in a reduction in operational costs;Distributed systems, Parallel jobs, Migration techniques, Gang scheduling;Universidade Federal do Ceará;pt_BR;Published;16;2018;2018-01-30 18:41:17
79994;Carla Katarina Marques;Algoritmos de Escalonamento Gangue com Estratégias de Migracão em um Ambiente MCMCA;Over the last decade, the rapid advancement of networking, hardware and middleware technologies, as well as the sophistication of software resources, have contributed to the emergence of new computational models. Consequently, there has been a growing capacity for the efficient and effective use of distributed resources in order to integrate them, in order to provide a widely distributed environment, whose computational capacity can be used to solve complex problems. The two most challenging aspects of distributed systems are resource management and task scheduling. This work contributes to minimizing such problems: (i) through the use of migration techniques (ii) the implementation of a multicore multicluster simulation environment with a load balancing mechanism, in order to analyze the system in different contexts (iii) implementation and analysis of gang escalators through metrics to measure performance in different situations. Thus, the results showed a better use of resources, resulting in a reduction in operational costs;Distributed systems, Parallel jobs, Migration techniques, Gang scheduling;Universidade Estadual Rio Grande do Norte;pt_BR;Published;16;2018;2018-01-30 18:41:17
80478;Hudson Geovane de Medeiros;Investigation of Archiving Techniques for Evolutionary Multi-objective Optimizers;Abstract: The optimization of multi-objective problems from the Pareto dominance viewpoint can lead to huge sets of incomparable solutions. Many heuristic techniques proposed to these problems have to deal with approximation sets that can be limited or not. Usually, a new solution generated by a heuristic is compared with other archived non-dominated solutions generated previously. Many techniques deal with limited size archives, since comparisons within unlimited archives may require significant computational effort. To maintain limited archives, solutions need to be discarded. Several techniques were proposed to deal with the problem of deciding which solutions remain in the archive and which are discarded. Previous investigations showed that those techniques might not prevent deterioration of the archives. In this study, we propose to store discarded solutions in a secondary archive and, periodically, recycle them, bringing them back to the optimization process. Three recycling techniques were investigated for three known methods. The datasets for the experiments consisted of 91 instances of discrete and continuous problems with 2, 3 and 4 objectives. The results showed that the recycling method can benefit the tested optimizers on many problem classes.;Archiving techniques, Multi-objective evolutionary algorithms, Recycling techniques.;Universidade Federal do Rio Grande do Norte;pt_BR;Published;16;2018;2018-02-20 18:53:27
80478;Elizabeth Ferreira Gouvêa Goldbarg;Investigation of Archiving Techniques for Evolutionary Multi-objective Optimizers;Abstract: The optimization of multi-objective problems from the Pareto dominance viewpoint can lead to huge sets of incomparable solutions. Many heuristic techniques proposed to these problems have to deal with approximation sets that can be limited or not. Usually, a new solution generated by a heuristic is compared with other archived non-dominated solutions generated previously. Many techniques deal with limited size archives, since comparisons within unlimited archives may require significant computational effort. To maintain limited archives, solutions need to be discarded. Several techniques were proposed to deal with the problem of deciding which solutions remain in the archive and which are discarded. Previous investigations showed that those techniques might not prevent deterioration of the archives. In this study, we propose to store discarded solutions in a secondary archive and, periodically, recycle them, bringing them back to the optimization process. Three recycling techniques were investigated for three known methods. The datasets for the experiments consisted of 91 instances of discrete and continuous problems with 2, 3 and 4 objectives. The results showed that the recycling method can benefit the tested optimizers on many problem classes.;Archiving techniques, Multi-objective evolutionary algorithms, Recycling techniques.;Universidade Federal do Rio Grande do Norte;pt_BR;Published;16;2018;2018-02-20 18:53:27
80478;Marco Cesar Goldbarg;Investigation of Archiving Techniques for Evolutionary Multi-objective Optimizers;Abstract: The optimization of multi-objective problems from the Pareto dominance viewpoint can lead to huge sets of incomparable solutions. Many heuristic techniques proposed to these problems have to deal with approximation sets that can be limited or not. Usually, a new solution generated by a heuristic is compared with other archived non-dominated solutions generated previously. Many techniques deal with limited size archives, since comparisons within unlimited archives may require significant computational effort. To maintain limited archives, solutions need to be discarded. Several techniques were proposed to deal with the problem of deciding which solutions remain in the archive and which are discarded. Previous investigations showed that those techniques might not prevent deterioration of the archives. In this study, we propose to store discarded solutions in a secondary archive and, periodically, recycle them, bringing them back to the optimization process. Three recycling techniques were investigated for three known methods. The datasets for the experiments consisted of 91 instances of discrete and continuous problems with 2, 3 and 4 objectives. The results showed that the recycling method can benefit the tested optimizers on many problem classes.;Archiving techniques, Multi-objective evolutionary algorithms, Recycling techniques.;Universidade Federal do Rio Grande do Norte;pt_BR;Published;16;2018;2018-02-20 18:53:27
80557;Daniel Marques Gomes Morais;Um modelo computacional para a simulação de sistemas de transporte urbano;Currently, the difficulties faced in urban commuting are considered an extremely important problem, especially in large cities. Proper planning of the urban transport system is essential to minimize travel time and costs, improve quality of life and improve the urban environment. This work is based on the premise that simulation systems can be used to study different alternatives for improve the transport system, so that decision-making can be better justified, and can optimize urban travel. Therefore, this work presents the proposal and development of a computational model for simulating urban transport systems. The proposed model aims to simulate mesoscopic and microscopic models, including user behaviors when planning routes. A structure for the development of simulation applications is described, with an implementation using the São Paulo Metropolitan (Metrô) as a scenario, considering data from the Origin-Destination survey for testing and validating the model proposed here.;Simulation, Urban transport, User decision process,;Instituto Federal de Educação, Ciência e Tecnologia de São Paulo (IFSP);pt_BR;Published;22;2018;2018-02-25 20:13:41
80557;Luciano Antonio Digiampietri;Um modelo computacional para a simulação de sistemas de transporte urbano;Currently, the difficulties faced in urban commuting are considered an extremely important problem, especially in large cities. Proper planning of the urban transport system is essential to minimize travel time and costs, improve quality of life and improve the urban environment. This work is based on the premise that simulation systems can be used to study different alternatives for improve the transport system, so that decision-making can be better justified, and can optimize urban travel. Therefore, this work presents the proposal and development of a computational model for simulating urban transport systems. The proposed model aims to simulate mesoscopic and microscopic models, including user behaviors when planning routes. A structure for the development of simulation applications is described, with an implementation using the São Paulo Metropolitan (Metrô) as a scenario, considering data from the Origin-Destination survey for testing and validating the model proposed here.;Simulation, Urban transport, User decision process,;Universidade de São Paulo;pt_BR;Published;22;2018;2018-02-25 20:13:41
80598;Eduardo Henrique Silva;Use of cytomorphometry for classification of subcellular patterns in 3D images;This paper presents a methodology for the classification of subcellular patterns by the extraction of cytomorphometric features in 3D isosurfaces. In order to validate the proposal, we used a database of 3D images of HeLa cells with nine classes. For each cell, several morphological attributes were extracted based on its isosurface. Using the Quadratic Discriminant Analysis (QDA) classifier with the hybrid attribute selector, we achieved 97.59 of accuracy and F1-score of 0.9757 when classifying the subcellular patterns.;Image Processing, Cytomorphometry, HeLa cells, QDA;1 - Faculdade de Computação - Universidade Federal de Uberlândia, Uberlândia, MG, Brasil2 - Centro Universitário de Patos de Minas (UNIPAM), Patos de Minas, MG, Brasil;en_US;Published;8;2018;2018-02-26 21:20:46
80598;Jefferson Rodrigo de Souza;Use of cytomorphometry for classification of subcellular patterns in 3D images;This paper presents a methodology for the classification of subcellular patterns by the extraction of cytomorphometric features in 3D isosurfaces. In order to validate the proposal, we used a database of 3D images of HeLa cells with nine classes. For each cell, several morphological attributes were extracted based on its isosurface. Using the Quadratic Discriminant Analysis (QDA) classifier with the hybrid attribute selector, we achieved 97.59 of accuracy and F1-score of 0.9757 when classifying the subcellular patterns.;Image Processing, Cytomorphometry, HeLa cells, QDA;Faculdade de Computação - Universidade Federal de Uberlândia, Uberlândia, MG, Brasil;en_US;Published;8;2018;2018-02-26 21:20:46
80598;Bruno Augusto Nassif Travençolo;Use of cytomorphometry for classification of subcellular patterns in 3D images;This paper presents a methodology for the classification of subcellular patterns by the extraction of cytomorphometric features in 3D isosurfaces. In order to validate the proposal, we used a database of 3D images of HeLa cells with nine classes. For each cell, several morphological attributes were extracted based on its isosurface. Using the Quadratic Discriminant Analysis (QDA) classifier with the hybrid attribute selector, we achieved 97.59 of accuracy and F1-score of 0.9757 when classifying the subcellular patterns.;Image Processing, Cytomorphometry, HeLa cells, QDA;Faculdade de Computação - Universidade Federal de Uberlândia, Uberlândia, MG, Brasil;en_US;Published;8;2018;2018-02-26 21:20:46
80702;Camila Martins Saporetti;Extreme Learning Machine combined with a Differential Evolution algorithm for lithology identification;Lithology identification, obtained through the analysis of several geophysical properties, has an important role in the process of characterization of oil reservoirs. The identification can be accomplished by direct and indirect methods, but these methods are not always feasible because of the cost or imprecision of the results generated. Consequently, there is a need to automate the procedure of reservoir characterization and, in this context, computational intelligence techniques appear as an alternative to lithology identification. However, to acquire proper performance, usually some parameters should be adjusted and this can become a hard task depending on the complexity of the underlying problem. This paper aims to apply an Extreme Learning Machine (ELM) adjusted with a Differential Evolution (DE) to classify data from the South Provence Basin, using a previously published paper as a baseline reference. The paper contributions include the use of an evolutionary algorithm as a tool for search on the hyperparameters of the ELM. In addition, an  activation function recently proposed in the literature is implemented and tested. The  computational approach developed here has the potential to assist in petrographic data classification and helps to improve the process of reservoir characterization and the production development planning.;Extreme Learning Machines, Differential Evolution, Lithology;Federal University of Juiz de Fora;en_US;Published;13;2018;2018-03-01 13:12:00
80702;Grasiele Regina Duarte;Extreme Learning Machine combined with a Differential Evolution algorithm for lithology identification;Lithology identification, obtained through the analysis of several geophysical properties, has an important role in the process of characterization of oil reservoirs. The identification can be accomplished by direct and indirect methods, but these methods are not always feasible because of the cost or imprecision of the results generated. Consequently, there is a need to automate the procedure of reservoir characterization and, in this context, computational intelligence techniques appear as an alternative to lithology identification. However, to acquire proper performance, usually some parameters should be adjusted and this can become a hard task depending on the complexity of the underlying problem. This paper aims to apply an Extreme Learning Machine (ELM) adjusted with a Differential Evolution (DE) to classify data from the South Provence Basin, using a previously published paper as a baseline reference. The paper contributions include the use of an evolutionary algorithm as a tool for search on the hyperparameters of the ELM. In addition, an  activation function recently proposed in the literature is implemented and tested. The  computational approach developed here has the potential to assist in petrographic data classification and helps to improve the process of reservoir characterization and the production development planning.;Extreme Learning Machines, Differential Evolution, Lithology;Federal University of Juiz de Fora;en_US;Published;13;2018;2018-03-01 13:12:00
80702;Tales Lima Fonseca;Extreme Learning Machine combined with a Differential Evolution algorithm for lithology identification;Lithology identification, obtained through the analysis of several geophysical properties, has an important role in the process of characterization of oil reservoirs. The identification can be accomplished by direct and indirect methods, but these methods are not always feasible because of the cost or imprecision of the results generated. Consequently, there is a need to automate the procedure of reservoir characterization and, in this context, computational intelligence techniques appear as an alternative to lithology identification. However, to acquire proper performance, usually some parameters should be adjusted and this can become a hard task depending on the complexity of the underlying problem. This paper aims to apply an Extreme Learning Machine (ELM) adjusted with a Differential Evolution (DE) to classify data from the South Provence Basin, using a previously published paper as a baseline reference. The paper contributions include the use of an evolutionary algorithm as a tool for search on the hyperparameters of the ELM. In addition, an  activation function recently proposed in the literature is implemented and tested. The  computational approach developed here has the potential to assist in petrographic data classification and helps to improve the process of reservoir characterization and the production development planning.;Extreme Learning Machines, Differential Evolution, Lithology;Federal University of Juiz de Fora;en_US;Published;13;2018;2018-03-01 13:12:00
80702;Leonardo Goliatt da Fonseca;Extreme Learning Machine combined with a Differential Evolution algorithm for lithology identification;Lithology identification, obtained through the analysis of several geophysical properties, has an important role in the process of characterization of oil reservoirs. The identification can be accomplished by direct and indirect methods, but these methods are not always feasible because of the cost or imprecision of the results generated. Consequently, there is a need to automate the procedure of reservoir characterization and, in this context, computational intelligence techniques appear as an alternative to lithology identification. However, to acquire proper performance, usually some parameters should be adjusted and this can become a hard task depending on the complexity of the underlying problem. This paper aims to apply an Extreme Learning Machine (ELM) adjusted with a Differential Evolution (DE) to classify data from the South Provence Basin, using a previously published paper as a baseline reference. The paper contributions include the use of an evolutionary algorithm as a tool for search on the hyperparameters of the ELM. In addition, an  activation function recently proposed in the literature is implemented and tested. The  computational approach developed here has the potential to assist in petrographic data classification and helps to improve the process of reservoir characterization and the production development planning.;Extreme Learning Machines, Differential Evolution, Lithology;Federal University of Juiz de Fora;en_US;Published;13;2018;2018-03-01 13:12:00
80702;Egberto Pereira;Extreme Learning Machine combined with a Differential Evolution algorithm for lithology identification;Lithology identification, obtained through the analysis of several geophysical properties, has an important role in the process of characterization of oil reservoirs. The identification can be accomplished by direct and indirect methods, but these methods are not always feasible because of the cost or imprecision of the results generated. Consequently, there is a need to automate the procedure of reservoir characterization and, in this context, computational intelligence techniques appear as an alternative to lithology identification. However, to acquire proper performance, usually some parameters should be adjusted and this can become a hard task depending on the complexity of the underlying problem. This paper aims to apply an Extreme Learning Machine (ELM) adjusted with a Differential Evolution (DE) to classify data from the South Provence Basin, using a previously published paper as a baseline reference. The paper contributions include the use of an evolutionary algorithm as a tool for search on the hyperparameters of the ELM. In addition, an  activation function recently proposed in the literature is implemented and tested. The  computational approach developed here has the potential to assist in petrographic data classification and helps to improve the process of reservoir characterization and the production development planning.;Extreme Learning Machines, Differential Evolution, Lithology;Rio de Janeiro State University;en_US;Published;13;2018;2018-03-01 13:12:00
80721;Alane Marie de Lima;Exact Algorithms for the Graph Coloring Problem;The graph coloring problem is the problem of partitioning the vertices of a graph into the smallest possible set of independent sets. Since it is a well-known NP-Hard problem, it is of great interest of the computer science finding results over exact algorithms that solve it. The main algorithms of this kind, though, are scattered through the literature. In this paper, we group and contextualize some of these algorithms, which are based in Dynamic Programming, Branch-and-Bound and Integer Linear Programming. The algorithms for the first group are based in the work of Lawler, which searches maximal independent sets on each subset of vertices of a graph as the base of his algorithm. In the second group, the algorithms are based in the work of Brelaz, which adapted the DSATUR procedure to an exact version, and in the work of Zykov, which introduced the definition of Zykov trees. The third group contains the algorithms based in the work of Mehrotra and Trick, which uses the Column Generation method.;Graph Theory, Graph Coloring, Exact Algorithms;Federal University of Paraná (UFPR);en_US;Published;16;2018;2018-03-01 17:14:53
80721;Renato Carmo;Exact Algorithms for the Graph Coloring Problem;The graph coloring problem is the problem of partitioning the vertices of a graph into the smallest possible set of independent sets. Since it is a well-known NP-Hard problem, it is of great interest of the computer science finding results over exact algorithms that solve it. The main algorithms of this kind, though, are scattered through the literature. In this paper, we group and contextualize some of these algorithms, which are based in Dynamic Programming, Branch-and-Bound and Integer Linear Programming. The algorithms for the first group are based in the work of Lawler, which searches maximal independent sets on each subset of vertices of a graph as the base of his algorithm. In the second group, the algorithms are based in the work of Brelaz, which adapted the DSATUR procedure to an exact version, and in the work of Zykov, which introduced the definition of Zykov trees. The third group contains the algorithms based in the work of Mehrotra and Trick, which uses the Column Generation method.;Graph Theory, Graph Coloring, Exact Algorithms;Federal University of Paraná (UFPR);en_US;Published;16;2018;2018-03-01 17:14:53
80912;Samuel da Silva Feitosa;Formal Semantics for Java-like Languages and Research Opportunities;The objective of this paper is twofold: first, we discuss the state of art on Java-like semantics, focusing on those that provide formal specification using operational semantics (big-step or small-step), studying in detail the most cited projects and presenting some derivative works that extend the originals aggregating useful features. Also, we filter our research for those that provide some insights in type-safety proofs. Furthermore, we provide a comparison between the most used projects in order to show which functionalities are covered in such projects. Second, our effort is focused towards the research opportunities in this area, showing some important works that can be applied to the previously presented projects to study features of object-oriented languages, and pointing for some possibilities to explore in future researches.;Java Semantics, Operational Semantics, Type Systems, Type Safety;Universidade Federal de Pelotas;en_US;Published;12;2018;2018-03-08 10:41:41
80912;Rodrigo Geraldo Ribeiro;Formal Semantics for Java-like Languages and Research Opportunities;The objective of this paper is twofold: first, we discuss the state of art on Java-like semantics, focusing on those that provide formal specification using operational semantics (big-step or small-step), studying in detail the most cited projects and presenting some derivative works that extend the originals aggregating useful features. Also, we filter our research for those that provide some insights in type-safety proofs. Furthermore, we provide a comparison between the most used projects in order to show which functionalities are covered in such projects. Second, our effort is focused towards the research opportunities in this area, showing some important works that can be applied to the previously presented projects to study features of object-oriented languages, and pointing for some possibilities to explore in future researches.;Java Semantics, Operational Semantics, Type Systems, Type Safety;Universidade Federal de Ouro Preto;en_US;Published;12;2018;2018-03-08 10:41:41
80912;Andre Rauber Du Bois;Formal Semantics for Java-like Languages and Research Opportunities;The objective of this paper is twofold: first, we discuss the state of art on Java-like semantics, focusing on those that provide formal specification using operational semantics (big-step or small-step), studying in detail the most cited projects and presenting some derivative works that extend the originals aggregating useful features. Also, we filter our research for those that provide some insights in type-safety proofs. Furthermore, we provide a comparison between the most used projects in order to show which functionalities are covered in such projects. Second, our effort is focused towards the research opportunities in this area, showing some important works that can be applied to the previously presented projects to study features of object-oriented languages, and pointing for some possibilities to explore in future researches.;Java Semantics, Operational Semantics, Type Systems, Type Safety;Universidade Federal de Pelotas;en_US;Published;12;2018;2018-03-08 10:41:41
82043;Airton Orlandini Junior;Performance Analysis of the Segment Transfer Rate of TCP-UEM;UsingTCP(TransmissionControlProtocol)inwirelessnetworkscanaffectitsperformanceduetoits lack of ability to identify packets losses properly, causing the triggering of its congestion control mechanism. Some TCP variants were proposed to improve this control, being TCP-UEM one of them. This variant allows the evaluation of the link reliability in wireless networks in time intervals, keeping the end-to-end semantics. TCP-UEM was implemented in FreeBSD OS and its performance with relation to segment transfer rate (in Mbps) was compared to two other variants, TCP-NEWRENO and TCP-CUBIC. This paper describes TCP-UEM, discusses results of the tests and the statistical analysis that were carried out using two scenarios. For each scenario, 30 samples of 30 seconds of execution time with different loss rates were collected. The results showed that TCP-UEM presented a good performance, achieving a performance higher than the other two variants in the majority of the tests, with different loss rates.;TCP-UEM, congestion control, wireless networks, performance analysis, segment transfer rate;-;en_US;Published;13;2018;2018-04-17 15:26:18
82043;Luciana Andréia Fondazzi Martimiano;Performance Analysis of the Segment Transfer Rate of TCP-UEM;UsingTCP(TransmissionControlProtocol)inwirelessnetworkscanaffectitsperformanceduetoits lack of ability to identify packets losses properly, causing the triggering of its congestion control mechanism. Some TCP variants were proposed to improve this control, being TCP-UEM one of them. This variant allows the evaluation of the link reliability in wireless networks in time intervals, keeping the end-to-end semantics. TCP-UEM was implemented in FreeBSD OS and its performance with relation to segment transfer rate (in Mbps) was compared to two other variants, TCP-NEWRENO and TCP-CUBIC. This paper describes TCP-UEM, discusses results of the tests and the statistical analysis that were carried out using two scenarios. For each scenario, 30 samples of 30 seconds of execution time with different loss rates were collected. The results showed that TCP-UEM presented a good performance, achieving a performance higher than the other two variants in the majority of the tests, with different loss rates.;TCP-UEM, congestion control, wireless networks, performance analysis, segment transfer rate;-;en_US;Published;13;2018;2018-04-17 15:26:18
82395;Rafael Castro G. Silva;Haskell Type System Analysis;Types systems of programming languages are becoming more and more sophisticated and, in some cases, they are based on concepts from Logic, Type Theory and Category Theory. Haskell is a language with a modern type system and it is often singled out as an example using such theories. This work presents a small formalization of the Haskell type system and an analysis based on the mentioned theories, including its relation with the Intuitionist Propositional Second Order Logic and its logical characteristics, if there is a category in its type system and how monads are just monoids in the category of Haskell's endofunctors.;Haskell, Categories, Logic, Types;Universidade do Estado de Santa Catarina;en_US;Published;13;2018;2018-04-29 13:44:39
82395;Karina Girardi Roggia;Haskell Type System Analysis;Types systems of programming languages are becoming more and more sophisticated and, in some cases, they are based on concepts from Logic, Type Theory and Category Theory. Haskell is a language with a modern type system and it is often singled out as an example using such theories. This work presents a small formalization of the Haskell type system and an analysis based on the mentioned theories, including its relation with the Intuitionist Propositional Second Order Logic and its logical characteristics, if there is a category in its type system and how monads are just monoids in the category of Haskell's endofunctors.;Haskell, Categories, Logic, Types;Professora no curso de Bacharelado em Ciência da Computação da Universidade do Estado de Santa Catarina;en_US;Published;13;2018;2018-04-29 13:44:39
82395;Cristiano Damiani Vasconcellos;Haskell Type System Analysis;Types systems of programming languages are becoming more and more sophisticated and, in some cases, they are based on concepts from Logic, Type Theory and Category Theory. Haskell is a language with a modern type system and it is often singled out as an example using such theories. This work presents a small formalization of the Haskell type system and an analysis based on the mentioned theories, including its relation with the Intuitionist Propositional Second Order Logic and its logical characteristics, if there is a category in its type system and how monads are just monoids in the category of Haskell's endofunctors.;Haskell, Categories, Logic, Types;Professor no curso de Bacharelado em Ciência da Computação da Universidade do Estado de Santa Catarina;en_US;Published;13;2018;2018-04-29 13:44:39
82412;Bruno Well Dantas Morais;Evolutionary Models applied to Multiprocessor TaskScheduling: Serial and Multipopulation Genetic Algorithm;This work presents the development of a multipopulation genetic algorithm for the task schedulingproblem with communication costs, aiming to compare its performance with the serial genetic algorithm. For thispurpose, a set of instances was developed and different approaches for genetic operations were compared.Experiments were conducted varying the number of populations and the number of processors available forscheduling. Solution quality and execution time were analyzed, and results show that the AGMP with adjustedparameters generally produces better solutions while requiring less execution time.;multipopulation genetic algorithm, multiprocessor task scheduling;Universidade Federal de Uberlândia;en_US;Published;14;2018;2018-04-30 1:04:13
82412;Gina Maira Barbosa de Oliveira;Evolutionary Models applied to Multiprocessor TaskScheduling: Serial and Multipopulation Genetic Algorithm;This work presents the development of a multipopulation genetic algorithm for the task schedulingproblem with communication costs, aiming to compare its performance with the serial genetic algorithm. For thispurpose, a set of instances was developed and different approaches for genetic operations were compared.Experiments were conducted varying the number of populations and the number of processors available forscheduling. Solution quality and execution time were analyzed, and results show that the AGMP with adjustedparameters generally produces better solutions while requiring less execution time.;multipopulation genetic algorithm, multiprocessor task scheduling;Universidade Federal de Uberlândia;en_US;Published;14;2018;2018-04-30 1:04:13
82412;Tiago Ismailer de Carvalho;Evolutionary Models applied to Multiprocessor TaskScheduling: Serial and Multipopulation Genetic Algorithm;This work presents the development of a multipopulation genetic algorithm for the task schedulingproblem with communication costs, aiming to compare its performance with the serial genetic algorithm. For thispurpose, a set of instances was developed and different approaches for genetic operations were compared.Experiments were conducted varying the number of populations and the number of processors available forscheduling. Solution quality and execution time were analyzed, and results show that the AGMP with adjustedparameters generally produces better solutions while requiring less execution time.;multipopulation genetic algorithm, multiprocessor task scheduling;Universidade Federal de Uberlândia;en_US;Published;14;2018;2018-04-30 1:04:13
82772;Arthur Giesel Vedana;V: a language with extensible record accessors and a trait-based type system;This article introduces the V language, a purely functional programming language with a novel approach to records.Based on a system of type traits, V attempts to solve issues commonly found when manipulating records in purely functional programming languages.;Functional Programming Languages, Records, Traits;Instituto de InformáticaUniversidade Federal do Rio Grande do Sul;en_US;Published;12;2018;2018-05-11 21:27:39
82772;Rodrigo Machado;V: a language with extensible record accessors and a trait-based type system;This article introduces the V language, a purely functional programming language with a novel approach to records.Based on a system of type traits, V attempts to solve issues commonly found when manipulating records in purely functional programming languages.;Functional Programming Languages, Records, Traits;Instituto de InformáticaUniversidade Federal do Rio Grande do Sul;en_US;Published;12;2018;2018-05-11 21:27:39
82772;Álvaro Freitas Moreira;V: a language with extensible record accessors and a trait-based type system;This article introduces the V language, a purely functional programming language with a novel approach to records.Based on a system of type traits, V attempts to solve issues commonly found when manipulating records in purely functional programming languages.;Functional Programming Languages, Records, Traits;Instituto de InformáticaUniversidade Federal do Rio Grande do Sul;en_US;Published;12;2018;2018-05-11 21:27:39
83498;Márcio Sergio Soares Austregésilo;Stochastic Models for Optimizing Availability, Cost and Sustainability of Data Center Power Architectures through Genetic Algorithm;In recent years, the growth of information technology has required higher reliability, accessibility, collaboration, availability, and a reduction of costs on data centers due to factors such as social network, cloud computing, and e-commerce. These systems require redundant mechanisms on the data center infrastrucutre to achieve high availability, which may increase the electric energy consumption, impacting in both the sustainability and cost. This work proposes a multi-objective optimization approach, based on Genetic Algorithms, to optimize cost, sustainability and availability of data center power infrastructures. The main goal is to maximize availability and minimize cost and exergy consumed (adopted to estimate the environmental impacts). In order to compute such metrics, this work adopts the energy flow model (EFM), reliability block diagrams (RBD) and stochastic petri nets (SPN). Two case studies are conducted to show the applicability of the proposed strategy: (i) takes into account 5 typical data center architectures that were optimized to conduct the validation process of the proposed strategy (ii) uses the optimization strategy in two architectures classified by ANSI / TIA-942 (TIER I and II). In both case studies, significant improvements were achieved in the results, which were very close to the optimum one that was obtained by a brute force algorithm that analyzes all the possibilities and returns the optimal solution. It is worth mentioning that the time used to obtain the results using the genetic algorithm approach was significantly lower (6,763,260 times), in comparison with the strategy which combines all the possible combinations to obtain the optimal result.;Genetic Algorithm, Energy Flow Model, Sustainability, Stochastic Petri Net, Reliability Block Diagrams, Availability;Federal Rural University of Pernambuco;en_US;Published;17;2018;2018-06-02 0:48:23
83498;Gustavo Callou;Stochastic Models for Optimizing Availability, Cost and Sustainability of Data Center Power Architectures through Genetic Algorithm;In recent years, the growth of information technology has required higher reliability, accessibility, collaboration, availability, and a reduction of costs on data centers due to factors such as social network, cloud computing, and e-commerce. These systems require redundant mechanisms on the data center infrastrucutre to achieve high availability, which may increase the electric energy consumption, impacting in both the sustainability and cost. This work proposes a multi-objective optimization approach, based on Genetic Algorithms, to optimize cost, sustainability and availability of data center power infrastructures. The main goal is to maximize availability and minimize cost and exergy consumed (adopted to estimate the environmental impacts). In order to compute such metrics, this work adopts the energy flow model (EFM), reliability block diagrams (RBD) and stochastic petri nets (SPN). Two case studies are conducted to show the applicability of the proposed strategy: (i) takes into account 5 typical data center architectures that were optimized to conduct the validation process of the proposed strategy (ii) uses the optimization strategy in two architectures classified by ANSI / TIA-942 (TIER I and II). In both case studies, significant improvements were achieved in the results, which were very close to the optimum one that was obtained by a brute force algorithm that analyzes all the possibilities and returns the optimal solution. It is worth mentioning that the time used to obtain the results using the genetic algorithm approach was significantly lower (6,763,260 times), in comparison with the strategy which combines all the possible combinations to obtain the optimal result.;Genetic Algorithm, Energy Flow Model, Sustainability, Stochastic Petri Net, Reliability Block Diagrams, Availability;Department of Computing, Federal Rural University of Pernambuco, Brazil;en_US;Published;17;2018;2018-06-02 0:48:23
83581;Alfredo Silveira Araújo Neto;Use of text mining techniques for unsupervised organization of digital procedural acts;The rapid advances in technologies related to the capture and storage of data in digital format have allowed to organizations the accumulation of a volume of information extremely high, constituted a higher proportion of data in unstructured format, represented by texts. However, it is noted that the retrieval of useful information from these large repositories has been a very challenging activity. In this context, data mining is presented as a self-discovery process that acts on large databases and enables the knowledge extraction from raw text documents. Among the many sources of textual documents are electronic diaries of justice, which are intended to make public officially all the acts of the Judiciary. Despite the publication in digital form has provided improvements represented by the removal of imperfections related to divulgation at printed format, it is observed that the application of data mining methods could render more rapid analysis of its contents. In this sense, this article establishes a tool capable of automatically grouping and categorizing digital procedural acts, based on the evaluation of text mining techniques applied to groups determination activity. In addition, the strategy of defining the descriptors of the groups, that is usually conducted based on the most frequent words in the documents, was evaluated and remodeled in order to use, instead of words, the most regularly identified concepts in the texts.;Data mining, heuristic, combinatorial optimization, bio-inspired computing;Techway Informática Ltda.;en_US;Published;28;2018;2018-06-05 5:25:51
83581;Marcos Negreiros;Use of text mining techniques for unsupervised organization of digital procedural acts;The rapid advances in technologies related to the capture and storage of data in digital format have allowed to organizations the accumulation of a volume of information extremely high, constituted a higher proportion of data in unstructured format, represented by texts. However, it is noted that the retrieval of useful information from these large repositories has been a very challenging activity. In this context, data mining is presented as a self-discovery process that acts on large databases and enables the knowledge extraction from raw text documents. Among the many sources of textual documents are electronic diaries of justice, which are intended to make public officially all the acts of the Judiciary. Despite the publication in digital form has provided improvements represented by the removal of imperfections related to divulgation at printed format, it is observed that the application of data mining methods could render more rapid analysis of its contents. In this sense, this article establishes a tool capable of automatically grouping and categorizing digital procedural acts, based on the evaluation of text mining techniques applied to groups determination activity. In addition, the strategy of defining the descriptors of the groups, that is usually conducted based on the most frequent words in the documents, was evaluated and remodeled in order to use, instead of words, the most regularly identified concepts in the texts.;Data mining, heuristic, combinatorial optimization, bio-inspired computing;Universidade Estadual do Ceará;en_US;Published;28;2018;2018-06-05 5:25:51
84330;Marco Aurélio Spohn;An Analysis of a Real Mobility Trace Based on Standard Mobility Metrics;Better understanding mobility, being it from pedestrians or any other moving object, is practical and insightful. Practical due to its applications to the fundamentals of communication, with special attention to wireless communication. Insightful because it might pinpoint the pros and cons of how we are moving, or being moved, around. There are plenty of studies focused on mobility in mobile wireless networks, including the proposals of several synthetic mobility models. Getting real mobility traces is not an easy task, but there has been some efforts to provide traces to the public through repositories. Synthetic mobility models are usually analyzed through mobility metrics, which are designed to capture mobility subtleties. This work research on the applicability of some representative mobility metrics for real traces analysis. To achieve that goal, a case study is accomplished with a dataset of mobility traces of taxi cabs in the city of Rome/Italy. The results suggest that the mobility metrics under consideration are capable of capturing mobility properties which would otherwise require more sophisticated analytical approaches.;Mobility analysis, mobility metrics, mobility traces;Universidade Federal da Fronteira Sul;en_US;Published;9;2018;2018-06-28 17:51:17
84330;Matheus Henrique Trichez;An Analysis of a Real Mobility Trace Based on Standard Mobility Metrics;Better understanding mobility, being it from pedestrians or any other moving object, is practical and insightful. Practical due to its applications to the fundamentals of communication, with special attention to wireless communication. Insightful because it might pinpoint the pros and cons of how we are moving, or being moved, around. There are plenty of studies focused on mobility in mobile wireless networks, including the proposals of several synthetic mobility models. Getting real mobility traces is not an easy task, but there has been some efforts to provide traces to the public through repositories. Synthetic mobility models are usually analyzed through mobility metrics, which are designed to capture mobility subtleties. This work research on the applicability of some representative mobility metrics for real traces analysis. To achieve that goal, a case study is accomplished with a dataset of mobility traces of taxi cabs in the city of Rome/Italy. The results suggest that the mobility metrics under consideration are capable of capturing mobility properties which would otherwise require more sophisticated analytical approaches.;Mobility analysis, mobility metrics, mobility traces;-;en_US;Published;9;2018;2018-06-28 17:51:17
84814;Luciano Antonio Digiampietri;A gene based bacterial whole genome comparison toolkit;Most of the computational biology analysis is made comparing genomic features. The nucleotide and amino acid sequence alignments are frequently used in gene function identification and genome comparison. Despite its widespread use, there are limitations in their analysis capabilities that need to be considered but are often overlooked or unknown by many researchers. This paper presents a gene based whole genome comparison toolkit which can be used not only as an alternative and more robust way to compare a set of whole genomes, but, also, to understand the tradeoff of the use of sequence local alignment in this kind of comparison. A study case was performed considering fifteen whole genomes of the Xanthomonas genus. The results were compared with the 16S rRNA-processing protein RimM phylogeny and some thresholds for the use of sequence alignments in this kind of analysis were discussed.;Bioinformatics, Whole genome, Genome comparison, Phylogeny, Pangenome, Genome visualization;Universidade de São Paulo;en_US;Published;10;2018;2018-07-15 9:38:08
84814;Vivian Mayumi Yamassaki Pereira;A gene based bacterial whole genome comparison toolkit;Most of the computational biology analysis is made comparing genomic features. The nucleotide and amino acid sequence alignments are frequently used in gene function identification and genome comparison. Despite its widespread use, there are limitations in their analysis capabilities that need to be considered but are often overlooked or unknown by many researchers. This paper presents a gene based whole genome comparison toolkit which can be used not only as an alternative and more robust way to compare a set of whole genomes, but, also, to understand the tradeoff of the use of sequence local alignment in this kind of comparison. A study case was performed considering fifteen whole genomes of the Xanthomonas genus. The results were compared with the 16S rRNA-processing protein RimM phylogeny and some thresholds for the use of sequence alignments in this kind of analysis were discussed.;Bioinformatics, Whole genome, Genome comparison, Phylogeny, Pangenome, Genome visualization;Universidade de São Paulo;en_US;Published;10;2018;2018-07-15 9:38:08
84814;Geraldo José Santos-Júnior;A gene based bacterial whole genome comparison toolkit;Most of the computational biology analysis is made comparing genomic features. The nucleotide and amino acid sequence alignments are frequently used in gene function identification and genome comparison. Despite its widespread use, there are limitations in their analysis capabilities that need to be considered but are often overlooked or unknown by many researchers. This paper presents a gene based whole genome comparison toolkit which can be used not only as an alternative and more robust way to compare a set of whole genomes, but, also, to understand the tradeoff of the use of sequence local alignment in this kind of comparison. A study case was performed considering fifteen whole genomes of the Xanthomonas genus. The results were compared with the 16S rRNA-processing protein RimM phylogeny and some thresholds for the use of sequence alignments in this kind of analysis were discussed.;Bioinformatics, Whole genome, Genome comparison, Phylogeny, Pangenome, Genome visualization;Universidade de São Paulo;en_US;Published;10;2018;2018-07-15 9:38:08
84814;Giovani Sousa-Leite;A gene based bacterial whole genome comparison toolkit;Most of the computational biology analysis is made comparing genomic features. The nucleotide and amino acid sequence alignments are frequently used in gene function identification and genome comparison. Despite its widespread use, there are limitations in their analysis capabilities that need to be considered but are often overlooked or unknown by many researchers. This paper presents a gene based whole genome comparison toolkit which can be used not only as an alternative and more robust way to compare a set of whole genomes, but, also, to understand the tradeoff of the use of sequence local alignment in this kind of comparison. A study case was performed considering fifteen whole genomes of the Xanthomonas genus. The results were compared with the 16S rRNA-processing protein RimM phylogeny and some thresholds for the use of sequence alignments in this kind of analysis were discussed.;Bioinformatics, Whole genome, Genome comparison, Phylogeny, Pangenome, Genome visualization;Universidade de São Paulo;en_US;Published;10;2018;2018-07-15 9:38:08
84814;Priscilla Koch Wagner;A gene based bacterial whole genome comparison toolkit;Most of the computational biology analysis is made comparing genomic features. The nucleotide and amino acid sequence alignments are frequently used in gene function identification and genome comparison. Despite its widespread use, there are limitations in their analysis capabilities that need to be considered but are often overlooked or unknown by many researchers. This paper presents a gene based whole genome comparison toolkit which can be used not only as an alternative and more robust way to compare a set of whole genomes, but, also, to understand the tradeoff of the use of sequence local alignment in this kind of comparison. A study case was performed considering fifteen whole genomes of the Xanthomonas genus. The results were compared with the 16S rRNA-processing protein RimM phylogeny and some thresholds for the use of sequence alignments in this kind of analysis were discussed.;Bioinformatics, Whole genome, Genome comparison, Phylogeny, Pangenome, Genome visualization;Universidade de São Paulo;en_US;Published;10;2018;2018-07-15 9:38:08
84814;Leandro Márcio Moreira;A gene based bacterial whole genome comparison toolkit;Most of the computational biology analysis is made comparing genomic features. The nucleotide and amino acid sequence alignments are frequently used in gene function identification and genome comparison. Despite its widespread use, there are limitations in their analysis capabilities that need to be considered but are often overlooked or unknown by many researchers. This paper presents a gene based whole genome comparison toolkit which can be used not only as an alternative and more robust way to compare a set of whole genomes, but, also, to understand the tradeoff of the use of sequence local alignment in this kind of comparison. A study case was performed considering fifteen whole genomes of the Xanthomonas genus. The results were compared with the 16S rRNA-processing protein RimM phylogeny and some thresholds for the use of sequence alignments in this kind of analysis were discussed.;Bioinformatics, Whole genome, Genome comparison, Phylogeny, Pangenome, Genome visualization;Universidade Federal de Ouro Preto;en_US;Published;10;2018;2018-07-15 9:38:08
84814;Caio Rafael do Nascimento Santiago;A gene based bacterial whole genome comparison toolkit;Most of the computational biology analysis is made comparing genomic features. The nucleotide and amino acid sequence alignments are frequently used in gene function identification and genome comparison. Despite its widespread use, there are limitations in their analysis capabilities that need to be considered but are often overlooked or unknown by many researchers. This paper presents a gene based whole genome comparison toolkit which can be used not only as an alternative and more robust way to compare a set of whole genomes, but, also, to understand the tradeoff of the use of sequence local alignment in this kind of comparison. A study case was performed considering fifteen whole genomes of the Xanthomonas genus. The results were compared with the 16S rRNA-processing protein RimM phylogeny and some thresholds for the use of sequence alignments in this kind of analysis were discussed.;Bioinformatics, Whole genome, Genome comparison, Phylogeny, Pangenome, Genome visualization;Universidade de São Paulo;en_US;Published;10;2018;2018-07-15 9:38:08
85091;Gabriella Lopes Andrade;Analysis of the Performance of Genetic Algorithm Parallelized with OpenMP Through Execution Traces;Run tracing allows you to identify issues affecting the performance of parallel applications. This work consists in evaluating the parallelization of a Genetic Algorithm applied to the Vehicle Routing Problem with OpenMP, where the performance obtained was not ideally expected. Being that it was possible to obtain a performance increase of 1.4 times in the architecture used, however, but still below ideal. Therefore, the general objective of this work is to investigate the causes of the low performance obtained by the Genetic Algorithm, performing an analysis from the execution traces. Our results showed that the parallelization of the Genetic Algorithm is according to the model in which it was implemented and to the set of instances of the target Vehicle Routing Problem used.;Genetic Algorithms, OpenMP, Paralelization, Performance, Score-P, Tracking, Vampir, Vehicle Routing Problem;Universidade Federal do Pampa;en_US;Published;9;2018;2018-07-23 19:38:04
85091;Marcia Cristina Cera;Analysis of the Performance of Genetic Algorithm Parallelized with OpenMP Through Execution Traces;Run tracing allows you to identify issues affecting the performance of parallel applications. This work consists in evaluating the parallelization of a Genetic Algorithm applied to the Vehicle Routing Problem with OpenMP, where the performance obtained was not ideally expected. Being that it was possible to obtain a performance increase of 1.4 times in the architecture used, however, but still below ideal. Therefore, the general objective of this work is to investigate the causes of the low performance obtained by the Genetic Algorithm, performing an analysis from the execution traces. Our results showed that the parallelization of the Genetic Algorithm is according to the model in which it was implemented and to the set of instances of the target Vehicle Routing Problem used.;Genetic Algorithms, OpenMP, Paralelization, Performance, Score-P, Tracking, Vampir, Vehicle Routing Problem;Universidade Federal do Pampa;en_US;Published;9;2018;2018-07-23 19:38:04
86380;Diógenes Antonio Marques José;OLSR Fuzzy Cost (OLSR-FC): an extension to OLSR protocol based on fuzzy logic and applied to avoid selfish nodes;The mobile ad-hoc networks (MANET) are those whose nodes have mobility, energy restriction and operate simultaneously as end systems and router. One of the main problems found in MANETs is the occurrence of selfish nodes, which are those that refuse to route packets for other nodes. To address the issue of selfish nodes in MANETs and improve the flow of traffic in these networks, this paper proposes an extension to the OLSR protocol, based on Fuzzy logic, called OLSR Fuzzy Cost (OLSR -FC). Using the NS-2 simulator, the OLSR-FC proposal was compared to other extensions of OLSR protocol (e.g., OLSR-ETX, OLSR-ML e OLSR-MD) concerning the performance metrics: packet loss, end-to-end delay, Jitter, power consumption, routing overhead and throughput. The results showed that OLSR-FC obtains better performance than the evaluated extensions, avoiding selfish nodes and selecting routes whose links have little packet losses.;MANETS, Selfish Nodes, Routing, OLSR, Fuzzy Logic;Universidade do Estado de Mato Grosso (UNEMAT);en_US;Published;17;2018;2018-09-02 19:36:56
86380;Renato F. Bulcão-Neto;OLSR Fuzzy Cost (OLSR-FC): an extension to OLSR protocol based on fuzzy logic and applied to avoid selfish nodes;The mobile ad-hoc networks (MANET) are those whose nodes have mobility, energy restriction and operate simultaneously as end systems and router. One of the main problems found in MANETs is the occurrence of selfish nodes, which are those that refuse to route packets for other nodes. To address the issue of selfish nodes in MANETs and improve the flow of traffic in these networks, this paper proposes an extension to the OLSR protocol, based on Fuzzy logic, called OLSR Fuzzy Cost (OLSR -FC). Using the NS-2 simulator, the OLSR-FC proposal was compared to other extensions of OLSR protocol (e.g., OLSR-ETX, OLSR-ML e OLSR-MD) concerning the performance metrics: packet loss, end-to-end delay, Jitter, power consumption, routing overhead and throughput. The results showed that OLSR-FC obtains better performance than the evaluated extensions, avoiding selfish nodes and selecting routes whose links have little packet losses.;MANETS, Selfish Nodes, Routing, OLSR, Fuzzy Logic;Universidade Federal de Goiás (UFG), Brasil;en_US;Published;17;2018;2018-09-02 19:36:56
86380;Vinícius Sebba Patto;OLSR Fuzzy Cost (OLSR-FC): an extension to OLSR protocol based on fuzzy logic and applied to avoid selfish nodes;The mobile ad-hoc networks (MANET) are those whose nodes have mobility, energy restriction and operate simultaneously as end systems and router. One of the main problems found in MANETs is the occurrence of selfish nodes, which are those that refuse to route packets for other nodes. To address the issue of selfish nodes in MANETs and improve the flow of traffic in these networks, this paper proposes an extension to the OLSR protocol, based on Fuzzy logic, called OLSR Fuzzy Cost (OLSR -FC). Using the NS-2 simulator, the OLSR-FC proposal was compared to other extensions of OLSR protocol (e.g., OLSR-ETX, OLSR-ML e OLSR-MD) concerning the performance metrics: packet loss, end-to-end delay, Jitter, power consumption, routing overhead and throughput. The results showed that OLSR-FC obtains better performance than the evaluated extensions, avoiding selfish nodes and selecting routes whose links have little packet losses.;MANETS, Selfish Nodes, Routing, OLSR, Fuzzy Logic;Universidade Federal de Goiás (UFG), Brasil;en_US;Published;17;2018;2018-09-02 19:36:56
86380;Iwens Gervásio Sene Júnior;OLSR Fuzzy Cost (OLSR-FC): an extension to OLSR protocol based on fuzzy logic and applied to avoid selfish nodes;The mobile ad-hoc networks (MANET) are those whose nodes have mobility, energy restriction and operate simultaneously as end systems and router. One of the main problems found in MANETs is the occurrence of selfish nodes, which are those that refuse to route packets for other nodes. To address the issue of selfish nodes in MANETs and improve the flow of traffic in these networks, this paper proposes an extension to the OLSR protocol, based on Fuzzy logic, called OLSR Fuzzy Cost (OLSR -FC). Using the NS-2 simulator, the OLSR-FC proposal was compared to other extensions of OLSR protocol (e.g., OLSR-ETX, OLSR-ML e OLSR-MD) concerning the performance metrics: packet loss, end-to-end delay, Jitter, power consumption, routing overhead and throughput. The results showed that OLSR-FC obtains better performance than the evaluated extensions, avoiding selfish nodes and selecting routes whose links have little packet losses.;MANETS, Selfish Nodes, Routing, OLSR, Fuzzy Logic;Universidade Federal de Goiás (UFG), Brasil;en_US;Published;17;2018;2018-09-02 19:36:56
86439;Genildo Nonato Santos;A Model for the Diffusive Filling-In Algorithm Operating in Spike Mode;To run cortical circuit simulations in spike mode, i.e. taking into account the neural representation of information in terms of sequences of electrical pulses (also known as spikes), the use of customized hardware, which is specific for this purpose, is recommended. Simulations using more traditional hardware can be prohibitive. In this context, theoretical predictions are important for customized hardware design. For example, theoretical predictions lead to an adequate neuron model choice. To make such theoretical predictions, the cortical circuit simulations are carried out in amplitude mode. Differently from the spike mode, in amplitude mode information is represented by sequences of scalar values that describe neural input and output spike rates. In this paper, it was proposed amplitude and spike mode simulations of a cortical algorithm, namely the diffusive filling-in algorithm, to investigate whether predictions based on the amplitude-mode results approximate well the behavior of the customized hardware (spike mode results). The diffusive filling-in algorithm was chosen because it is simple enough for spike-mode simulation in a conventional computer, but the proposed amplitude-mode prediction method is the same for more complex algorithms or circuits. We provide a highly realistic comparison between amplitude-mode and spike-mode in the diffusive filling-in case, which suggests that the amplitude mode is reliable for theoretical predictions useful for customized hardware design for cortical circuit simulation. The goal of this paper is not to bring closure to these discussions but to suggest a way of avoiding possible issues that could compromise the success of the customized device design.;diffusive filling-in, visual system, silicon retina, spike encoding;instituto Federal de Educação, Ciência e Tecnologia do Rio de Janeiro;en_US;Published;13;2018;2018-09-04 19:58:09
86439;José Gabriel Rodriguez Carneiro Gomes;A Model for the Diffusive Filling-In Algorithm Operating in Spike Mode;To run cortical circuit simulations in spike mode, i.e. taking into account the neural representation of information in terms of sequences of electrical pulses (also known as spikes), the use of customized hardware, which is specific for this purpose, is recommended. Simulations using more traditional hardware can be prohibitive. In this context, theoretical predictions are important for customized hardware design. For example, theoretical predictions lead to an adequate neuron model choice. To make such theoretical predictions, the cortical circuit simulations are carried out in amplitude mode. Differently from the spike mode, in amplitude mode information is represented by sequences of scalar values that describe neural input and output spike rates. In this paper, it was proposed amplitude and spike mode simulations of a cortical algorithm, namely the diffusive filling-in algorithm, to investigate whether predictions based on the amplitude-mode results approximate well the behavior of the customized hardware (spike mode results). The diffusive filling-in algorithm was chosen because it is simple enough for spike-mode simulation in a conventional computer, but the proposed amplitude-mode prediction method is the same for more complex algorithms or circuits. We provide a highly realistic comparison between amplitude-mode and spike-mode in the diffusive filling-in case, which suggests that the amplitude mode is reliable for theoretical predictions useful for customized hardware design for cortical circuit simulation. The goal of this paper is not to bring closure to these discussions but to suggest a way of avoiding possible issues that could compromise the success of the customized device design.;diffusive filling-in, visual system, silicon retina, spike encoding;UFRJ;en_US;Published;13;2018;2018-09-04 19:58:09
86478;Rian Dutra Cunha;Virtual Reality-based Training for the Motor Developmentof People With Intellectual and Multiple Disabilities;This study investigated the potential of using a virtual reality-based motor coordination and cognitivetraining for people with intellectual and multiple disabilities, focusing on their autonomy and communityparticipation. A low-cost innovative interaction interface between the user and the virtual environment wasproposed to provide a more realistic and intuitive interaction experience. Nine people with disabilities wereselected from an institutional care for people with disabilities and they were enrolled in a four-week experiment,which consisted in going through three challenges. The results show that the patients demonstrated statisticallysignificant improvements regarding the task gave, also based on the observational analysis of the instructorand physiotherapist. Thus, the analysis indicates that the VR-based method can be effective in the motordevelopment of those patients, improving their autonomy and cognitive skills.;Virtual Reality, Augmented Reality, Intellectual and multiple disability;Universidade Federal de Juiz de Fora;en_US;Published;9;2018;2018-09-06 15:17:29
86478;Frâncila Weidt Neiva;Virtual Reality-based Training for the Motor Developmentof People With Intellectual and Multiple Disabilities;This study investigated the potential of using a virtual reality-based motor coordination and cognitivetraining for people with intellectual and multiple disabilities, focusing on their autonomy and communityparticipation. A low-cost innovative interaction interface between the user and the virtual environment wasproposed to provide a more realistic and intuitive interaction experience. Nine people with disabilities wereselected from an institutional care for people with disabilities and they were enrolled in a four-week experiment,which consisted in going through three challenges. The results show that the patients demonstrated statisticallysignificant improvements regarding the task gave, also based on the observational analysis of the instructorand physiotherapist. Thus, the analysis indicates that the VR-based method can be effective in the motordevelopment of those patients, improving their autonomy and cognitive skills.;Virtual Reality, Augmented Reality, Intellectual and multiple disability;Universidade Federal do Rio de Janeiro;en_US;Published;9;2018;2018-09-06 15:17:29
86478;Rodrigo Luis de Souza da Silva;Virtual Reality-based Training for the Motor Developmentof People With Intellectual and Multiple Disabilities;This study investigated the potential of using a virtual reality-based motor coordination and cognitivetraining for people with intellectual and multiple disabilities, focusing on their autonomy and communityparticipation. A low-cost innovative interaction interface between the user and the virtual environment wasproposed to provide a more realistic and intuitive interaction experience. Nine people with disabilities wereselected from an institutional care for people with disabilities and they were enrolled in a four-week experiment,which consisted in going through three challenges. The results show that the patients demonstrated statisticallysignificant improvements regarding the task gave, also based on the observational analysis of the instructorand physiotherapist. Thus, the analysis indicates that the VR-based method can be effective in the motordevelopment of those patients, improving their autonomy and cognitive skills.;Virtual Reality, Augmented Reality, Intellectual and multiple disability;Universidade Federal de Juiz de Fora;en_US;Published;9;2018;2018-09-06 15:17:29
86534;Antônio Carlos Rocha Costa;Elementary Economic Systems in Material Agent Societies;This paper formally characterizes the elementary economic systems of material agent societies, on the bases of the notions of (individual and group) elementary economic behavior, elementary economic exchange and elementary economic process. The equilibrium of an elementary economic system is defined in terms of the equilibrium of the set of group elementary economic processes that constitute such system. A case study illustrates the proposed concepts.;Agent societies, Material agent societies, Elementary economic systems, Elementary economic processes, Elementary economic exchanges, Elementary economic behaviors;Universidade Federal do Rio Grande - FURG;en_US;Published;13;2018;2018-09-09 10:47:12
87085;Munyque Mittelmann;Data Fusion through Fuzzy-Bayesian Networks for Belief Generation in Cognitive Agents;Situation Awareness provides a theory for agents decision making to allow perception and comprehension of his environment. However, the transformation of the sensory stimulus in beliefs to favor the BDI reasoning cycle is still an unexplored subject. Autonomous agent projects often require the use of multiple sensors to capture environmental aspects. The natural variability of the physical world and the imprecision contained in linguistic concepts used by humans mean that sensory data contain different types of uncertainty in their measurements. Thus, to obtain the Situational Awareness for Agents with physical sensors, it is necessary to define a data fusion process to perform uncertainty treatment. This paper presents a model to beliefs generation using fuzzy-bayesian inference. An example in robotics navigation and location is used to illustrate the proposal.;Situation Awareness, Data Fusion, Belief Generation, Fuzzy-Bayesian Networks;Universidade Federal de Santa Catarina;en_US;Published;11;2018;2018-09-30 22:13:18
87085;Jerusa Marchi;Data Fusion through Fuzzy-Bayesian Networks for Belief Generation in Cognitive Agents;Situation Awareness provides a theory for agents decision making to allow perception and comprehension of his environment. However, the transformation of the sensory stimulus in beliefs to favor the BDI reasoning cycle is still an unexplored subject. Autonomous agent projects often require the use of multiple sensors to capture environmental aspects. The natural variability of the physical world and the imprecision contained in linguistic concepts used by humans mean that sensory data contain different types of uncertainty in their measurements. Thus, to obtain the Situational Awareness for Agents with physical sensors, it is necessary to define a data fusion process to perform uncertainty treatment. This paper presents a model to beliefs generation using fuzzy-bayesian inference. An example in robotics navigation and location is used to illustrate the proposal.;Situation Awareness, Data Fusion, Belief Generation, Fuzzy-Bayesian Networks;Universidade Federal de Santa Catarina;en_US;Published;11;2018;2018-09-30 22:13:18
87085;Aldo von Wangenheim;Data Fusion through Fuzzy-Bayesian Networks for Belief Generation in Cognitive Agents;Situation Awareness provides a theory for agents decision making to allow perception and comprehension of his environment. However, the transformation of the sensory stimulus in beliefs to favor the BDI reasoning cycle is still an unexplored subject. Autonomous agent projects often require the use of multiple sensors to capture environmental aspects. The natural variability of the physical world and the imprecision contained in linguistic concepts used by humans mean that sensory data contain different types of uncertainty in their measurements. Thus, to obtain the Situational Awareness for Agents with physical sensors, it is necessary to define a data fusion process to perform uncertainty treatment. This paper presents a model to beliefs generation using fuzzy-bayesian inference. An example in robotics navigation and location is used to illustrate the proposal.;Situation Awareness, Data Fusion, Belief Generation, Fuzzy-Bayesian Networks;Universidade Federal de Santa Catarina;en_US;Published;11;2018;2018-09-30 22:13:18
87511;Rajeev Ranjan Yadav;A Strategy for Performance Evaluation and Modeling of Cloud Computing Services;On-demand services and reduced costs made cloud computing a popular mechanism to provide scalable resources according to the user’s expectations. This paradigm is an important role in business and academic organizations, supporting applications and services deployed based on virtual machines and containers, two different technologies for virtualization. Cloud environments can support workloads generated by several numbers of users, that request the cloud environment to execute transactions and its performance should be evaluated and estimated in order to achieve clients satisfactions when cloud services are offered. This work proposes a performance evaluation strategy composed of a performance model and a methodology for evaluating the performance of services configured in virtual machines and containers in cloud infrastructures. The performance model for the evaluation of virtual machines and containers in cloud infrastructures is based on stochastic Petri nets. A case study in a real public cloud is presented to illustrate the feasibility of the performance evaluation strategy. The case study experiments were performed with virtual machines and containers supporting workloads related to social networks transactions.;Performance evaluation, Cloud Computing, Containers, Virtual machines, Performance model;Federal Rural University of Pernambuco;en_US;Published;12;2018;2018-10-17 20:06:27
87511;Gleidson A. S. Campos;A Strategy for Performance Evaluation and Modeling of Cloud Computing Services;On-demand services and reduced costs made cloud computing a popular mechanism to provide scalable resources according to the user’s expectations. This paradigm is an important role in business and academic organizations, supporting applications and services deployed based on virtual machines and containers, two different technologies for virtualization. Cloud environments can support workloads generated by several numbers of users, that request the cloud environment to execute transactions and its performance should be evaluated and estimated in order to achieve clients satisfactions when cloud services are offered. This work proposes a performance evaluation strategy composed of a performance model and a methodology for evaluating the performance of services configured in virtual machines and containers in cloud infrastructures. The performance model for the evaluation of virtual machines and containers in cloud infrastructures is based on stochastic Petri nets. A case study in a real public cloud is presented to illustrate the feasibility of the performance evaluation strategy. The case study experiments were performed with virtual machines and containers supporting workloads related to social networks transactions.;Performance evaluation, Cloud Computing, Containers, Virtual machines, Performance model;Federal Rural University of Pernambuco;en_US;Published;12;2018;2018-10-17 20:06:27
87511;Erica Teixeira Gomes Sousa;A Strategy for Performance Evaluation and Modeling of Cloud Computing Services;On-demand services and reduced costs made cloud computing a popular mechanism to provide scalable resources according to the user’s expectations. This paradigm is an important role in business and academic organizations, supporting applications and services deployed based on virtual machines and containers, two different technologies for virtualization. Cloud environments can support workloads generated by several numbers of users, that request the cloud environment to execute transactions and its performance should be evaluated and estimated in order to achieve clients satisfactions when cloud services are offered. This work proposes a performance evaluation strategy composed of a performance model and a methodology for evaluating the performance of services configured in virtual machines and containers in cloud infrastructures. The performance model for the evaluation of virtual machines and containers in cloud infrastructures is based on stochastic Petri nets. A case study in a real public cloud is presented to illustrate the feasibility of the performance evaluation strategy. The case study experiments were performed with virtual machines and containers supporting workloads related to social networks transactions.;Performance evaluation, Cloud Computing, Containers, Virtual machines, Performance model;Federal Rural University of Pernambuco;en_US;Published;12;2018;2018-10-17 20:06:27
87511;Fernando Aires Lins;A Strategy for Performance Evaluation and Modeling of Cloud Computing Services;On-demand services and reduced costs made cloud computing a popular mechanism to provide scalable resources according to the user’s expectations. This paradigm is an important role in business and academic organizations, supporting applications and services deployed based on virtual machines and containers, two different technologies for virtualization. Cloud environments can support workloads generated by several numbers of users, that request the cloud environment to execute transactions and its performance should be evaluated and estimated in order to achieve clients satisfactions when cloud services are offered. This work proposes a performance evaluation strategy composed of a performance model and a methodology for evaluating the performance of services configured in virtual machines and containers in cloud infrastructures. The performance model for the evaluation of virtual machines and containers in cloud infrastructures is based on stochastic Petri nets. A case study in a real public cloud is presented to illustrate the feasibility of the performance evaluation strategy. The case study experiments were performed with virtual machines and containers supporting workloads related to social networks transactions.;Performance evaluation, Cloud Computing, Containers, Virtual machines, Performance model;Federal Rural University of Pernambuco;en_US;Published;12;2018;2018-10-17 20:06:27
88822;Felipe Fernandes Lima Melo;Evaluating the impact of maintenance policies associated to SLA contracts on the dependability of data centers electrical infrastructures;Due to the growth of cloud computing, data center environment has grown in importance and in use. Data centers are responsible for maintaining and processing several critical-value applications. Therefore, data center infrastructures must be evaluated in order to improve the high availability and reliability demanded for such environments. This work adopts Stochastic Petri Nets (SPN) to evaluate the impact of maintenance policies on the data center dependability. The main goal is to analyze maintenance policies, associated to SLA contracts, and to propose improvements. In order to accomplish this, an optimization strategy that uses Euclidean distance is adopted to indicate the most appropriate solution assuming conflicting requirements (e.g., cost and availability). To illustrate the applicability of the proposed models and approach, this work presents case studies comparing different SLA contracts and maintenance policies (preventive and corrective) applied on data center electrical infrastructures.;Dependability, Maintenance, Stochastic Petri Nets and Data Center;Universidade Federal Rural de Pernambuco;en_US;Published;12;2018;2018-12-11 16:18:19
88822;Joao Ferreira Silva Junior;Evaluating the impact of maintenance policies associated to SLA contracts on the dependability of data centers electrical infrastructures;Due to the growth of cloud computing, data center environment has grown in importance and in use. Data centers are responsible for maintaining and processing several critical-value applications. Therefore, data center infrastructures must be evaluated in order to improve the high availability and reliability demanded for such environments. This work adopts Stochastic Petri Nets (SPN) to evaluate the impact of maintenance policies on the data center dependability. The main goal is to analyze maintenance policies, associated to SLA contracts, and to propose improvements. In order to accomplish this, an optimization strategy that uses Euclidean distance is adopted to indicate the most appropriate solution assuming conflicting requirements (e.g., cost and availability). To illustrate the applicability of the proposed models and approach, this work presents case studies comparing different SLA contracts and maintenance policies (preventive and corrective) applied on data center electrical infrastructures.;Dependability, Maintenance, Stochastic Petri Nets and Data Center;Universidade Federal Rural de Pernambuco;en_US;Published;12;2018;2018-12-11 16:18:19
88822;Gustavo Rau de Almeida Callou;Evaluating the impact of maintenance policies associated to SLA contracts on the dependability of data centers electrical infrastructures;Due to the growth of cloud computing, data center environment has grown in importance and in use. Data centers are responsible for maintaining and processing several critical-value applications. Therefore, data center infrastructures must be evaluated in order to improve the high availability and reliability demanded for such environments. This work adopts Stochastic Petri Nets (SPN) to evaluate the impact of maintenance policies on the data center dependability. The main goal is to analyze maintenance policies, associated to SLA contracts, and to propose improvements. In order to accomplish this, an optimization strategy that uses Euclidean distance is adopted to indicate the most appropriate solution assuming conflicting requirements (e.g., cost and availability). To illustrate the applicability of the proposed models and approach, this work presents case studies comparing different SLA contracts and maintenance policies (preventive and corrective) applied on data center electrical infrastructures.;Dependability, Maintenance, Stochastic Petri Nets and Data Center;Universidade Federal Rural de Pernambuco;en_US;Published;12;2018;2018-12-11 16:18:19
88911;Wilson Castello Branco Neto;Hybrid Neural Networks Applied to Brazilian Stock Market;The stock market is a stochastic, dynamic environment and is in constant evolution, and its prediction represents a big challenge. Many studies presented in the state of the art are facing this challenge, by making use of Artificial Neural Networks (ANN) as a tool to make such prediction. In this paper a comparative study is made with different methods in order to predict the Brazilian stock market through the Bovespa Index. An ANN was developed and its performance was compared against a hybrid model, in which a Genetic Algorithm (GA) is proposed as an alternative to improve the performance of this ANN. The results obtained were an average accuracy of 55.04% and 55.73% respectively, demonstrating that algorithms such as a GA have the capability of improving the performance of ANN for the stock market prediciton.;Artificial Intelligence, Artificial Neural Networks, Genetic Algorithms.;Instituto Federal de Educação, Ciência e Tecnologia de Santa Catarina;en_US;Published;23;2018;2018-12-19 9:56:14
88911;Andrey de Aguiar Salvi;Hybrid Neural Networks Applied to Brazilian Stock Market;The stock market is a stochastic, dynamic environment and is in constant evolution, and its prediction represents a big challenge. Many studies presented in the state of the art are facing this challenge, by making use of Artificial Neural Networks (ANN) as a tool to make such prediction. In this paper a comparative study is made with different methods in order to predict the Brazilian stock market through the Bovespa Index. An ANN was developed and its performance was compared against a hybrid model, in which a Genetic Algorithm (GA) is proposed as an alternative to improve the performance of this ANN. The results obtained were an average accuracy of 55.04% and 55.73% respectively, demonstrating that algorithms such as a GA have the capability of improving the performance of ANN for the stock market prediciton.;Artificial Intelligence, Artificial Neural Networks, Genetic Algorithms.;Instituto Federal de Educação, Ciência e Tecnologia de Santa Catarina;en_US;Published;23;2018;2018-12-19 9:56:14
88911;William Passig de Souza;Hybrid Neural Networks Applied to Brazilian Stock Market;The stock market is a stochastic, dynamic environment and is in constant evolution, and its prediction represents a big challenge. Many studies presented in the state of the art are facing this challenge, by making use of Artificial Neural Networks (ANN) as a tool to make such prediction. In this paper a comparative study is made with different methods in order to predict the Brazilian stock market through the Bovespa Index. An ANN was developed and its performance was compared against a hybrid model, in which a Genetic Algorithm (GA) is proposed as an alternative to improve the performance of this ANN. The results obtained were an average accuracy of 55.04% and 55.73% respectively, demonstrating that algorithms such as a GA have the capability of improving the performance of ANN for the stock market prediciton.;Artificial Intelligence, Artificial Neural Networks, Genetic Algorithms.;Instituto Federal de Educação, Ciência e Tecnologia de Santa Catarina;en_US;Published;23;2018;2018-12-19 9:56:14
89063;Alexandra Katiuska Ramos Diaz;Biclustering and coclustering: concepts, algorithms and viability for text mining;Biclustering and coclustering are data mining tasks capable of extracting relevant information from data by applying similarity criteria simultaneously to rows and columns of data matrices. Algorithms used to accomplish these tasks simultaneously cluster objects and attributes, enabling the discovery of biclusters or coclusters. Although similar, the natures and aims of these tasks are different, and coclustering can be seen as a generalization of biclustering. An accurate study on algorithms related to biclustering and coclustering is essential to achieve effectiveness when solving real-world problems. Determining the values appropriate for the parameters of these algorithms is even more difficult when complex real-world data are analyzed. For example, when biclustering or coclustering is applied to textual data (i.e., in text mining), a representation through a vector space model is required. Such representation usually generates vector spaces with a high number of dimensions and high sparsity, which influences the performance of many algorithms. This tutorial aims to didactically present concepts related to the biclustering and coclustering tasks and how two basic algorithms address these concepts. In addition, experiments are presented in data contexts with a high number of dimensions and high sparsity, represented by both a synthetic dataset and a corpus of real-world news. In general and comparative terms, the results obtained show the algorithm used for coclustering (i.e., NBVD) as the most appropriate for the experiments’ context. Although the biclustering algorithm (i.e., Cheng and Church) was responsible for producing less relevant results in textual data than NBVD, its application in data with a high number of dimensions and high sparsity provided a suitable study environment to understand its operation.;Cogrouping, Bigrouping, Text Mining;Universidade de São Paulo;en_US;Published;36;2018;2018-12-18 15:35:52
89063;Sarajane Marques Peres;Biclustering and coclustering: concepts, algorithms and viability for text mining;Biclustering and coclustering are data mining tasks capable of extracting relevant information from data by applying similarity criteria simultaneously to rows and columns of data matrices. Algorithms used to accomplish these tasks simultaneously cluster objects and attributes, enabling the discovery of biclusters or coclusters. Although similar, the natures and aims of these tasks are different, and coclustering can be seen as a generalization of biclustering. An accurate study on algorithms related to biclustering and coclustering is essential to achieve effectiveness when solving real-world problems. Determining the values appropriate for the parameters of these algorithms is even more difficult when complex real-world data are analyzed. For example, when biclustering or coclustering is applied to textual data (i.e., in text mining), a representation through a vector space model is required. Such representation usually generates vector spaces with a high number of dimensions and high sparsity, which influences the performance of many algorithms. This tutorial aims to didactically present concepts related to the biclustering and coclustering tasks and how two basic algorithms address these concepts. In addition, experiments are presented in data contexts with a high number of dimensions and high sparsity, represented by both a synthetic dataset and a corpus of real-world news. In general and comparative terms, the results obtained show the algorithm used for coclustering (i.e., NBVD) as the most appropriate for the experiments’ context. Although the biclustering algorithm (i.e., Cheng and Church) was responsible for producing less relevant results in textual data than NBVD, its application in data with a high number of dimensions and high sparsity provided a suitable study environment to understand its operation.;Cogrouping, Bigrouping, Text Mining;Universidade de São Paulo;en_US;Published;36;2018;2018-12-18 15:35:52
89115;Gilmário Barbosa Santos;PHOC Descriptor Applied for Mammography Classification;This paper describes experiments with PHOC (Pyramid Histogram of Color) features descriptor in terms of capacity for representing features presented in breast radiograph (also known as mammography). Patches were taken from regions in digital mammographies, representing benign, cancerous, normal tissues and image’s background. The motivation is to evaluate the proposal in perspective of using it for execution in an inexpensive ordinary desktop computer in places located far from medical experts. The images were obtained from DDSM database and processed producing the feature-dataset used for training an Artificial Neural Network, the results were evaluated by analysis of the learning rate curve and ROC curves, besides these graphical analytical tools the confusion matrix and other quantitative metrics (TPR, FPR and Accuracy) were also extracted and analyzed. The average accuracy ≈ 0.8 and the other metrics extracted from results demonstrate that the proposal presents potential for further developments. At the best effort, PHOC was not found in literature for applications in mammographies such as it is proposed here.;;UDESC;en_US;Published;9;2018;2018-12-20 18:43:43
90724;Antonio Rafael Braga;BeeNotified! A Notification System of Physical Quantities for Beehives Remote Monitoring;One of the ways to reduce inappropriate management of hives and monitor bee health is to send notifications/alerts about the data collected through sensors. This study presents  BeeNotified!, a solution for sending notifications through Telegram, e-mail, and SMS. The notifications warn about the level of temperature, humidity, sound, carbon dioxide, oxygen, hive weight and delay in data gathering. From this data, researchers and beekeepers can be informed and make changes in the locations of the hives, avoiding catastrophes and possible diseases. The results obtained with the processing time in the sending of messages showed that the messages sent via SMS and Telegram have a shorter processing time compared to the sending via e-mail. In regards to sending notifications according to user preferences, all notifications were sent correctly.;Notifications — Beekeeping — Environmental monitoring;Universidade Federal do Ceará;en_US;Published;11;2019;2019-03-06 18:46:41
90724;Juliana de Castro Rabelo;BeeNotified! A Notification System of Physical Quantities for Beehives Remote Monitoring;One of the ways to reduce inappropriate management of hives and monitor bee health is to send notifications/alerts about the data collected through sensors. This study presents  BeeNotified!, a solution for sending notifications through Telegram, e-mail, and SMS. The notifications warn about the level of temperature, humidity, sound, carbon dioxide, oxygen, hive weight and delay in data gathering. From this data, researchers and beekeepers can be informed and make changes in the locations of the hives, avoiding catastrophes and possible diseases. The results obtained with the processing time in the sending of messages showed that the messages sent via SMS and Telegram have a shorter processing time compared to the sending via e-mail. In regards to sending notifications according to user preferences, all notifications were sent correctly.;Notifications — Beekeeping — Environmental monitoring;Universidade Federal do Ceará;en_US;Published;11;2019;2019-03-06 18:46:41
90724;Arthur de Castro Callado;BeeNotified! A Notification System of Physical Quantities for Beehives Remote Monitoring;One of the ways to reduce inappropriate management of hives and monitor bee health is to send notifications/alerts about the data collected through sensors. This study presents  BeeNotified!, a solution for sending notifications through Telegram, e-mail, and SMS. The notifications warn about the level of temperature, humidity, sound, carbon dioxide, oxygen, hive weight and delay in data gathering. From this data, researchers and beekeepers can be informed and make changes in the locations of the hives, avoiding catastrophes and possible diseases. The results obtained with the processing time in the sending of messages showed that the messages sent via SMS and Telegram have a shorter processing time compared to the sending via e-mail. In regards to sending notifications according to user preferences, all notifications were sent correctly.;Notifications — Beekeeping — Environmental monitoring;Universidade Federal do Ceará;en_US;Published;11;2019;2019-03-06 18:46:41
90724;Atslands Rego da Rocha;BeeNotified! A Notification System of Physical Quantities for Beehives Remote Monitoring;One of the ways to reduce inappropriate management of hives and monitor bee health is to send notifications/alerts about the data collected through sensors. This study presents  BeeNotified!, a solution for sending notifications through Telegram, e-mail, and SMS. The notifications warn about the level of temperature, humidity, sound, carbon dioxide, oxygen, hive weight and delay in data gathering. From this data, researchers and beekeepers can be informed and make changes in the locations of the hives, avoiding catastrophes and possible diseases. The results obtained with the processing time in the sending of messages showed that the messages sent via SMS and Telegram have a shorter processing time compared to the sending via e-mail. In regards to sending notifications according to user preferences, all notifications were sent correctly.;Notifications — Beekeeping — Environmental monitoring;Universidade Federal do Ceará;en_US;Published;11;2019;2019-03-06 18:46:41
90724;Breno M. Freitas;BeeNotified! A Notification System of Physical Quantities for Beehives Remote Monitoring;One of the ways to reduce inappropriate management of hives and monitor bee health is to send notifications/alerts about the data collected through sensors. This study presents  BeeNotified!, a solution for sending notifications through Telegram, e-mail, and SMS. The notifications warn about the level of temperature, humidity, sound, carbon dioxide, oxygen, hive weight and delay in data gathering. From this data, researchers and beekeepers can be informed and make changes in the locations of the hives, avoiding catastrophes and possible diseases. The results obtained with the processing time in the sending of messages showed that the messages sent via SMS and Telegram have a shorter processing time compared to the sending via e-mail. In regards to sending notifications according to user preferences, all notifications were sent correctly.;Notifications — Beekeeping — Environmental monitoring;Universidade Federal do Ceará;en_US;Published;11;2019;2019-03-06 18:46:41
90724;Danielo G. Gomes;BeeNotified! A Notification System of Physical Quantities for Beehives Remote Monitoring;One of the ways to reduce inappropriate management of hives and monitor bee health is to send notifications/alerts about the data collected through sensors. This study presents  BeeNotified!, a solution for sending notifications through Telegram, e-mail, and SMS. The notifications warn about the level of temperature, humidity, sound, carbon dioxide, oxygen, hive weight and delay in data gathering. From this data, researchers and beekeepers can be informed and make changes in the locations of the hives, avoiding catastrophes and possible diseases. The results obtained with the processing time in the sending of messages showed that the messages sent via SMS and Telegram have a shorter processing time compared to the sending via e-mail. In regards to sending notifications according to user preferences, all notifications were sent correctly.;Notifications — Beekeeping — Environmental monitoring;Universidade Federal do Ceará;en_US;Published;11;2019;2019-03-06 18:46:41
90822;Agustín Alejandro Ortiz Díaz;An Online Tree-Based Approach for Mining Non-Stationary High-Speed Data Streams; This paper presents a new learning algorithm for inducing decision trees from data streams. In these domains, large amounts of data are constantly arriving over time, possibly at high speed. The proposed algorithm uses a top-down induction method for building trees, splitting leaf nodes recursively, until none of them can be expanded. The new algorithm combines two split methods in the tree induction. The first method is able to guarantee, with statistical significance, that each split chosen would be the same as that chosen using infinite examples. By doing so, it aims at ensuring that the tree induced online is close to the optimal model. However, this split method often needs too many examples to make a decision about the best split, which delays the accuracy improvement of the online predictive learning model. Therefore, the second method is used to split nodes more quickly, speeding up the tree growth. The second split method is based on the observation that larger trees are able to store more information about the training examples and to represent more complex concepts. The first split method is also used to correct splits previously suggested by the second one, when it has sufficient evidence. Finally, an additional procedure rebuilds the tree model according to the suggestions made with an adequate level of statistical significance. The proposed algorithm is empirically compared with several well-known induction algorithms for learning decision trees from data streams. In the tests it is possible to observe that the proposed algorithm is more competitive in terms of accuracy and model size using various synthetic and real world datasets.   ;Data stream, decision tree, incremental learning, machine learning, online learning;Universidade do estado de Santa Catarina.;en_US;Published;11;2019;2019-03-10 22:55:24
90822;Isvani Inocencio Frías Blanco;An Online Tree-Based Approach for Mining Non-Stationary High-Speed Data Streams; This paper presents a new learning algorithm for inducing decision trees from data streams. In these domains, large amounts of data are constantly arriving over time, possibly at high speed. The proposed algorithm uses a top-down induction method for building trees, splitting leaf nodes recursively, until none of them can be expanded. The new algorithm combines two split methods in the tree induction. The first method is able to guarantee, with statistical significance, that each split chosen would be the same as that chosen using infinite examples. By doing so, it aims at ensuring that the tree induced online is close to the optimal model. However, this split method often needs too many examples to make a decision about the best split, which delays the accuracy improvement of the online predictive learning model. Therefore, the second method is used to split nodes more quickly, speeding up the tree growth. The second split method is based on the observation that larger trees are able to store more information about the training examples and to represent more complex concepts. The first split method is also used to correct splits previously suggested by the second one, when it has sufficient evidence. Finally, an additional procedure rebuilds the tree model according to the suggestions made with an adequate level of statistical significance. The proposed algorithm is empirically compared with several well-known induction algorithms for learning decision trees from data streams. In the tests it is possible to observe that the proposed algorithm is more competitive in terms of accuracy and model size using various synthetic and real world datasets.   ;Data stream, decision tree, incremental learning, machine learning, online learning;LexisNexis Risk Solutions;en_US;Published;11;2019;2019-03-10 22:55:24
90822;Laura María Palomino Mariño;An Online Tree-Based Approach for Mining Non-Stationary High-Speed Data Streams; This paper presents a new learning algorithm for inducing decision trees from data streams. In these domains, large amounts of data are constantly arriving over time, possibly at high speed. The proposed algorithm uses a top-down induction method for building trees, splitting leaf nodes recursively, until none of them can be expanded. The new algorithm combines two split methods in the tree induction. The first method is able to guarantee, with statistical significance, that each split chosen would be the same as that chosen using infinite examples. By doing so, it aims at ensuring that the tree induced online is close to the optimal model. However, this split method often needs too many examples to make a decision about the best split, which delays the accuracy improvement of the online predictive learning model. Therefore, the second method is used to split nodes more quickly, speeding up the tree growth. The second split method is based on the observation that larger trees are able to store more information about the training examples and to represent more complex concepts. The first split method is also used to correct splits previously suggested by the second one, when it has sufficient evidence. Finally, an additional procedure rebuilds the tree model according to the suggestions made with an adequate level of statistical significance. The proposed algorithm is empirically compared with several well-known induction algorithms for learning decision trees from data streams. In the tests it is possible to observe that the proposed algorithm is more competitive in terms of accuracy and model size using various synthetic and real world datasets.   ;Data stream, decision tree, incremental learning, machine learning, online learning;Pernambuco Federal University;en_US;Published;11;2019;2019-03-10 22:55:24
90822;Fabiano Baldo;An Online Tree-Based Approach for Mining Non-Stationary High-Speed Data Streams; This paper presents a new learning algorithm for inducing decision trees from data streams. In these domains, large amounts of data are constantly arriving over time, possibly at high speed. The proposed algorithm uses a top-down induction method for building trees, splitting leaf nodes recursively, until none of them can be expanded. The new algorithm combines two split methods in the tree induction. The first method is able to guarantee, with statistical significance, that each split chosen would be the same as that chosen using infinite examples. By doing so, it aims at ensuring that the tree induced online is close to the optimal model. However, this split method often needs too many examples to make a decision about the best split, which delays the accuracy improvement of the online predictive learning model. Therefore, the second method is used to split nodes more quickly, speeding up the tree growth. The second split method is based on the observation that larger trees are able to store more information about the training examples and to represent more complex concepts. The first split method is also used to correct splits previously suggested by the second one, when it has sufficient evidence. Finally, an additional procedure rebuilds the tree model according to the suggestions made with an adequate level of statistical significance. The proposed algorithm is empirically compared with several well-known induction algorithms for learning decision trees from data streams. In the tests it is possible to observe that the proposed algorithm is more competitive in terms of accuracy and model size using various synthetic and real world datasets.   ;Data stream, decision tree, incremental learning, machine learning, online learning;Santa Catarina State University;en_US;Published;11;2019;2019-03-10 22:55:24
91414;Francisco Imperes Filho;Group Labeling Methodology Using Distance-based Data Grouping Algorithms;Clustering algorithms are often used to form groups based on the similarity of their members. In this context, understanding a group is just as important as its composition. Identifying, or labeling groups can assist with their interpretation and, consequently, guide decision-making efforts by taking into account the features from each group. Interpreting groups can be beneficial when it is necessary to know what makes an element a part of a given group, what are the main features of a group, and what are the differences and similarities among them. This work describes a method for finding relevant features and generate labels for the elements of each group, uniquely identifying them. This way, our approach solves the problem of finding relevant definitions that can identify groups. The proposed method transforms the standard output of an unsupervised distance-based clustering algorithm into a Pertinence Degree (GP), where each element of the database receives a GP concerning each formed group. The elements with their GPs are used to formulate ranges of values for their attributes. Such ranges can identify the groups uniquely. The labels produced by this approach averaged 94.83% of correct answers for the analyzed databases, allowing a natural interpretation of the generated definitions.;Data Labeling, Data Definition, Data Grouping, Machine Learning;Federal University of Piaui;en_US;Published;13;2019;2019-03-28 15:59:32
91414;Vinicius Ponte Machado;Group Labeling Methodology Using Distance-based Data Grouping Algorithms;Clustering algorithms are often used to form groups based on the similarity of their members. In this context, understanding a group is just as important as its composition. Identifying, or labeling groups can assist with their interpretation and, consequently, guide decision-making efforts by taking into account the features from each group. Interpreting groups can be beneficial when it is necessary to know what makes an element a part of a given group, what are the main features of a group, and what are the differences and similarities among them. This work describes a method for finding relevant features and generate labels for the elements of each group, uniquely identifying them. This way, our approach solves the problem of finding relevant definitions that can identify groups. The proposed method transforms the standard output of an unsupervised distance-based clustering algorithm into a Pertinence Degree (GP), where each element of the database receives a GP concerning each formed group. The elements with their GPs are used to formulate ranges of values for their attributes. Such ranges can identify the groups uniquely. The labels produced by this approach averaged 94.83% of correct answers for the analyzed databases, allowing a natural interpretation of the generated definitions.;Data Labeling, Data Definition, Data Grouping, Machine Learning;Departamento de Computacão, Universidade Federal do Piauí, Brasil;en_US;Published;13;2019;2019-03-28 15:59:32
91414;Rodrigo de Melo Souza Veras;Group Labeling Methodology Using Distance-based Data Grouping Algorithms;Clustering algorithms are often used to form groups based on the similarity of their members. In this context, understanding a group is just as important as its composition. Identifying, or labeling groups can assist with their interpretation and, consequently, guide decision-making efforts by taking into account the features from each group. Interpreting groups can be beneficial when it is necessary to know what makes an element a part of a given group, what are the main features of a group, and what are the differences and similarities among them. This work describes a method for finding relevant features and generate labels for the elements of each group, uniquely identifying them. This way, our approach solves the problem of finding relevant definitions that can identify groups. The proposed method transforms the standard output of an unsupervised distance-based clustering algorithm into a Pertinence Degree (GP), where each element of the database receives a GP concerning each formed group. The elements with their GPs are used to formulate ranges of values for their attributes. Such ranges can identify the groups uniquely. The labels produced by this approach averaged 94.83% of correct answers for the analyzed databases, allowing a natural interpretation of the generated definitions.;Data Labeling, Data Definition, Data Grouping, Machine Learning;Departamento de Computacao, Universidade Federal do Piauı, Brasil;en_US;Published;13;2019;2019-03-28 15:59:32
91414;Kelson Romulo Teixeira Aires;Group Labeling Methodology Using Distance-based Data Grouping Algorithms;Clustering algorithms are often used to form groups based on the similarity of their members. In this context, understanding a group is just as important as its composition. Identifying, or labeling groups can assist with their interpretation and, consequently, guide decision-making efforts by taking into account the features from each group. Interpreting groups can be beneficial when it is necessary to know what makes an element a part of a given group, what are the main features of a group, and what are the differences and similarities among them. This work describes a method for finding relevant features and generate labels for the elements of each group, uniquely identifying them. This way, our approach solves the problem of finding relevant definitions that can identify groups. The proposed method transforms the standard output of an unsupervised distance-based clustering algorithm into a Pertinence Degree (GP), where each element of the database receives a GP concerning each formed group. The elements with their GPs are used to formulate ranges of values for their attributes. Such ranges can identify the groups uniquely. The labels produced by this approach averaged 94.83% of correct answers for the analyzed databases, allowing a natural interpretation of the generated definitions.;Data Labeling, Data Definition, Data Grouping, Machine Learning;Departamento de Computacao, Universidade Federal do Piaui, Brasil;en_US;Published;13;2019;2019-03-28 15:59:32
91414;Aline Montenegro Leal Silva;Group Labeling Methodology Using Distance-based Data Grouping Algorithms;Clustering algorithms are often used to form groups based on the similarity of their members. In this context, understanding a group is just as important as its composition. Identifying, or labeling groups can assist with their interpretation and, consequently, guide decision-making efforts by taking into account the features from each group. Interpreting groups can be beneficial when it is necessary to know what makes an element a part of a given group, what are the main features of a group, and what are the differences and similarities among them. This work describes a method for finding relevant features and generate labels for the elements of each group, uniquely identifying them. This way, our approach solves the problem of finding relevant definitions that can identify groups. The proposed method transforms the standard output of an unsupervised distance-based clustering algorithm into a Pertinence Degree (GP), where each element of the database receives a GP concerning each formed group. The elements with their GPs are used to formulate ranges of values for their attributes. Such ranges can identify the groups uniquely. The labels produced by this approach averaged 94.83% of correct answers for the analyzed databases, allowing a natural interpretation of the generated definitions.;Data Labeling, Data Definition, Data Grouping, Machine Learning;Centro de Educacao Aberta e a Distancia, Universidade Federal do Piauı, Brasil;en_US;Published;13;2019;2019-03-28 15:59:32
91522;Thiago Rateke;Road Surface Classification with Images Captured From Low-cost Camera - Road Traversing Knowledge (RTK) Dataset;The type of road pavement directly influences the way vehicles are driven. It’s common to find papers that deal with path detection but don’t take into account major changes in road surface patterns. The quality of the road surface has a direct impact on the comfort and especially on the safety of road users. In emerging countries it’s common to find unpaved roads or roads with no maintenance. Unpaved or damaged roads also impact in higher fuel costs and vehicle maintenance. This kind of analysis can be useful for both road maintenance departments as well as for autonomous vehicle navigation systems to verify potential critical points. For the experiments accomplishment upon the surface types and quality classification, we present a new dataset, collected with a low-cost camera. This dataset has examples of good and bad asphalt (with potholes and other damages) other types of pavement and also many examples of unpaved roads (with and without potholes). We also provide several frames from our dataset manually sorted in surface types for tests accuracy verification. Our road type and quality classifier was done through a simple Convolutional Neural Network with few steps and presents promising results in different datasets.;Road Surface Classification, Road Surface Quality Classification, Road dataset;Graduate Program in Computer Science (PPGCC), Federal University of Santa Catarina (UFSC).Image Processing and Computer Graphics Lab (Lapix) at Brazilian Institute for Digital Convergence (INCoD), Federal University of Santa Catarina (UFSC), Brazil.;en_US;Published;14;2019;2019-04-01 17:44:38
91522;Karla Aparecida Justen;Road Surface Classification with Images Captured From Low-cost Camera - Road Traversing Knowledge (RTK) Dataset;The type of road pavement directly influences the way vehicles are driven. It’s common to find papers that deal with path detection but don’t take into account major changes in road surface patterns. The quality of the road surface has a direct impact on the comfort and especially on the safety of road users. In emerging countries it’s common to find unpaved roads or roads with no maintenance. Unpaved or damaged roads also impact in higher fuel costs and vehicle maintenance. This kind of analysis can be useful for both road maintenance departments as well as for autonomous vehicle navigation systems to verify potential critical points. For the experiments accomplishment upon the surface types and quality classification, we present a new dataset, collected with a low-cost camera. This dataset has examples of good and bad asphalt (with potholes and other damages) other types of pavement and also many examples of unpaved roads (with and without potholes). We also provide several frames from our dataset manually sorted in surface types for tests accuracy verification. Our road type and quality classifier was done through a simple Convolutional Neural Network with few steps and presents promising results in different datasets.;Road Surface Classification, Road Surface Quality Classification, Road dataset;Image Processing and Computer Graphics Lab (Lapix) at Brazilian Institute for Digital Convergence (INCoD), Federal University of Santa Catarina (UFSC), Brazil.;en_US;Published;14;2019;2019-04-01 17:44:38
91522;Aldo von Wangenheim;Road Surface Classification with Images Captured From Low-cost Camera - Road Traversing Knowledge (RTK) Dataset;The type of road pavement directly influences the way vehicles are driven. It’s common to find papers that deal with path detection but don’t take into account major changes in road surface patterns. The quality of the road surface has a direct impact on the comfort and especially on the safety of road users. In emerging countries it’s common to find unpaved roads or roads with no maintenance. Unpaved or damaged roads also impact in higher fuel costs and vehicle maintenance. This kind of analysis can be useful for both road maintenance departments as well as for autonomous vehicle navigation systems to verify potential critical points. For the experiments accomplishment upon the surface types and quality classification, we present a new dataset, collected with a low-cost camera. This dataset has examples of good and bad asphalt (with potholes and other damages) other types of pavement and also many examples of unpaved roads (with and without potholes). We also provide several frames from our dataset manually sorted in surface types for tests accuracy verification. Our road type and quality classifier was done through a simple Convolutional Neural Network with few steps and presents promising results in different datasets.;Road Surface Classification, Road Surface Quality Classification, Road dataset;Graduate Program in Computer Science (PPGCC), Federal University of Santa Catarina (UFSC).Image Processing and Computer Graphics Lab (Lapix) at Brazilian Institute for Digital Convergence (INCoD), Federal University of Santa Catarina (UFSC), Brazil.;en_US;Published;14;2019;2019-04-01 17:44:38
91814;Carlos Melo;Models to evaluate service Provisioning over Cloud Computing Environments - A Blockchain-As-A-Service case study;ThestrictnessoftheServiceLevelAgreements(SLAs)ismainlyduetoasetofconstraintsrelated to performance and dependability attributes, such as availability. This paper shows that system’s availability values may be improved by deploying services over a private environment, which may obtain better availability values with improved management, security, and control. However, how much a company needs to afford to keep this improved availability? As an additional activity, this paper compares the obtained availability values with the infrastructure deployment expenses and establishes a cost × benefit relationship. As for the system’s evaluation technique, we choose modeling while for the service used to demonstrate the models’ feasibility, the blockchain-as-a-service was the selected one. This paper proposes and evaluate four different infrastructures hosting blockchains: (i) baseline (ii) double redundant (iii) triple redundant, and (iv) hyper-converged. The obtained results pointed out that the hyper-converged architecture had an advantage over a full triple redundant environment regarding availability and deployment cost.;Availability, Blockchain-as-a-Service, Hyper-converged, Virtualization;Federal University of Pernambuco;en_US;Published;9;2019;2019-04-12 7:57:39
91814;Jamilson Dantas;Models to evaluate service Provisioning over Cloud Computing Environments - A Blockchain-As-A-Service case study;ThestrictnessoftheServiceLevelAgreements(SLAs)ismainlyduetoasetofconstraintsrelated to performance and dependability attributes, such as availability. This paper shows that system’s availability values may be improved by deploying services over a private environment, which may obtain better availability values with improved management, security, and control. However, how much a company needs to afford to keep this improved availability? As an additional activity, this paper compares the obtained availability values with the infrastructure deployment expenses and establishes a cost × benefit relationship. As for the system’s evaluation technique, we choose modeling while for the service used to demonstrate the models’ feasibility, the blockchain-as-a-service was the selected one. This paper proposes and evaluate four different infrastructures hosting blockchains: (i) baseline (ii) double redundant (iii) triple redundant, and (iv) hyper-converged. The obtained results pointed out that the hyper-converged architecture had an advantage over a full triple redundant environment regarding availability and deployment cost.;Availability, Blockchain-as-a-Service, Hyper-converged, Virtualization;Federal University of Pernambuco;en_US;Published;9;2019;2019-04-12 7:57:39
91814;Ronierison Maciel;Models to evaluate service Provisioning over Cloud Computing Environments - A Blockchain-As-A-Service case study;ThestrictnessoftheServiceLevelAgreements(SLAs)ismainlyduetoasetofconstraintsrelated to performance and dependability attributes, such as availability. This paper shows that system’s availability values may be improved by deploying services over a private environment, which may obtain better availability values with improved management, security, and control. However, how much a company needs to afford to keep this improved availability? As an additional activity, this paper compares the obtained availability values with the infrastructure deployment expenses and establishes a cost × benefit relationship. As for the system’s evaluation technique, we choose modeling while for the service used to demonstrate the models’ feasibility, the blockchain-as-a-service was the selected one. This paper proposes and evaluate four different infrastructures hosting blockchains: (i) baseline (ii) double redundant (iii) triple redundant, and (iv) hyper-converged. The obtained results pointed out that the hyper-converged architecture had an advantage over a full triple redundant environment regarding availability and deployment cost.;Availability, Blockchain-as-a-Service, Hyper-converged, Virtualization;Federal University of Pernambuco;en_US;Published;9;2019;2019-04-12 7:57:39
91814;Paulo Silva;Models to evaluate service Provisioning over Cloud Computing Environments - A Blockchain-As-A-Service case study;ThestrictnessoftheServiceLevelAgreements(SLAs)ismainlyduetoasetofconstraintsrelated to performance and dependability attributes, such as availability. This paper shows that system’s availability values may be improved by deploying services over a private environment, which may obtain better availability values with improved management, security, and control. However, how much a company needs to afford to keep this improved availability? As an additional activity, this paper compares the obtained availability values with the infrastructure deployment expenses and establishes a cost × benefit relationship. As for the system’s evaluation technique, we choose modeling while for the service used to demonstrate the models’ feasibility, the blockchain-as-a-service was the selected one. This paper proposes and evaluate four different infrastructures hosting blockchains: (i) baseline (ii) double redundant (iii) triple redundant, and (iv) hyper-converged. The obtained results pointed out that the hyper-converged architecture had an advantage over a full triple redundant environment regarding availability and deployment cost.;Availability, Blockchain-as-a-Service, Hyper-converged, Virtualization;Federal University of Pernambuco;en_US;Published;9;2019;2019-04-12 7:57:39
91814;Paulo Maciel;Models to evaluate service Provisioning over Cloud Computing Environments - A Blockchain-As-A-Service case study;ThestrictnessoftheServiceLevelAgreements(SLAs)ismainlyduetoasetofconstraintsrelated to performance and dependability attributes, such as availability. This paper shows that system’s availability values may be improved by deploying services over a private environment, which may obtain better availability values with improved management, security, and control. However, how much a company needs to afford to keep this improved availability? As an additional activity, this paper compares the obtained availability values with the infrastructure deployment expenses and establishes a cost × benefit relationship. As for the system’s evaluation technique, we choose modeling while for the service used to demonstrate the models’ feasibility, the blockchain-as-a-service was the selected one. This paper proposes and evaluate four different infrastructures hosting blockchains: (i) baseline (ii) double redundant (iii) triple redundant, and (iv) hyper-converged. The obtained results pointed out that the hyper-converged architecture had an advantage over a full triple redundant environment regarding availability and deployment cost.;Availability, Blockchain-as-a-Service, Hyper-converged, Virtualization;Federal University of Pernambuco;en_US;Published;9;2019;2019-04-12 7:57:39
92117;Christian Moreira Matos;SafeFollowing: A collaborative model for public security agents to assist people with disabilities and the elderly;In the last survey about the Brazilian population conducted by IBGE, more than 45.6 million of Brazilians declared that they have some kind of disability. Besides that, the same survey revealed that the number of elderly people has been increasing over the years and at that time it already represented 12.1 \% of the total. In this scenario, public policies that can efficiently ensure the rights of this share of the population became even more necessary. In the same manner, with the popularization of mobile devices, opportunities to develop new solutions are arising, offering more independence and quality of life to them. Ubiquitous accessibility support solutions have been proposed, such as the MASC model, which resorts to the concept of smart assistive city and more recently the AccompCare, which predicts the monitoring of people with disabilities and the elderly. This article proposes the SafeFollowing, a model that enables integrated and collaborative acting of the community aiming to assist people with some kind of disability or elderly people. The SafeFollowing predicts the use of police cars mapping, in order to provide a specific follow-up in adverse daily situations. The validation of the model is also presented in the article, which was performed through experiments in real test scenarios.;;University of the Sinos Valley;en_US;Published;14;2019;2019-04-23 15:51:54
92117;Vítor Kehl Matter;SafeFollowing: A collaborative model for public security agents to assist people with disabilities and the elderly;In the last survey about the Brazilian population conducted by IBGE, more than 45.6 million of Brazilians declared that they have some kind of disability. Besides that, the same survey revealed that the number of elderly people has been increasing over the years and at that time it already represented 12.1 \% of the total. In this scenario, public policies that can efficiently ensure the rights of this share of the population became even more necessary. In the same manner, with the popularization of mobile devices, opportunities to develop new solutions are arising, offering more independence and quality of life to them. Ubiquitous accessibility support solutions have been proposed, such as the MASC model, which resorts to the concept of smart assistive city and more recently the AccompCare, which predicts the monitoring of people with disabilities and the elderly. This article proposes the SafeFollowing, a model that enables integrated and collaborative acting of the community aiming to assist people with some kind of disability or elderly people. The SafeFollowing predicts the use of police cars mapping, in order to provide a specific follow-up in adverse daily situations. The validation of the model is also presented in the article, which was performed through experiments in real test scenarios.;;University of the Sinos Valley;en_US;Published;14;2019;2019-04-23 15:51:54
92117;Fábio Viegas;SafeFollowing: A collaborative model for public security agents to assist people with disabilities and the elderly;In the last survey about the Brazilian population conducted by IBGE, more than 45.6 million of Brazilians declared that they have some kind of disability. Besides that, the same survey revealed that the number of elderly people has been increasing over the years and at that time it already represented 12.1 \% of the total. In this scenario, public policies that can efficiently ensure the rights of this share of the population became even more necessary. In the same manner, with the popularization of mobile devices, opportunities to develop new solutions are arising, offering more independence and quality of life to them. Ubiquitous accessibility support solutions have been proposed, such as the MASC model, which resorts to the concept of smart assistive city and more recently the AccompCare, which predicts the monitoring of people with disabilities and the elderly. This article proposes the SafeFollowing, a model that enables integrated and collaborative acting of the community aiming to assist people with some kind of disability or elderly people. The SafeFollowing predicts the use of police cars mapping, in order to provide a specific follow-up in adverse daily situations. The validation of the model is also presented in the article, which was performed through experiments in real test scenarios.;;University of the Sinos Valley;en_US;Published;14;2019;2019-04-23 15:51:54
92117;Márcio Garcia Martins;SafeFollowing: A collaborative model for public security agents to assist people with disabilities and the elderly;In the last survey about the Brazilian population conducted by IBGE, more than 45.6 million of Brazilians declared that they have some kind of disability. Besides that, the same survey revealed that the number of elderly people has been increasing over the years and at that time it already represented 12.1 \% of the total. In this scenario, public policies that can efficiently ensure the rights of this share of the population became even more necessary. In the same manner, with the popularization of mobile devices, opportunities to develop new solutions are arising, offering more independence and quality of life to them. Ubiquitous accessibility support solutions have been proposed, such as the MASC model, which resorts to the concept of smart assistive city and more recently the AccompCare, which predicts the monitoring of people with disabilities and the elderly. This article proposes the SafeFollowing, a model that enables integrated and collaborative acting of the community aiming to assist people with some kind of disability or elderly people. The SafeFollowing predicts the use of police cars mapping, in order to provide a specific follow-up in adverse daily situations. The validation of the model is also presented in the article, which was performed through experiments in real test scenarios.;;University of the Sinos Valley;en_US;Published;14;2019;2019-04-23 15:51:54
92117;João Elison Da Rosa Tavares;SafeFollowing: A collaborative model for public security agents to assist people with disabilities and the elderly;In the last survey about the Brazilian population conducted by IBGE, more than 45.6 million of Brazilians declared that they have some kind of disability. Besides that, the same survey revealed that the number of elderly people has been increasing over the years and at that time it already represented 12.1 \% of the total. In this scenario, public policies that can efficiently ensure the rights of this share of the population became even more necessary. In the same manner, with the popularization of mobile devices, opportunities to develop new solutions are arising, offering more independence and quality of life to them. Ubiquitous accessibility support solutions have been proposed, such as the MASC model, which resorts to the concept of smart assistive city and more recently the AccompCare, which predicts the monitoring of people with disabilities and the elderly. This article proposes the SafeFollowing, a model that enables integrated and collaborative acting of the community aiming to assist people with some kind of disability or elderly people. The SafeFollowing predicts the use of police cars mapping, in order to provide a specific follow-up in adverse daily situations. The validation of the model is also presented in the article, which was performed through experiments in real test scenarios.;;University of the Sinos Valley;en_US;Published;14;2019;2019-04-23 15:51:54
92117;Jorge Luis Victória Barbosa;SafeFollowing: A collaborative model for public security agents to assist people with disabilities and the elderly;In the last survey about the Brazilian population conducted by IBGE, more than 45.6 million of Brazilians declared that they have some kind of disability. Besides that, the same survey revealed that the number of elderly people has been increasing over the years and at that time it already represented 12.1 \% of the total. In this scenario, public policies that can efficiently ensure the rights of this share of the population became even more necessary. In the same manner, with the popularization of mobile devices, opportunities to develop new solutions are arising, offering more independence and quality of life to them. Ubiquitous accessibility support solutions have been proposed, such as the MASC model, which resorts to the concept of smart assistive city and more recently the AccompCare, which predicts the monitoring of people with disabilities and the elderly. This article proposes the SafeFollowing, a model that enables integrated and collaborative acting of the community aiming to assist people with some kind of disability or elderly people. The SafeFollowing predicts the use of police cars mapping, in order to provide a specific follow-up in adverse daily situations. The validation of the model is also presented in the article, which was performed through experiments in real test scenarios.;;University of the Sinos Valley;en_US;Published;14;2019;2019-04-23 15:51:54
92486;Flaviano Williams Fernandes;HICOLM: High-Performance Platform of Physical Simulations by Using Low Computational Cost Methods;For decades, computational simulation models have been used by scientists in search for new materials with technological applications in several areas of knowledge. For this, software based on several theoretical-computational models were developed in order to obtain an analysis of the physical properties at atomic levels. The objective of this work is proposing a widely functional software to analyze the physical properties of nanostructures based on carbon and condensed systems using theories of low computational cost. Therefore, a Fortran language computational program called HICOLM was developed, whose theoretical bases are based on two commonly known models (Tight-binding and Molecular Dynamics). The physical properties of condensed systems can be obtained in the thermodynamic equilibrium in several statistical ensembles, and possible to obtain an analysis of the properties of the material and its evolution in the time-dependent on its thermodynamic conditions like temperature and pressure. Moreover, from the tight-binding model, the HICOLM program is also capable of performing a physical analysis of carbon-based nanostructures from the calculation of the material band structure.;Tight-binding, Molecular dynamics, Nanotechnology, Molecular structure;Instituto Federal do Paraná, Campus Irati;en_US;Published;13;2019;2019-05-14 19:29:42
93733;José Gomes Lopes Filho;Traveling Salesman Problem with Optional Bonus Collection, Pickup Time and Passengers;This study introduces a variant of the Traveling Salesman Problem, named Traveling Salesman Problem with Optional Bonus Collection, Pickup Time and Passengers (PCVP-BoTc). It is a variant that incorporates elements of the Prize Collecting Traveling Salesman Problem and Ridesharing into the PCV. The objective is to optimize the revenue of the driver, which selectively defines which delivery or collection tasks to perform along the route. The economic effect of the collection is modeled by a bonus. The model can be applied to the solution of hybrid routing systems with route tasks and solidary transport. The driver, while performing the selected tasks, can give rides to persons who share route costs with him. Passengers are protected by restrictions concerning the maximum value they agree to pay for a ride and maximum travel duration. The activity of collecting the bonus in each locality demands a specific amount of time, affects the route duration, and is interconnected with the embarkment of passengers. Two mathematical formulations are presented for the problem and validated by a computational experiment using a solver. We propose four heuristic algorithms three of them are hybrid metaheuristics. We tested the mathematical formulation implementations for 24 instances and the heuristic algorithms for 48.;Travelling salesman Problem, Travelling salesman Problem with profits, Ridesharing;Universidade Federal do Rio Grande do Norte;en_US;Published;20;2019;2019-06-14 15:05:40
93733;Marco Cesar Goldbarg;Traveling Salesman Problem with Optional Bonus Collection, Pickup Time and Passengers;This study introduces a variant of the Traveling Salesman Problem, named Traveling Salesman Problem with Optional Bonus Collection, Pickup Time and Passengers (PCVP-BoTc). It is a variant that incorporates elements of the Prize Collecting Traveling Salesman Problem and Ridesharing into the PCV. The objective is to optimize the revenue of the driver, which selectively defines which delivery or collection tasks to perform along the route. The economic effect of the collection is modeled by a bonus. The model can be applied to the solution of hybrid routing systems with route tasks and solidary transport. The driver, while performing the selected tasks, can give rides to persons who share route costs with him. Passengers are protected by restrictions concerning the maximum value they agree to pay for a ride and maximum travel duration. The activity of collecting the bonus in each locality demands a specific amount of time, affects the route duration, and is interconnected with the embarkment of passengers. Two mathematical formulations are presented for the problem and validated by a computational experiment using a solver. We propose four heuristic algorithms three of them are hybrid metaheuristics. We tested the mathematical formulation implementations for 24 instances and the heuristic algorithms for 48.;Travelling salesman Problem, Travelling salesman Problem with profits, Ridesharing;-;en_US;Published;20;2019;2019-06-14 15:05:40
93733;Elizabeth Ferreira Gouvêa Goldbarg;Traveling Salesman Problem with Optional Bonus Collection, Pickup Time and Passengers;This study introduces a variant of the Traveling Salesman Problem, named Traveling Salesman Problem with Optional Bonus Collection, Pickup Time and Passengers (PCVP-BoTc). It is a variant that incorporates elements of the Prize Collecting Traveling Salesman Problem and Ridesharing into the PCV. The objective is to optimize the revenue of the driver, which selectively defines which delivery or collection tasks to perform along the route. The economic effect of the collection is modeled by a bonus. The model can be applied to the solution of hybrid routing systems with route tasks and solidary transport. The driver, while performing the selected tasks, can give rides to persons who share route costs with him. Passengers are protected by restrictions concerning the maximum value they agree to pay for a ride and maximum travel duration. The activity of collecting the bonus in each locality demands a specific amount of time, affects the route duration, and is interconnected with the embarkment of passengers. Two mathematical formulations are presented for the problem and validated by a computational experiment using a solver. We propose four heuristic algorithms three of them are hybrid metaheuristics. We tested the mathematical formulation implementations for 24 instances and the heuristic algorithms for 48.;Travelling salesman Problem, Travelling salesman Problem with profits, Ridesharing;-;en_US;Published;20;2019;2019-06-14 15:05:40
93733;Vinícius Araújo Petch;Traveling Salesman Problem with Optional Bonus Collection, Pickup Time and Passengers;This study introduces a variant of the Traveling Salesman Problem, named Traveling Salesman Problem with Optional Bonus Collection, Pickup Time and Passengers (PCVP-BoTc). It is a variant that incorporates elements of the Prize Collecting Traveling Salesman Problem and Ridesharing into the PCV. The objective is to optimize the revenue of the driver, which selectively defines which delivery or collection tasks to perform along the route. The economic effect of the collection is modeled by a bonus. The model can be applied to the solution of hybrid routing systems with route tasks and solidary transport. The driver, while performing the selected tasks, can give rides to persons who share route costs with him. Passengers are protected by restrictions concerning the maximum value they agree to pay for a ride and maximum travel duration. The activity of collecting the bonus in each locality demands a specific amount of time, affects the route duration, and is interconnected with the embarkment of passengers. Two mathematical formulations are presented for the problem and validated by a computational experiment using a solver. We propose four heuristic algorithms three of them are hybrid metaheuristics. We tested the mathematical formulation implementations for 24 instances and the heuristic algorithms for 48.;Travelling salesman Problem, Travelling salesman Problem with profits, Ridesharing;-;en_US;Published;20;2019;2019-06-14 15:05:40
94016;Almir Pereira Guimarães;Performability Evaluation of Voice Services in Converged Networks;In the last years, the transmission of voice services in converged networks has experienced a huge growth. However, there are still some questions considering the ability of these networks to deliver voice services with acceptable quality. In this paper, we applied analytical modeling and simulation to analyze the quality of voice services using a new index, called MOS a , which considers jointly the MOS index and the availability of the subjacent infrastructure. We consider the influence of different CODECs (G.711 and G.729), queuing policies (Priority Queuing and Custom Queuing), and the warm standby redundancy mechanism. Our goal is to analyze the quality of these services by taking into account overloading conditions in different  architectures/scenarios. These scenarios were constructed using the modeling mechanisms Reliability Block Diagram and Stochastic Petri Nets in addition to a discrete event simulator. Experimental results indicate that the G.711 CODEC has a higher sensitivity both in terms of data traffic volume and allocated network resources in relation to the G.729 CODEC.;VoIP, MOS, Availability, Reliability Block Diagram, Stochastic Petri Nets;Federal University of Alagoas;en_US;Published;8;2019;2019-06-26 11:03:17
94016;Dênis da Silva Rodrigues;Performability Evaluation of Voice Services in Converged Networks;In the last years, the transmission of voice services in converged networks has experienced a huge growth. However, there are still some questions considering the ability of these networks to deliver voice services with acceptable quality. In this paper, we applied analytical modeling and simulation to analyze the quality of voice services using a new index, called MOS a , which considers jointly the MOS index and the availability of the subjacent infrastructure. We consider the influence of different CODECs (G.711 and G.729), queuing policies (Priority Queuing and Custom Queuing), and the warm standby redundancy mechanism. Our goal is to analyze the quality of these services by taking into account overloading conditions in different  architectures/scenarios. These scenarios were constructed using the modeling mechanisms Reliability Block Diagram and Stochastic Petri Nets in addition to a discrete event simulator. Experimental results indicate that the G.711 CODEC has a higher sensitivity both in terms of data traffic volume and allocated network resources in relation to the G.729 CODEC.;VoIP, MOS, Availability, Reliability Block Diagram, Stochastic Petri Nets;Federal University of Alagoas;en_US;Published;8;2019;2019-06-26 11:03:17
94016;Bruno Costa e Silva Nogueira;Performability Evaluation of Voice Services in Converged Networks;In the last years, the transmission of voice services in converged networks has experienced a huge growth. However, there are still some questions considering the ability of these networks to deliver voice services with acceptable quality. In this paper, we applied analytical modeling and simulation to analyze the quality of voice services using a new index, called MOS a , which considers jointly the MOS index and the availability of the subjacent infrastructure. We consider the influence of different CODECs (G.711 and G.729), queuing policies (Priority Queuing and Custom Queuing), and the warm standby redundancy mechanism. Our goal is to analyze the quality of these services by taking into account overloading conditions in different  architectures/scenarios. These scenarios were constructed using the modeling mechanisms Reliability Block Diagram and Stochastic Petri Nets in addition to a discrete event simulator. Experimental results indicate that the G.711 CODEC has a higher sensitivity both in terms of data traffic volume and allocated network resources in relation to the G.729 CODEC.;VoIP, MOS, Availability, Reliability Block Diagram, Stochastic Petri Nets;Federal University of Alagoas;en_US;Published;8;2019;2019-06-26 11:03:17
94082;Ygor Alcântara de Medeiros;Prize Collecting Traveling Salesman Problem with Ridesharing;The Prize Collecting Traveling Salesman Problem with Ridesharing is a model that joins elements from the Prize Collecting Traveling Salesman and the collaborative transport. The salesman is the driver of a capacitated vehicle and uses a ridesharing system to minimize travel costs. There are a penalty and a bonus associated with each vertex of a graph, G, that represents the problem. There is also a cost associated with each edge of G. The salesman must choose a subset of vertices to be visited so that the total bonus collection is at least a given a parameter. The length of the tour plus the sum of penalties of all vertices not visited is as small as possible. There is a set of persons demanding rides. The ride request consists of a pickup and a drop off location, a maximum travel duration, and the maximum amount the person agrees to pay. The driver shares the cost associated with each arc in the tour with the passengers in the vehicle. Constraints from ride requests, as well as the capacity of the car, must be satisfied. We present a mathematical formulation for the problem investigated in this study and solve it in an optimization tool. We also present three heuristics that hybridize exact and heuristic methods. These algorithms use a decomposition strategy that other enriched vehicle routing problems can utilize.;Travelling salesman Problem, Ridesharing, Metaheuristic;UFRN;en_US;Published;16;2019;2019-06-28 15:53:43
94082;Marco Cesar Goldbarg;Prize Collecting Traveling Salesman Problem with Ridesharing;The Prize Collecting Traveling Salesman Problem with Ridesharing is a model that joins elements from the Prize Collecting Traveling Salesman and the collaborative transport. The salesman is the driver of a capacitated vehicle and uses a ridesharing system to minimize travel costs. There are a penalty and a bonus associated with each vertex of a graph, G, that represents the problem. There is also a cost associated with each edge of G. The salesman must choose a subset of vertices to be visited so that the total bonus collection is at least a given a parameter. The length of the tour plus the sum of penalties of all vertices not visited is as small as possible. There is a set of persons demanding rides. The ride request consists of a pickup and a drop off location, a maximum travel duration, and the maximum amount the person agrees to pay. The driver shares the cost associated with each arc in the tour with the passengers in the vehicle. Constraints from ride requests, as well as the capacity of the car, must be satisfied. We present a mathematical formulation for the problem investigated in this study and solve it in an optimization tool. We also present three heuristics that hybridize exact and heuristic methods. These algorithms use a decomposition strategy that other enriched vehicle routing problems can utilize.;Travelling salesman Problem, Ridesharing, Metaheuristic;-;en_US;Published;16;2019;2019-06-28 15:53:43
94082;Elizabeth Ferreira Gouvêa Goldbarg;Prize Collecting Traveling Salesman Problem with Ridesharing;The Prize Collecting Traveling Salesman Problem with Ridesharing is a model that joins elements from the Prize Collecting Traveling Salesman and the collaborative transport. The salesman is the driver of a capacitated vehicle and uses a ridesharing system to minimize travel costs. There are a penalty and a bonus associated with each vertex of a graph, G, that represents the problem. There is also a cost associated with each edge of G. The salesman must choose a subset of vertices to be visited so that the total bonus collection is at least a given a parameter. The length of the tour plus the sum of penalties of all vertices not visited is as small as possible. There is a set of persons demanding rides. The ride request consists of a pickup and a drop off location, a maximum travel duration, and the maximum amount the person agrees to pay. The driver shares the cost associated with each arc in the tour with the passengers in the vehicle. Constraints from ride requests, as well as the capacity of the car, must be satisfied. We present a mathematical formulation for the problem investigated in this study and solve it in an optimization tool. We also present three heuristics that hybridize exact and heuristic methods. These algorithms use a decomposition strategy that other enriched vehicle routing problems can utilize.;Travelling salesman Problem, Ridesharing, Metaheuristic;-;en_US;Published;16;2019;2019-06-28 15:53:43
94235;Taison Anderson Bortolin;WebGIS development for base flow separation and recharge estimation;The basic flow rate is characterized by an important hydrological component being responsible for the estimation of the water recharge. Due to the difficulty of measurement, mathematical methods are used to calculate the flow separation. However, when hydrographic analysis is based on long historical series, the use of these methods becomes impracticable, making it necessary to use computational resources. A WebGIS (Web Geographical Information System) was developed for data selection and calculation of base flow separation, based on hydrological data from fluviometric stations located in the Taquari-Antas basin, located in the state of Rio Grande do Sul. A modified version of the Unified Process was used as a software development methodology. We used the MVC software architecture standard and the programming languages PHP 7.0, HTML5, JS and CSS3 for programmatic development of the constituent layers of the system. The hydrological data comes from the HIDROWEB portal, part of the National Information System on Water Resources (SNIRH), with hydrological information collected by the National Hydrometeorological Network (RHN) coordinated by the National Water Agency (ANA). The system facilitates the use of remote and distributed hydrological data, shared over the Internet, for various hydrological analyzes.;Separation of the Base Flow, WebGIS, Base flow, Water recharge;University of Caxias do Sul;en_US;Published;9;2019;2019-07-03 18:22:44
94235;Lucas Moraes dos Santos;WebGIS development for base flow separation and recharge estimation;The basic flow rate is characterized by an important hydrological component being responsible for the estimation of the water recharge. Due to the difficulty of measurement, mathematical methods are used to calculate the flow separation. However, when hydrographic analysis is based on long historical series, the use of these methods becomes impracticable, making it necessary to use computational resources. A WebGIS (Web Geographical Information System) was developed for data selection and calculation of base flow separation, based on hydrological data from fluviometric stations located in the Taquari-Antas basin, located in the state of Rio Grande do Sul. A modified version of the Unified Process was used as a software development methodology. We used the MVC software architecture standard and the programming languages PHP 7.0, HTML5, JS and CSS3 for programmatic development of the constituent layers of the system. The hydrological data comes from the HIDROWEB portal, part of the National Information System on Water Resources (SNIRH), with hydrological information collected by the National Hydrometeorological Network (RHN) coordinated by the National Water Agency (ANA). The system facilitates the use of remote and distributed hydrological data, shared over the Internet, for various hydrological analyzes.;Separation of the Base Flow, WebGIS, Base flow, Water recharge;University of Caxias do Sul;en_US;Published;9;2019;2019-07-03 18:22:44
94235;Adriano Gomes da Silva;WebGIS development for base flow separation and recharge estimation;The basic flow rate is characterized by an important hydrological component being responsible for the estimation of the water recharge. Due to the difficulty of measurement, mathematical methods are used to calculate the flow separation. However, when hydrographic analysis is based on long historical series, the use of these methods becomes impracticable, making it necessary to use computational resources. A WebGIS (Web Geographical Information System) was developed for data selection and calculation of base flow separation, based on hydrological data from fluviometric stations located in the Taquari-Antas basin, located in the state of Rio Grande do Sul. A modified version of the Unified Process was used as a software development methodology. We used the MVC software architecture standard and the programming languages PHP 7.0, HTML5, JS and CSS3 for programmatic development of the constituent layers of the system. The hydrological data comes from the HIDROWEB portal, part of the National Information System on Water Resources (SNIRH), with hydrological information collected by the National Hydrometeorological Network (RHN) coordinated by the National Water Agency (ANA). The system facilitates the use of remote and distributed hydrological data, shared over the Internet, for various hydrological analyzes.;Separation of the Base Flow, WebGIS, Base flow, Water recharge;University of Caxias do Sul;en_US;Published;9;2019;2019-07-03 18:22:44
94235;Vania Elisabete Schneider;WebGIS development for base flow separation and recharge estimation;The basic flow rate is characterized by an important hydrological component being responsible for the estimation of the water recharge. Due to the difficulty of measurement, mathematical methods are used to calculate the flow separation. However, when hydrographic analysis is based on long historical series, the use of these methods becomes impracticable, making it necessary to use computational resources. A WebGIS (Web Geographical Information System) was developed for data selection and calculation of base flow separation, based on hydrological data from fluviometric stations located in the Taquari-Antas basin, located in the state of Rio Grande do Sul. A modified version of the Unified Process was used as a software development methodology. We used the MVC software architecture standard and the programming languages PHP 7.0, HTML5, JS and CSS3 for programmatic development of the constituent layers of the system. The hydrological data comes from the HIDROWEB portal, part of the National Information System on Water Resources (SNIRH), with hydrological information collected by the National Hydrometeorological Network (RHN) coordinated by the National Water Agency (ANA). The system facilitates the use of remote and distributed hydrological data, shared over the Internet, for various hydrological analyzes.;Separation of the Base Flow, WebGIS, Base flow, Water recharge;University of Caxias do Sul;en_US;Published;9;2019;2019-07-03 18:22:44
94319;Giovani Farias;Water Resources Analysis: An Approach based on Agent-Based Modeling;The paper aims to present a river basin modeling using GAMA platform for water resources analysis. Currently, several models based on multi-agent systems are used for natural resources management and they present satisfactory results for this type of scenario. GAMA is agents based and widely used in this context with several studies already published. In this study, the Sa ̃o Gonc ̧alo and Lagoa Mirim basins were considered from georeferenced data. In the modeling, regions, and rivers are agents on the system where rivers water can flow among neighbors regions.;Multi-Agent Systems, Natural Resources, Agent-Based Model, GAMA;FURG;en_US;Published;14;2019;2019-07-07 16:20:27
94319;Bruna Leitzke;Water Resources Analysis: An Approach based on Agent-Based Modeling;The paper aims to present a river basin modeling using GAMA platform for water resources analysis. Currently, several models based on multi-agent systems are used for natural resources management and they present satisfactory results for this type of scenario. GAMA is agents based and widely used in this context with several studies already published. In this study, the Sa ̃o Gonc ̧alo and Lagoa Mirim basins were considered from georeferenced data. In the modeling, regions, and rivers are agents on the system where rivers water can flow among neighbors regions.;Multi-Agent Systems, Natural Resources, Agent-Based Model, GAMA;FURG;en_US;Published;14;2019;2019-07-07 16:20:27
94319;Míriam Born;Water Resources Analysis: An Approach based on Agent-Based Modeling;The paper aims to present a river basin modeling using GAMA platform for water resources analysis. Currently, several models based on multi-agent systems are used for natural resources management and they present satisfactory results for this type of scenario. GAMA is agents based and widely used in this context with several studies already published. In this study, the Sa ̃o Gonc ̧alo and Lagoa Mirim basins were considered from georeferenced data. In the modeling, regions, and rivers are agents on the system where rivers water can flow among neighbors regions.;Multi-Agent Systems, Natural Resources, Agent-Based Model, GAMA;UFPel;en_US;Published;14;2019;2019-07-07 16:20:27
94319;Marilton Aguiar;Water Resources Analysis: An Approach based on Agent-Based Modeling;The paper aims to present a river basin modeling using GAMA platform for water resources analysis. Currently, several models based on multi-agent systems are used for natural resources management and they present satisfactory results for this type of scenario. GAMA is agents based and widely used in this context with several studies already published. In this study, the Sa ̃o Gonc ̧alo and Lagoa Mirim basins were considered from georeferenced data. In the modeling, regions, and rivers are agents on the system where rivers water can flow among neighbors regions.;Multi-Agent Systems, Natural Resources, Agent-Based Model, GAMA;UFPel;en_US;Published;14;2019;2019-07-07 16:20:27
94319;Diana Adamatti;Water Resources Analysis: An Approach based on Agent-Based Modeling;The paper aims to present a river basin modeling using GAMA platform for water resources analysis. Currently, several models based on multi-agent systems are used for natural resources management and they present satisfactory results for this type of scenario. GAMA is agents based and widely used in this context with several studies already published. In this study, the Sa ̃o Gonc ̧alo and Lagoa Mirim basins were considered from georeferenced data. In the modeling, regions, and rivers are agents on the system where rivers water can flow among neighbors regions.;Multi-Agent Systems, Natural Resources, Agent-Based Model, GAMA;FURG;en_US;Published;14;2019;2019-07-07 16:20:27
94845;Luís Gustavo Ludescher;Effects of reward distribution strategies and perseverance profiles on agent-based coalitions dynamics;In a conventional political system, leaders decide how to distribute benefits to the population and coalitions can emerge when other individuals support the candidates. This work intends to analyze how different leader strategies and individual profiles affect the way coalitions are formed and rewards are shared. Using agent-based simulation, we simulated a model in which individuals of three different perseverance profiles (patient, intermediate and impatient) eventually decide to be part of coalitions by supporting certain leaders when aiming to maximize their own earnings. Leaders can follow one of three different strategies to share rewards: altruistic, intermediate and egoistic. The results show that egoistic leaders stimulate the competition for rewards and the formation of coalitions, causing greater inequalities, while impatient individuals also promote more instability and lead to a higher concentration of rewards.;Agent-based simulation, Coalitions, Public welfare, Inequality;Escola Politécnica da Universidade de São Paulo;en_US;Published;10;2019;2019-07-25 18:34:42
94845;Jaime Simão Sichman;Effects of reward distribution strategies and perseverance profiles on agent-based coalitions dynamics;In a conventional political system, leaders decide how to distribute benefits to the population and coalitions can emerge when other individuals support the candidates. This work intends to analyze how different leader strategies and individual profiles affect the way coalitions are formed and rewards are shared. Using agent-based simulation, we simulated a model in which individuals of three different perseverance profiles (patient, intermediate and impatient) eventually decide to be part of coalitions by supporting certain leaders when aiming to maximize their own earnings. Leaders can follow one of three different strategies to share rewards: altruistic, intermediate and egoistic. The results show that egoistic leaders stimulate the competition for rewards and the formation of coalitions, causing greater inequalities, while impatient individuals also promote more instability and lead to a higher concentration of rewards.;Agent-based simulation, Coalitions, Public welfare, Inequality;Escola Politécnica da Universidade de São Paulo;en_US;Published;10;2019;2019-07-25 18:34:42
95095;Lucas Silveira Melo;Mosaik and PADE: Multiagents and Co-simulation for smart grids modeling;This paper describes the integration process between two tools in order to perform co-simulation for representation and analysis of dynamic environments in the context of smart grids. The integrated tools are Mosaik, a software to co-simulation management, and PADE, a software to multi-agent systems development. As a study case for demonstrate the integration, a scenary was utilized composed of a low voltage electricity distribution grid with 37 load bus, 20 photo-voltaic distributed generations, randomly connected to load bus, as well as, 20 PADE agents associated to distributed generation, modeling the behavior of electricity storage systems. The simulation results show the integration happening and demonstrate how useful is to model the dynamics of distributed electric resources with multi-agent systems.;multi-agent system, co-simulation, smart grids, distributed energetic resources;Federal University of Ceará;en_US;Published;8;2019;2019-07-31 21:00:14
95095;Filipe Saraiva;Mosaik and PADE: Multiagents and Co-simulation for smart grids modeling;This paper describes the integration process between two tools in order to perform co-simulation for representation and analysis of dynamic environments in the context of smart grids. The integrated tools are Mosaik, a software to co-simulation management, and PADE, a software to multi-agent systems development. As a study case for demonstrate the integration, a scenary was utilized composed of a low voltage electricity distribution grid with 37 load bus, 20 photo-voltaic distributed generations, randomly connected to load bus, as well as, 20 PADE agents associated to distributed generation, modeling the behavior of electricity storage systems. The simulation results show the integration happening and demonstrate how useful is to model the dynamics of distributed electric resources with multi-agent systems.;multi-agent system, co-simulation, smart grids, distributed energetic resources;-;en_US;Published;8;2019;2019-07-31 21:00:14
95095;Ruth Leão;Mosaik and PADE: Multiagents and Co-simulation for smart grids modeling;This paper describes the integration process between two tools in order to perform co-simulation for representation and analysis of dynamic environments in the context of smart grids. The integrated tools are Mosaik, a software to co-simulation management, and PADE, a software to multi-agent systems development. As a study case for demonstrate the integration, a scenary was utilized composed of a low voltage electricity distribution grid with 37 load bus, 20 photo-voltaic distributed generations, randomly connected to load bus, as well as, 20 PADE agents associated to distributed generation, modeling the behavior of electricity storage systems. The simulation results show the integration happening and demonstrate how useful is to model the dynamics of distributed electric resources with multi-agent systems.;multi-agent system, co-simulation, smart grids, distributed energetic resources;-;en_US;Published;8;2019;2019-07-31 21:00:14
95095;Raimundo Furtado Sampaio;Mosaik and PADE: Multiagents and Co-simulation for smart grids modeling;This paper describes the integration process between two tools in order to perform co-simulation for representation and analysis of dynamic environments in the context of smart grids. The integrated tools are Mosaik, a software to co-simulation management, and PADE, a software to multi-agent systems development. As a study case for demonstrate the integration, a scenary was utilized composed of a low voltage electricity distribution grid with 37 load bus, 20 photo-voltaic distributed generations, randomly connected to load bus, as well as, 20 PADE agents associated to distributed generation, modeling the behavior of electricity storage systems. The simulation results show the integration happening and demonstrate how useful is to model the dynamics of distributed electric resources with multi-agent systems.;multi-agent system, co-simulation, smart grids, distributed energetic resources;-;en_US;Published;8;2019;2019-07-31 21:00:14
95095;Giovanni Cordeiro Barroso;Mosaik and PADE: Multiagents and Co-simulation for smart grids modeling;This paper describes the integration process between two tools in order to perform co-simulation for representation and analysis of dynamic environments in the context of smart grids. The integrated tools are Mosaik, a software to co-simulation management, and PADE, a software to multi-agent systems development. As a study case for demonstrate the integration, a scenary was utilized composed of a low voltage electricity distribution grid with 37 load bus, 20 photo-voltaic distributed generations, randomly connected to load bus, as well as, 20 PADE agents associated to distributed generation, modeling the behavior of electricity storage systems. The simulation results show the integration happening and demonstrate how useful is to model the dynamics of distributed electric resources with multi-agent systems.;multi-agent system, co-simulation, smart grids, distributed energetic resources;-;en_US;Published;8;2019;2019-07-31 21:00:14
96081;Marcos de Souza Oliveira;Unsupervised Feature Selection Methodology for Clustering in High Dimensionality Datasets;Feature selection is an important research area that seeks to eliminate unwanted features from datasets. Many feature selection methods are suggested in the literature, but the evaluation of the best set of features is usually performed using supervised metrics, where labels are required. In this work we propose a methodology that tries to aid data specialists to answer simple but important questions, such as: (1) do current feature selection methods give similar results? (2) is there is a consistently better method ? (3) how to select the m-best features? (4) as the methods are not parameter-free, how to choose the best parameters in the unsupervised scenario? and (5) given different options of selection, could we get better results if we fusion the results of the methods? If yes, how can we combine the results? We analyze these issues and propose a methodology that, based on some unsupervised methods, will make feature selection using strategies that turn the execution of the process fully automatic and unsupervised, in high-dimensional datasets. After, we evaluate the obtained results, when we see that they are better than those obtained by using the selection methods at standard configurations. In the end, we also list some further improvements that can be made in future works.;Feature Selection, Clustering, Dimensionality Reduction, Unsupervised Learning;Universidade Federal de Pernambuco;en_US;Published;11;2019;2019-09-01 1:07:26
96081;Sergio Queiroz;Unsupervised Feature Selection Methodology for Clustering in High Dimensionality Datasets;Feature selection is an important research area that seeks to eliminate unwanted features from datasets. Many feature selection methods are suggested in the literature, but the evaluation of the best set of features is usually performed using supervised metrics, where labels are required. In this work we propose a methodology that tries to aid data specialists to answer simple but important questions, such as: (1) do current feature selection methods give similar results? (2) is there is a consistently better method ? (3) how to select the m-best features? (4) as the methods are not parameter-free, how to choose the best parameters in the unsupervised scenario? and (5) given different options of selection, could we get better results if we fusion the results of the methods? If yes, how can we combine the results? We analyze these issues and propose a methodology that, based on some unsupervised methods, will make feature selection using strategies that turn the execution of the process fully automatic and unsupervised, in high-dimensional datasets. After, we evaluate the obtained results, when we see that they are better than those obtained by using the selection methods at standard configurations. In the end, we also list some further improvements that can be made in future works.;Feature Selection, Clustering, Dimensionality Reduction, Unsupervised Learning;Universidade Federal de Pernambuco;en_US;Published;11;2019;2019-09-01 1:07:26
96181;Ademir Batista Santos Neto;Studying the Performance of Cognitive Models in Time Series Forecasting;Cognitive models have been paramount for modeling phenomena for which empirical data are unavailable, scarce, or only partially relevant. These approaches are based on methods dedicated to preparing experts and then to elicit their opinions about the variables that describe the phenomena under study. In time series forecasting exercises, elicitation processes seek to obtain accurate estimates, overcoming human heuristic biases, while being less time consuming. This paper aims to compare the performance of cognitive and mathematical time series predictors, regarding accuracy. The results are based on the comparison of predictors of the cognitive and mathematical models for several time series from the M3-Competition. From the results, one can see that cognitive models are, at least, as accurate as ARIMA models predictions.;Cognitive  Models, ARIMA  Models, Elicitation  of  Knowledge, Time  Series  Forecasting;Univerisdade Federal Rural de Pernambuco;en_US;Published;8;2019;2019-09-03 13:28:33
96181;Tiago Alessandro Espindola Ferreira;Studying the Performance of Cognitive Models in Time Series Forecasting;Cognitive models have been paramount for modeling phenomena for which empirical data are unavailable, scarce, or only partially relevant. These approaches are based on methods dedicated to preparing experts and then to elicit their opinions about the variables that describe the phenomena under study. In time series forecasting exercises, elicitation processes seek to obtain accurate estimates, overcoming human heuristic biases, while being less time consuming. This paper aims to compare the performance of cognitive and mathematical time series predictors, regarding accuracy. The results are based on the comparison of predictors of the cognitive and mathematical models for several time series from the M3-Competition. From the results, one can see that cognitive models are, at least, as accurate as ARIMA models predictions.;Cognitive  Models, ARIMA  Models, Elicitation  of  Knowledge, Time  Series  Forecasting;Univerisdade Federal Rural de Pernambuco;en_US;Published;8;2019;2019-09-03 13:28:33
96181;Maria Conceição Moraes Batista;Studying the Performance of Cognitive Models in Time Series Forecasting;Cognitive models have been paramount for modeling phenomena for which empirical data are unavailable, scarce, or only partially relevant. These approaches are based on methods dedicated to preparing experts and then to elicit their opinions about the variables that describe the phenomena under study. In time series forecasting exercises, elicitation processes seek to obtain accurate estimates, overcoming human heuristic biases, while being less time consuming. This paper aims to compare the performance of cognitive and mathematical time series predictors, regarding accuracy. The results are based on the comparison of predictors of the cognitive and mathematical models for several time series from the M3-Competition. From the results, one can see that cognitive models are, at least, as accurate as ARIMA models predictions.;Cognitive  Models, ARIMA  Models, Elicitation  of  Knowledge, Time  Series  Forecasting;Univerisdade Federal Rural de Pernambuco;en_US;Published;8;2019;2019-09-03 13:28:33
96181;Paulo Renato Alves Firmino;Studying the Performance of Cognitive Models in Time Series Forecasting;Cognitive models have been paramount for modeling phenomena for which empirical data are unavailable, scarce, or only partially relevant. These approaches are based on methods dedicated to preparing experts and then to elicit their opinions about the variables that describe the phenomena under study. In time series forecasting exercises, elicitation processes seek to obtain accurate estimates, overcoming human heuristic biases, while being less time consuming. This paper aims to compare the performance of cognitive and mathematical time series predictors, regarding accuracy. The results are based on the comparison of predictors of the cognitive and mathematical models for several time series from the M3-Competition. From the results, one can see that cognitive models are, at least, as accurate as ARIMA models predictions.;Cognitive  Models, ARIMA  Models, Elicitation  of  Knowledge, Time  Series  Forecasting;Universidade Federal do Cariri;en_US;Published;8;2019;2019-09-03 13:28:33
97411;Saulo Popov Zambiasi;Arisa Nest – A Cloud-Based Platform for Development of Virtual Assistant;The use of virtual assistants has become popularized by their adhesion by large companies. This is because they have brought benefits to a certain extent in people’s daily activities or in their tasks in organizations, with reminders, automation of repetitive tasks, proposing solutions for various situations, etc.. To meet this demand, several tools have been developed to create this type of agent. Following the same trends, this paper presents the Arisa Nest platform, under the Platform as a Service model, as a tool to create virtual assistants with resources to manage information about users, create algorithms to give more effective answers, consume web services to the interoperability with other systems and with proactivity behaviors. Finally, some cases are presented using the platform in real scenarios and tests.;Virtual Assistants, Cloud Computing, Chatbots, Platform;Universidade do Sul de Santa Catarina;en_US;Published;10;2019;2019-10-15 15:00:37
97411;Ricardo J. Rabelo;Arisa Nest – A Cloud-Based Platform for Development of Virtual Assistant;The use of virtual assistants has become popularized by their adhesion by large companies. This is because they have brought benefits to a certain extent in people’s daily activities or in their tasks in organizations, with reminders, automation of repetitive tasks, proposing solutions for various situations, etc.. To meet this demand, several tools have been developed to create this type of agent. Following the same trends, this paper presents the Arisa Nest platform, under the Platform as a Service model, as a tool to create virtual assistants with resources to manage information about users, create algorithms to give more effective answers, consume web services to the interoperability with other systems and with proactivity behaviors. Finally, some cases are presented using the platform in real scenarios and tests.;Virtual Assistants, Cloud Computing, Chatbots, Platform;UFSC;en_US;Published;10;2019;2019-10-15 15:00:37
97479;Italo Ramon da Costa Campos;Reactive multi-agent system applied to self-healing in Smart Grids;This paper presents a decentralized algorithm for application in the smart grids self-healing problem, at the distribution level. The algorithm implementation is made using a reactive multi-agent system, which models the electrical grid in terms of autonomous agents which perform the algorithm operations in a distributed and parallel way. To validate this algorithm, two distribution network test models are used: a 15 bus model and a 33 bus model — standardized by IEEE. The results are obtained by means of computational simulation and shown in this paper, to each one of the network models. The results show that the proposed approach is able to recover all the nodes of the grid, within the simulation conditions. Moreover, it is seen that the multi-agent system directs the work load exactly to the failure point, preventing the involvement of the entire grid to the self-healing process.;Discentralized algorithm, Multiagent system, Self-healing, Smart grids;Universidade Federal do Pará;en_US;Published;12;2019;2019-10-18 10:54:29
97479;Filipe Saraiva;Reactive multi-agent system applied to self-healing in Smart Grids;This paper presents a decentralized algorithm for application in the smart grids self-healing problem, at the distribution level. The algorithm implementation is made using a reactive multi-agent system, which models the electrical grid in terms of autonomous agents which perform the algorithm operations in a distributed and parallel way. To validate this algorithm, two distribution network test models are used: a 15 bus model and a 33 bus model — standardized by IEEE. The results are obtained by means of computational simulation and shown in this paper, to each one of the network models. The results show that the proposed approach is able to recover all the nodes of the grid, within the simulation conditions. Moreover, it is seen that the multi-agent system directs the work load exactly to the failure point, preventing the involvement of the entire grid to the self-healing process.;Discentralized algorithm, Multiagent system, Self-healing, Smart grids;Universidade Federal do Pará;en_US;Published;12;2019;2019-10-18 10:54:29
97835;Paulo César Pereira Júnior;Comparison of Classical Computer Vision vs. Convolutional Neural Networks for Weed Mapping in Aerial Images;In this paper, we present a comparison between convolutional neural networks and classicalcomputer vision approaches, for the specific precision agriculture problem of weed mapping on sugarcane fields aerial images. A systematic literature review was conducted to find which computer vision methods are being used on this specific problem. The most cited methods were implemented, as well as four models of convolutional neural networks. All implemented approaches were tested using the same dataset, and their results were quantitatively and qualitatively analyzed. The obtained results were compared to a human expert made ground truth, for validation. The results indicate that the convolutional neural networks present better precision and generalize better than the classical models;Convolutional Neural Networks, Deep Learning, Digital Image Processing, Precision Agriculture, Semantic Segmentation, Unmanned Aerial Vehicles;Federal University of Santa Catarina;en_US;Published;13;2019;2019-11-01 15:25:03
97835;Alexandre Monteiro;Comparison of Classical Computer Vision vs. Convolutional Neural Networks for Weed Mapping in Aerial Images;In this paper, we present a comparison between convolutional neural networks and classicalcomputer vision approaches, for the specific precision agriculture problem of weed mapping on sugarcane fields aerial images. A systematic literature review was conducted to find which computer vision methods are being used on this specific problem. The most cited methods were implemented, as well as four models of convolutional neural networks. All implemented approaches were tested using the same dataset, and their results were quantitatively and qualitatively analyzed. The obtained results were compared to a human expert made ground truth, for validation. The results indicate that the convolutional neural networks present better precision and generalize better than the classical models;Convolutional Neural Networks, Deep Learning, Digital Image Processing, Precision Agriculture, Semantic Segmentation, Unmanned Aerial Vehicles;Federal University of Santa Catarina;en_US;Published;13;2019;2019-11-01 15:25:03
97835;Rafael da Luz Ribeiro;Comparison of Classical Computer Vision vs. Convolutional Neural Networks for Weed Mapping in Aerial Images;In this paper, we present a comparison between convolutional neural networks and classicalcomputer vision approaches, for the specific precision agriculture problem of weed mapping on sugarcane fields aerial images. A systematic literature review was conducted to find which computer vision methods are being used on this specific problem. The most cited methods were implemented, as well as four models of convolutional neural networks. All implemented approaches were tested using the same dataset, and their results were quantitatively and qualitatively analyzed. The obtained results were compared to a human expert made ground truth, for validation. The results indicate that the convolutional neural networks present better precision and generalize better than the classical models;Convolutional Neural Networks, Deep Learning, Digital Image Processing, Precision Agriculture, Semantic Segmentation, Unmanned Aerial Vehicles;Federal University of Santa Catarina;en_US;Published;13;2019;2019-11-01 15:25:03
97835;Antonio Carlos Sobieranski;Comparison of Classical Computer Vision vs. Convolutional Neural Networks for Weed Mapping in Aerial Images;In this paper, we present a comparison between convolutional neural networks and classicalcomputer vision approaches, for the specific precision agriculture problem of weed mapping on sugarcane fields aerial images. A systematic literature review was conducted to find which computer vision methods are being used on this specific problem. The most cited methods were implemented, as well as four models of convolutional neural networks. All implemented approaches were tested using the same dataset, and their results were quantitatively and qualitatively analyzed. The obtained results were compared to a human expert made ground truth, for validation. The results indicate that the convolutional neural networks present better precision and generalize better than the classical models;Convolutional Neural Networks, Deep Learning, Digital Image Processing, Precision Agriculture, Semantic Segmentation, Unmanned Aerial Vehicles;Federal University of Santa Catarina;en_US;Published;13;2019;2019-11-01 15:25:03
97835;Aldo von Wangenheim;Comparison of Classical Computer Vision vs. Convolutional Neural Networks for Weed Mapping in Aerial Images;In this paper, we present a comparison between convolutional neural networks and classicalcomputer vision approaches, for the specific precision agriculture problem of weed mapping on sugarcane fields aerial images. A systematic literature review was conducted to find which computer vision methods are being used on this specific problem. The most cited methods were implemented, as well as four models of convolutional neural networks. All implemented approaches were tested using the same dataset, and their results were quantitatively and qualitatively analyzed. The obtained results were compared to a human expert made ground truth, for validation. The results indicate that the convolutional neural networks present better precision and generalize better than the classical models;Convolutional Neural Networks, Deep Learning, Digital Image Processing, Precision Agriculture, Semantic Segmentation, Unmanned Aerial Vehicles;Federal University of Santa Catarina;en_US;Published;13;2019;2019-11-01 15:25:03
98362;Guilherme Bayer Schneider;Int-FLBCC: Model for Load Balancing in Cloud Computing using Fuzzy Logic Type-2 and Admissible Orders.;Dynamic consolidation of virtual machines (VMs) is an effective way to improve resource utilization and power efficiency in cloud computing, directly affecting Quality of Service aspects. This paper presents Int-FLBCC, a new proposal with exploring a Type-2 Fuzzy Logic approach to address the uncertainties and inaccuracies in determining resource usage, aiming at energy savings with minimal performance degradation. Validation results in a simulated cloud computing environment showed improvements in energy efficiency of 8.83% with IQR_XY and 22.43% with MAD_XY. For fulfillment of Service Level Agreements (SLA), the best values achieved were 9.06% with MAD_XY and 25% of THR_Lex1.;Fuzzy logic, Uncertainty, Cloud computing, Resource management, Data centers, Random access memory, Quality of service;Centro de Desenvolvimento Tecnológico (CDTEC), Universidade Federal de Pelotas (UFPel), Pelotas, Rio Grande do Sul, Brasil;en_US;Published;15;2019;2019-11-20 12:38:49
98362;Bruno Moura Paz de Moura;Int-FLBCC: Model for Load Balancing in Cloud Computing using Fuzzy Logic Type-2 and Admissible Orders.;Dynamic consolidation of virtual machines (VMs) is an effective way to improve resource utilization and power efficiency in cloud computing, directly affecting Quality of Service aspects. This paper presents Int-FLBCC, a new proposal with exploring a Type-2 Fuzzy Logic approach to address the uncertainties and inaccuracies in determining resource usage, aiming at energy savings with minimal performance degradation. Validation results in a simulated cloud computing environment showed improvements in energy efficiency of 8.83% with IQR_XY and 22.43% with MAD_XY. For fulfillment of Service Level Agreements (SLA), the best values achieved were 9.06% with MAD_XY and 25% of THR_Lex1.;Fuzzy logic, Uncertainty, Cloud computing, Resource management, Data centers, Random access memory, Quality of service;Centro de Desenvolvimento Tecnológico (CDTEC), Universidade Federal de Pelotas (UFPel), Pelotas, Rio Grande do Sul, Brasil;en_US;Published;15;2019;2019-11-20 12:38:49
98362;Adenauer C Yamin;Int-FLBCC: Model for Load Balancing in Cloud Computing using Fuzzy Logic Type-2 and Admissible Orders.;Dynamic consolidation of virtual machines (VMs) is an effective way to improve resource utilization and power efficiency in cloud computing, directly affecting Quality of Service aspects. This paper presents Int-FLBCC, a new proposal with exploring a Type-2 Fuzzy Logic approach to address the uncertainties and inaccuracies in determining resource usage, aiming at energy savings with minimal performance degradation. Validation results in a simulated cloud computing environment showed improvements in energy efficiency of 8.83% with IQR_XY and 22.43% with MAD_XY. For fulfillment of Service Level Agreements (SLA), the best values achieved were 9.06% with MAD_XY and 25% of THR_Lex1.;Fuzzy logic, Uncertainty, Cloud computing, Resource management, Data centers, Random access memory, Quality of service;Centro de Desenvolvimento Tecnológico (CDTEC), Universidade Federal de Pelotas (UFPel), Pelotas, Rio Grande do Sul, Brasil;en_US;Published;15;2019;2019-11-20 12:38:49
98362;Renata Hax Sander Reiser;Int-FLBCC: Model for Load Balancing in Cloud Computing using Fuzzy Logic Type-2 and Admissible Orders.;Dynamic consolidation of virtual machines (VMs) is an effective way to improve resource utilization and power efficiency in cloud computing, directly affecting Quality of Service aspects. This paper presents Int-FLBCC, a new proposal with exploring a Type-2 Fuzzy Logic approach to address the uncertainties and inaccuracies in determining resource usage, aiming at energy savings with minimal performance degradation. Validation results in a simulated cloud computing environment showed improvements in energy efficiency of 8.83% with IQR_XY and 22.43% with MAD_XY. For fulfillment of Service Level Agreements (SLA), the best values achieved were 9.06% with MAD_XY and 25% of THR_Lex1.;Fuzzy logic, Uncertainty, Cloud computing, Resource management, Data centers, Random access memory, Quality of service;Centro de Desenvolvimento Tecnológico (CDTEC), Universidade Federal de Pelotas (UFPel), Pelotas, Rio Grande do Sul, Brasil;en_US;Published;15;2019;2019-11-20 12:38:49
98367;Ana Lúcia Cetertich Bazzan;Similar Yet Different: the Structure of Social Networks of Characters in Seinfeld, Friends, How I met Your Mother, and The Big Bang Theory;Networktheoryhasbeenusedtoanalyzestructuresofnarrativesinworksoffiction.Indeed,previous works have shed light on issues related to role detection, for instance. However, few comparative works exist that deal with TV shows. Since these shows are very popular, there are several Internet forums that suggest how similar some of them are, mostly by comparing roles or importance of core characters. Is this popular intuition backed by an objective, numerical analysis using tools from network theory? The goal of this paper is to compare four situation comedies (Seinfeld, Friends, How I Met Your Mother, and The Big Bang Theory) that share a lot in common since their characters are friends living in similar, urban, environments, struggling with their daily lives, careers, and so on. Using tools for analyzing social networks, these shows were compared, showing that their structures and the roles of the core characters are fundamentally different. The only measure that proved to be similar among the four shows is entropy of their graphs, especially when computed over the degree distribution.;Fictional characters social network, Centrality measures, Entropy in Networks, TV series;Universidade Federal do Rio Grande do Sul;en_US;Published;14;2019;2019-11-21 8:54:35
98369;Andre Tavares da Silva;DenseNet-DC: Optimizing DenseNet Parameters Through Feature Map Generation Control;Convolutional Neural Networks still suffer from the need for great computational power, oftenrestricting their use on various platforms. Therefore, we propose a new optimization method made for DenseNet, a convolutional neural network that has the characteristic of being completely connected. The objective of the method is to control the generation of the characteristic maps in relation to the moment the network is in, aiming to reduce the size of the network with the minimum of loss in accuracy. This control occurs reducing the number of feature maps through the addition of a new parameter called the Decrease Control or dc value, where the decrease occurs from half of the layers. In order to validate the behavior of the proposed model, experiments were performed using different image bases: MNIST, Fashion-MNIST, CIFAR-10, CIFAR-100, CALTECH-101, Cats vs Dogs and TinyImageNet. Some of the results achieved were: for the MNIST and Fashion-MNIST base, there was 43% parameter reduction. For the CIFAR-10 base achieved a 44% reduction in network parameters, while in base CIFAR-100 the parameter reduction are 43%. In the CALTECH-101 base the parameter optimization was 35%, while the Cats vs Dogs optimized 30% of model parameters. Finally, the TinyImageNet base was reduced 31% of the parameters.;CNN, DenseNet, Optimization;Universidade do Estado de Santa Catarina, Centro de Ciências Tecnológicas. Rua Paulo Malschitzki, s/numero - Campus Universitário Prof. Avelino MarcanteZona Industrial Norte89219-710 - Joinville, SC - BrasilTelefone: (47) 40097900Ramal: 7832Fax: (47) 40097940URL da Homepage: http://www.joinville.udesc.br/portal/professores/andretavares/;en_US;Published;14;2019;2019-11-20 11:53:13
98369;Cristiano Roberto Siebert;DenseNet-DC: Optimizing DenseNet Parameters Through Feature Map Generation Control;Convolutional Neural Networks still suffer from the need for great computational power, oftenrestricting their use on various platforms. Therefore, we propose a new optimization method made for DenseNet, a convolutional neural network that has the characteristic of being completely connected. The objective of the method is to control the generation of the characteristic maps in relation to the moment the network is in, aiming to reduce the size of the network with the minimum of loss in accuracy. This control occurs reducing the number of feature maps through the addition of a new parameter called the Decrease Control or dc value, where the decrease occurs from half of the layers. In order to validate the behavior of the proposed model, experiments were performed using different image bases: MNIST, Fashion-MNIST, CIFAR-10, CIFAR-100, CALTECH-101, Cats vs Dogs and TinyImageNet. Some of the results achieved were: for the MNIST and Fashion-MNIST base, there was 43% parameter reduction. For the CIFAR-10 base achieved a 44% reduction in network parameters, while in base CIFAR-100 the parameter reduction are 43%. In the CALTECH-101 base the parameter optimization was 35%, while the Cats vs Dogs optimized 30% of model parameters. Finally, the TinyImageNet base was reduced 31% of the parameters.;CNN, DenseNet, Optimization;Universidade do Estado de Santa Catarina, Centro de Ciências Tecnológicas. Rua Paulo Malschitzki, s/numero - Campus Universitário Prof. Avelino MarcanteZona Industrial Norte89219-710 - Joinville, SC - BrasilTelefone: (47) 40097900Ramal: 7832Fax: (47) 40097940URL da Homepage: http://www.joinville.udesc.br/portal/professores/andretavares/;en_US;Published;14;2019;2019-11-20 11:53:13
98483;Alfio Ricardo Martini;Reasoning about Partial Correctness Assertions in Isabelle/HOL;Hoare Logic has a long tradition in formal verification and has been continuously developed and used to verify a broad class of programs, including sequential, object-oriented and concurrent programs. The purpose of this work is to provide a detailed and accessible exposition of the several ways the user can conduct, explore and write proofs of correctness of sequential imperative programs with Hoare logic and the ISABELLE proof assistant. With the proof language Isar, it is possible to write structured, readable proofs that are suitable for human understanding and communication.;Hoare Logic, program verification, theorem proving;Av. Marechal Andrea 11/210, Porto Alegre/RS/Brasil;en_US;Published;17;2019;2019-11-25 15:25:23
98904;Danilo Silva Alves;Experiments on Model-Based Software Energy Consumption Analysis Involving Sorting Algorithms;Although energy has become an important aspect in software development, little support exists for creating energy-efficient programs. One reason for that is the lack of abstractions and tools to enable the analysis of relevant properties involving energy consumption. This paper presents the results of some experiments involving the gathering, modelling, and analysis of energy-related information, in particular, the costs of executing certain parts of a software. We combine some existing free and open-source tools to carry out the experiments, extending one of them to handle energy information. Our experiments consider a comparison of energy consumption of Java implementations of the Bubble Sort, Insertion Sort and Selection Sort algorithms using different data structures. We show how to combine an energy measurement tool and a model analysis tool to carry such a comparison. Based on this support and on our experiments, we believe this is a first step to allow developers to start creating more energy-efficient software.;Energy consumption, Behaviour models, Sorting algorithms;Universidade Federal do Rio Grande do Sul;en_US;Published;11;2019;2019-12-10 18:18:12
98904;Oseias Ayres Ferreira;Experiments on Model-Based Software Energy Consumption Analysis Involving Sorting Algorithms;Although energy has become an important aspect in software development, little support exists for creating energy-efficient programs. One reason for that is the lack of abstractions and tools to enable the analysis of relevant properties involving energy consumption. This paper presents the results of some experiments involving the gathering, modelling, and analysis of energy-related information, in particular, the costs of executing certain parts of a software. We combine some existing free and open-source tools to carry out the experiments, extending one of them to handle energy information. Our experiments consider a comparison of energy consumption of Java implementations of the Bubble Sort, Insertion Sort and Selection Sort algorithms using different data structures. We show how to combine an energy measurement tool and a model analysis tool to carry such a comparison. Based on this support and on our experiments, we believe this is a first step to allow developers to start creating more energy-efficient software.;Energy consumption, Behaviour models, Sorting algorithms;Universidade Federal do Rio Grande do Sul;en_US;Published;11;2019;2019-12-10 18:18:12
98904;Lucio Mauro Duarte;Experiments on Model-Based Software Energy Consumption Analysis Involving Sorting Algorithms;Although energy has become an important aspect in software development, little support exists for creating energy-efficient programs. One reason for that is the lack of abstractions and tools to enable the analysis of relevant properties involving energy consumption. This paper presents the results of some experiments involving the gathering, modelling, and analysis of energy-related information, in particular, the costs of executing certain parts of a software. We combine some existing free and open-source tools to carry out the experiments, extending one of them to handle energy information. Our experiments consider a comparison of energy consumption of Java implementations of the Bubble Sort, Insertion Sort and Selection Sort algorithms using different data structures. We show how to combine an energy measurement tool and a model analysis tool to carry such a comparison. Based on this support and on our experiments, we believe this is a first step to allow developers to start creating more energy-efficient software.;Energy consumption, Behaviour models, Sorting algorithms;Universidade Federal do Rio Grande do Sul;en_US;Published;11;2019;2019-12-10 18:18:12
98904;Davi Silva;Experiments on Model-Based Software Energy Consumption Analysis Involving Sorting Algorithms;Although energy has become an important aspect in software development, little support exists for creating energy-efficient programs. One reason for that is the lack of abstractions and tools to enable the analysis of relevant properties involving energy consumption. This paper presents the results of some experiments involving the gathering, modelling, and analysis of energy-related information, in particular, the costs of executing certain parts of a software. We combine some existing free and open-source tools to carry out the experiments, extending one of them to handle energy information. Our experiments consider a comparison of energy consumption of Java implementations of the Bubble Sort, Insertion Sort and Selection Sort algorithms using different data structures. We show how to combine an energy measurement tool and a model analysis tool to carry such a comparison. Based on this support and on our experiments, we believe this is a first step to allow developers to start creating more energy-efficient software.;Energy consumption, Behaviour models, Sorting algorithms;Universidade Estadual do Ceará;en_US;Published;11;2019;2019-12-10 18:18:12
98904;Paulo Henrique Maia;Experiments on Model-Based Software Energy Consumption Analysis Involving Sorting Algorithms;Although energy has become an important aspect in software development, little support exists for creating energy-efficient programs. One reason for that is the lack of abstractions and tools to enable the analysis of relevant properties involving energy consumption. This paper presents the results of some experiments involving the gathering, modelling, and analysis of energy-related information, in particular, the costs of executing certain parts of a software. We combine some existing free and open-source tools to carry out the experiments, extending one of them to handle energy information. Our experiments consider a comparison of energy consumption of Java implementations of the Bubble Sort, Insertion Sort and Selection Sort algorithms using different data structures. We show how to combine an energy measurement tool and a model analysis tool to carry such a comparison. Based on this support and on our experiments, we believe this is a first step to allow developers to start creating more energy-efficient software.;Energy consumption, Behaviour models, Sorting algorithms;Universidade Estadual do Ceará;en_US;Published;11;2019;2019-12-10 18:18:12
99849;Joan Davi Santos Silva;Automatic identification of knowledge related to dengue cases in the state of Piauí in public databases using Filtered-Association Rules Networks;Dengue is an endemic disease in Brazil since the 1980s and since 1996 in Piau ́ı. The number of cases increases each year, with the incidence of more severe symptoms. This research aimed to evaluate the use of an automatic knowledge identification technique in factors related to the number of dengue occurrences. We built a dataset formed by data available in the Information System for Notifiable Diseases (SINAN) and meteorological data of the municipalities of the coastal plain of Piau ́ı. The technique used was that of Filtered Association Rules Networks, which allows visual analysis of knowledge through the use of network structures and rules filtering. As a main result, we confirmed the understanding that the most significant number of cases occurs in May, as it is the moment when the rainfall indexes are decreasing, besides that socio-cultural and race factors do not interfere in the identification of the population of higher risk. This research presents the innovation of the use of a computational technique of automatic knowledge discovery that can assist in the elaboration of prevention actions by epidemiological surveillance.;Association Rules, Dengue, Epidemiological surveillance, Knowledge Discovery, Networks;Universidade Estadual do Piauí (UESPI) -- Campus Alexandre Alves de Oliveira -- Parnaíba, PI -- Brazil;en_US;Published;9;2020;2020-01-24 12:16:21
99849;Jâina Carolina Meneses Calçada;Automatic identification of knowledge related to dengue cases in the state of Piauí in public databases using Filtered-Association Rules Networks;Dengue is an endemic disease in Brazil since the 1980s and since 1996 in Piau ́ı. The number of cases increases each year, with the incidence of more severe symptoms. This research aimed to evaluate the use of an automatic knowledge identification technique in factors related to the number of dengue occurrences. We built a dataset formed by data available in the Information System for Notifiable Diseases (SINAN) and meteorological data of the municipalities of the coastal plain of Piau ́ı. The technique used was that of Filtered Association Rules Networks, which allows visual analysis of knowledge through the use of network structures and rules filtering. As a main result, we confirmed the understanding that the most significant number of cases occurs in May, as it is the moment when the rainfall indexes are decreasing, besides that socio-cultural and race factors do not interfere in the identification of the population of higher risk. This research presents the innovation of the use of a computational technique of automatic knowledge discovery that can assist in the elaboration of prevention actions by epidemiological surveillance.;Association Rules, Dengue, Epidemiological surveillance, Knowledge Discovery, Networks;Universidade Estadual Vale do Acaraú;en_US;Published;9;2020;2020-01-24 12:16:21
99849;Solange Oliveira Rezende;Automatic identification of knowledge related to dengue cases in the state of Piauí in public databases using Filtered-Association Rules Networks;Dengue is an endemic disease in Brazil since the 1980s and since 1996 in Piau ́ı. The number of cases increases each year, with the incidence of more severe symptoms. This research aimed to evaluate the use of an automatic knowledge identification technique in factors related to the number of dengue occurrences. We built a dataset formed by data available in the Information System for Notifiable Diseases (SINAN) and meteorological data of the municipalities of the coastal plain of Piau ́ı. The technique used was that of Filtered Association Rules Networks, which allows visual analysis of knowledge through the use of network structures and rules filtering. As a main result, we confirmed the understanding that the most significant number of cases occurs in May, as it is the moment when the rainfall indexes are decreasing, besides that socio-cultural and race factors do not interfere in the identification of the population of higher risk. This research presents the innovation of the use of a computational technique of automatic knowledge discovery that can assist in the elaboration of prevention actions by epidemiological surveillance.;Association Rules, Dengue, Epidemiological surveillance, Knowledge Discovery, Networks;Universidade de São Paulo;en_US;Published;9;2020;2020-01-24 12:16:21
99849;Dario Brito Calçada;Automatic identification of knowledge related to dengue cases in the state of Piauí in public databases using Filtered-Association Rules Networks;Dengue is an endemic disease in Brazil since the 1980s and since 1996 in Piau ́ı. The number of cases increases each year, with the incidence of more severe symptoms. This research aimed to evaluate the use of an automatic knowledge identification technique in factors related to the number of dengue occurrences. We built a dataset formed by data available in the Information System for Notifiable Diseases (SINAN) and meteorological data of the municipalities of the coastal plain of Piau ́ı. The technique used was that of Filtered Association Rules Networks, which allows visual analysis of knowledge through the use of network structures and rules filtering. As a main result, we confirmed the understanding that the most significant number of cases occurs in May, as it is the moment when the rainfall indexes are decreasing, besides that socio-cultural and race factors do not interfere in the identification of the population of higher risk. This research presents the innovation of the use of a computational technique of automatic knowledge discovery that can assist in the elaboration of prevention actions by epidemiological surveillance.;Association Rules, Dengue, Epidemiological surveillance, Knowledge Discovery, Networks;Universidade Estadual do Piauí;en_US;Published;9;2020;2020-01-24 12:16:21
103182;Christhian Henrique Gomes Fonseca;Low-Latency f0 Estimation for the Finger Plucked Electric Bass Guitar Using the Absolute Difference Function;Audio-to-MIDI conversion can be used to allow digital musical control through an analog instrument. Audio-to-MIDI converters rely on fundamental frequency estimators that are usually restricted to a minimum delay of two fundamental periods. This delay is perceptible for the case of bass notes. In this dissertation, we propose a low-latency fundamental frequency estimation method that relies on specific characteristics of the electric bass guitar. By means of physical modeling and signal  acquisition, we show that the assumptions of this method are based on the generalization of all electric basses. We evaluated our method in a dataset with musical notes played by diverse bassists. Results show that our method outperforms the Yin method in low-latency settings, which indicates its suitability for low-latency audio-to-MIDI conversion of the electric bass sound.;f0 estimation, low latency, Audio-to-MIDI converter, Music information retrieval, MIDI-bass;School of Electrical and Computer Engineering, University of Campinas;en_US;Published;15;2020;2020-05-19 20:19:21
103182;Tiago Tavares;Low-Latency f0 Estimation for the Finger Plucked Electric Bass Guitar Using the Absolute Difference Function;Audio-to-MIDI conversion can be used to allow digital musical control through an analog instrument. Audio-to-MIDI converters rely on fundamental frequency estimators that are usually restricted to a minimum delay of two fundamental periods. This delay is perceptible for the case of bass notes. In this dissertation, we propose a low-latency fundamental frequency estimation method that relies on specific characteristics of the electric bass guitar. By means of physical modeling and signal  acquisition, we show that the assumptions of this method are based on the generalization of all electric basses. We evaluated our method in a dataset with musical notes played by diverse bassists. Results show that our method outperforms the Yin method in low-latency settings, which indicates its suitability for low-latency audio-to-MIDI conversion of the electric bass sound.;f0 estimation, low latency, Audio-to-MIDI converter, Music information retrieval, MIDI-bass;School of Electric and Computer Engineering (FEEC), University of Campinas, Brazil;en_US;Published;15;2020;2020-05-19 20:19:21
104342;Luan Luiz Gonçalves;Creating Digital Musical Instruments with libmosaic-sound and Mosaicode;Music has been influenced by digital technology over the last few decades. With the computer and the Digital Musical Instruments, the musical composition could trespass the use of acoustic instruments demanding to musicians and composers a sort of computer programming skills for the development of musical applications. In order to simplify the development of musical applications several tools and musical programming languages arose bringing some facilities to lay-musicians on computer programming to use the computer to make music. This work presents the development of a Visual Programming Language (VPL) to develop DMI applications in the Mosaicode programming environment, simplifying sound design and making the creation of digital instruments more accessible to digital artists. It is also presented the implementation of libmosaic-sound library, which supported the VPL development, for the specific domain of Music Computing and DMI creation.;Mosaicode, Digital Musical Instrument, Code generation, Library development;Federal University of São João del-Rei;en_US;Published;12;2020;2020-06-15 23:55:50
104342;Flávio Luiz Schiavoni;Creating Digital Musical Instruments with libmosaic-sound and Mosaicode;Music has been influenced by digital technology over the last few decades. With the computer and the Digital Musical Instruments, the musical composition could trespass the use of acoustic instruments demanding to musicians and composers a sort of computer programming skills for the development of musical applications. In order to simplify the development of musical applications several tools and musical programming languages arose bringing some facilities to lay-musicians on computer programming to use the computer to make music. This work presents the development of a Visual Programming Language (VPL) to develop DMI applications in the Mosaicode programming environment, simplifying sound design and making the creation of digital instruments more accessible to digital artists. It is also presented the implementation of libmosaic-sound library, which supported the VPL development, for the specific domain of Music Computing and DMI creation.;Mosaicode, Digital Musical Instrument, Code generation, Library development;Federal University of São João del-Rei;en_US;Published;12;2020;2020-06-15 23:55:50
105893;Felipe Nedopetalski;Overload Control in a Token Player for a Fuzzy Workflow Management System;The underlying proposal of this work is to control overload of a Token Player in a Fuzzy Workflow Management System. In order to accomplish this, possibility theory is used to measure how much the Token Player can be overloaded. The model used in this work is built using Colored Petri net and the simulation is made using CPN Tools. Finally, is possible to control overload of the Token Player in a Fuzzy Workflow Management System, nevertheless more time is spent to achieve the end of activities.;CPN Tools, Workflow Management Systems, Fuzzy, Overload Control, Petri Net;Universidade Federal de Jataí;en_US;Published;10;2020;2020-07-29 16:45:27
105893;Joslaine Cristina Jeske de Freitas;Overload Control in a Token Player for a Fuzzy Workflow Management System;The underlying proposal of this work is to control overload of a Token Player in a Fuzzy Workflow Management System. In order to accomplish this, possibility theory is used to measure how much the Token Player can be overloaded. The model used in this work is built using Colored Petri net and the simulation is made using CPN Tools. Finally, is possible to control overload of the Token Player in a Fuzzy Workflow Management System, nevertheless more time is spent to achieve the end of activities.;CPN Tools, Workflow Management Systems, Fuzzy, Overload Control, Petri Net;Universidade Federal de Jataí;en_US;Published;10;2020;2020-07-29 16:45:27
106277;Zineb Lamghari;An operational support approach for Mining Unstructured Business Processes;The refined process mining framework contains a set of activities that use extracted information from event logs, discovered models and normative ones. Among these activities, we find those dealing with running events in a Structured Business Process (SBP) context, which are the Detect, the Predict and the Recommend activities. These three activities are nominated as an operational support system that aims at detecting deviations, predicting events and recommending actions. In this regard, operational support systems perform well on SBP while, it stills a challenging task for an Unstructured Business Process (UBP). This puts forward the difficulty of predicting events and recommending actions for UBP, because of its complex structure. In this context, simplification and structuring operations must be applied. Therefore, the intervention of other process mining activities is required for business process simplification and structuring. To this end, we present an operational support approach dealing with UBP, using the refined process mining framework activities.;Process Mining, Operational support, Unstructured Business Processes, Structured Business Process, Spaghetti process model, Lasagna process model, Structuring techniques, Heuristics Miner, BPMN;LRIT associated unit to CNRST (URAC 29), Rabat IT Center, Faculty of Sciences, Mohammed V University in Rabat, Morocco;en_US;Published;16;2020;2020-08-30 2:59:48
106277;Maryam Radgui;An operational support approach for Mining Unstructured Business Processes;The refined process mining framework contains a set of activities that use extracted information from event logs, discovered models and normative ones. Among these activities, we find those dealing with running events in a Structured Business Process (SBP) context, which are the Detect, the Predict and the Recommend activities. These three activities are nominated as an operational support system that aims at detecting deviations, predicting events and recommending actions. In this regard, operational support systems perform well on SBP while, it stills a challenging task for an Unstructured Business Process (UBP). This puts forward the difficulty of predicting events and recommending actions for UBP, because of its complex structure. In this context, simplification and structuring operations must be applied. Therefore, the intervention of other process mining activities is required for business process simplification and structuring. To this end, we present an operational support approach dealing with UBP, using the refined process mining framework activities.;Process Mining, Operational support, Unstructured Business Processes, Structured Business Process, Spaghetti process model, Lasagna process model, Structuring techniques, Heuristics Miner, BPMN;SI2M Laboratory and LRIT associated unit to CNRST, Rabat;en_US;Published;16;2020;2020-08-30 2:59:48
106277;Rajaa Saidi;An operational support approach for Mining Unstructured Business Processes;The refined process mining framework contains a set of activities that use extracted information from event logs, discovered models and normative ones. Among these activities, we find those dealing with running events in a Structured Business Process (SBP) context, which are the Detect, the Predict and the Recommend activities. These three activities are nominated as an operational support system that aims at detecting deviations, predicting events and recommending actions. In this regard, operational support systems perform well on SBP while, it stills a challenging task for an Unstructured Business Process (UBP). This puts forward the difficulty of predicting events and recommending actions for UBP, because of its complex structure. In this context, simplification and structuring operations must be applied. Therefore, the intervention of other process mining activities is required for business process simplification and structuring. To this end, we present an operational support approach dealing with UBP, using the refined process mining framework activities.;Process Mining, Operational support, Unstructured Business Processes, Structured Business Process, Spaghetti process model, Lasagna process model, Structuring techniques, Heuristics Miner, BPMN;SI2M Laboratory and LRIT associated unit to CNRST, Rabat;en_US;Published;16;2020;2020-08-30 2:59:48
106277;Moulay Driss Rahmani;An operational support approach for Mining Unstructured Business Processes;The refined process mining framework contains a set of activities that use extracted information from event logs, discovered models and normative ones. Among these activities, we find those dealing with running events in a Structured Business Process (SBP) context, which are the Detect, the Predict and the Recommend activities. These three activities are nominated as an operational support system that aims at detecting deviations, predicting events and recommending actions. In this regard, operational support systems perform well on SBP while, it stills a challenging task for an Unstructured Business Process (UBP). This puts forward the difficulty of predicting events and recommending actions for UBP, because of its complex structure. In this context, simplification and structuring operations must be applied. Therefore, the intervention of other process mining activities is required for business process simplification and structuring. To this end, we present an operational support approach dealing with UBP, using the refined process mining framework activities.;Process Mining, Operational support, Unstructured Business Processes, Structured Business Process, Spaghetti process model, Lasagna process model, Structuring techniques, Heuristics Miner, BPMN;LRIT associated unit to CNRST (URAC 29), Faculty of Sciences, Mohammed V University in Rabat, Morocco;en_US;Published;16;2020;2020-08-30 2:59:48
106794;Maicon Ança dos Santos;Cloud infrastructure for HPC investment analysis;With the consolidation of cloud computing technology, there is a growing interest in exploring it to support High Performance Computing (HPC). However, migrating such applications to public or private cloud environments brings some challenges, in particular, the cost in financing the migration process. In this paper, a literature review is presented with selected papers about analyzing cloud infrastructure investments. In particular, the selected papers analyse how investments impact applications. For discussion of related works, conditions for running HPC applications in the cloud are characterized.;Cloud computing, HPC, Cost model, Investment;Universidade Federal de Pelotas;en_US;Published;17;2020;2020-08-24 19:19:10
106794;Gerson Geraldo H. Cavalheiro;Cloud infrastructure for HPC investment analysis;With the consolidation of cloud computing technology, there is a growing interest in exploring it to support High Performance Computing (HPC). However, migrating such applications to public or private cloud environments brings some challenges, in particular, the cost in financing the migration process. In this paper, a literature review is presented with selected papers about analyzing cloud infrastructure investments. In particular, the selected papers analyse how investments impact applications. For discussion of related works, conditions for running HPC applications in the cloud are characterized.;Cloud computing, HPC, Cost model, Investment;Universidade Federal de Pelotas;en_US;Published;17;2020;2020-08-24 19:19:10
106828;Florian Spree;Business Process Models in the Context of Predictive Process Monitoring;Predictive process monitoring is a subject of growing interest in academic research. As a result, an increased number of papers on this topic have been published. Due to the high complexity in this research area a wide range of different experimental setups and methods have been applied which makes it very difficult to reliably compare research results. This paper's objective is to investigate how business process models and their characteristics are used during experimental setups and how they can contribute to academic research. First, a literature review is conducted to analyze and discuss the awareness of business process models in experimental setups. Secondly, the paper discusses identified research problems and proposes the concept of a web-based business process model metric suite and the idea of ranked metrics. Through a metric suite researchers and practitioners can automatically evaluate business process model characteristics in their future work. Further, a contextualization of metrics by introducing a ranking of characteristics can potentially indicate how the outcome of experimental setups will be. Hence, the paper's work demonstrates the importance of business process models and their characteristics in the context of predictive process monitoring and proposes the concept of a tool approach and ranking to reliably evaluate business process models characteristics.;Business Process Management, Predicitve Process Monitoring, Business Process Model Complexity;-;en_US;Published;7;2020;2020-08-25 16:51:55
106891;Carina Alves;Integrating Exploitative and Explorative Thinking in Business Process Analysis: A Conceptual Model and Method;Ambidextrous BPM has gained increasing interest from researchers and practitioners in the last years. It refers to the ability to use exploitative and explorative capabilities in BPM projects. In this paper, we investigate how the integration of exploitative and explorative ideas can leverage the analysis of business processes. The key contributions of this paper are a conceptual model and a method that integrate ambidextrous thinking in a complementary way. Both artefacts were evaluated by means of an expert opinion survey. We also present a case study at an organisation that has implemented our proposed method. We believe that ambidextrous analysis of business processes enables organisations tackling current operational bottlenecks while simultaneously exploring external opportunities for designing innovation in business processes.;Ambidextrous BPM, Exploitative and Explorative Techniques, Business Process Analysis, Design Science Research;Universidade Federal de Pernambuco;en_US;Published;15;2020;2020-08-27 15:20:23
106891;Higor Monteiro;Integrating Exploitative and Explorative Thinking in Business Process Analysis: A Conceptual Model and Method;Ambidextrous BPM has gained increasing interest from researchers and practitioners in the last years. It refers to the ability to use exploitative and explorative capabilities in BPM projects. In this paper, we investigate how the integration of exploitative and explorative ideas can leverage the analysis of business processes. The key contributions of this paper are a conceptual model and a method that integrate ambidextrous thinking in a complementary way. Both artefacts were evaluated by means of an expert opinion survey. We also present a case study at an organisation that has implemented our proposed method. We believe that ambidextrous analysis of business processes enables organisations tackling current operational bottlenecks while simultaneously exploring external opportunities for designing innovation in business processes.;Ambidextrous BPM, Exploitative and Explorative Techniques, Business Process Analysis, Design Science Research;Universidade de Pernambuco;en_US;Published;15;2020;2020-08-27 15:20:23
107021;Carlos Vicente Soares Araujo;A Model for Predicting Music Popularity on Streaming Platforms;The global music market moves billions of dollars every year, most of which comes from streamingplatforms. In this paper, we present a model for predicting whether or not a song will appear in Spotify’s Top 50, a ranking of the 50 most popular songs in Spotify, which is one of today’s biggest streaming services. To make this prediction, we trained different classifiers with information from audio features from songs that appeared in this ranking between November 2018 and January 2019. When tested with data from June and July 2019, an SVM classifier with RBF kernel obtained accuracy, precision, and AUC above 80%.;Music, Hit Song Science, Machine Learning, Spotify;Federal University of Amazonas;en_US;Published;9;2020;2020-08-30 18:25:21
107021;Marco Antônio Pinheiro de Cristo;A Model for Predicting Music Popularity on Streaming Platforms;The global music market moves billions of dollars every year, most of which comes from streamingplatforms. In this paper, we present a model for predicting whether or not a song will appear in Spotify’s Top 50, a ranking of the 50 most popular songs in Spotify, which is one of today’s biggest streaming services. To make this prediction, we trained different classifiers with information from audio features from songs that appeared in this ranking between November 2018 and January 2019. When tested with data from June and July 2019, an SVM classifier with RBF kernel obtained accuracy, precision, and AUC above 80%.;Music, Hit Song Science, Machine Learning, Spotify;Federal Univeristy of Amazonas;en_US;Published;9;2020;2020-08-30 18:25:21
107021;Rafael Giusti;A Model for Predicting Music Popularity on Streaming Platforms;The global music market moves billions of dollars every year, most of which comes from streamingplatforms. In this paper, we present a model for predicting whether or not a song will appear in Spotify’s Top 50, a ranking of the 50 most popular songs in Spotify, which is one of today’s biggest streaming services. To make this prediction, we trained different classifiers with information from audio features from songs that appeared in this ranking between November 2018 and January 2019. When tested with data from June and July 2019, an SVM classifier with RBF kernel obtained accuracy, precision, and AUC above 80%.;Music, Hit Song Science, Machine Learning, Spotify;Federal University of Amazonas;en_US;Published;9;2020;2020-08-30 18:25:21
107041;Guido Dutra de Oliveira;An Agent-Based Model for Simulating Irrigated Agriculture in the Samambaia Basin in Goiás;Agriculture is one of the main economic activities in Brazil. The intensive use of water for irrigated agriculture leads to water rise demand contributing to increase water stress. Agent-based models help assess this problem with promising applications entailing an organizing principle to inform us of how to view a real-world system and effectively build a model. In this work, agent-based modeling is applied to simulate water usage for irrigation in agricultural production in the Samambaia river basin in the municipality of Cristalina in the Goias state of Brazil. The use of real data enables analysis of resource availability in a scenario with high demand irrigation, allowing a greater understanding of the needs of the parties involved.;agent-based modeling, agent-based simulation, irrigated agriculture, water resources;Computer Science DepartmentUniversity of Brasilia;en_US;Published;16;2020;2020-08-30 22:22:28
107041;Pedro Phelipe Gonçalves Porto;An Agent-Based Model for Simulating Irrigated Agriculture in the Samambaia Basin in Goiás;Agriculture is one of the main economic activities in Brazil. The intensive use of water for irrigated agriculture leads to water rise demand contributing to increase water stress. Agent-based models help assess this problem with promising applications entailing an organizing principle to inform us of how to view a real-world system and effectively build a model. In this work, agent-based modeling is applied to simulate water usage for irrigation in agricultural production in the Samambaia river basin in the municipality of Cristalina in the Goias state of Brazil. The use of real data enables analysis of resource availability in a scenario with high demand irrigation, allowing a greater understanding of the needs of the parties involved.;agent-based modeling, agent-based simulation, irrigated agriculture, water resources;Programa de Pós-graduação em Tecnologia Ambiental e Recursos HídricosDepartamento de Engenharia Civil e Ambiental;en_US;Published;16;2020;2020-08-30 22:22:28
107041;Conceição de Maria Albuquerque Alves;An Agent-Based Model for Simulating Irrigated Agriculture in the Samambaia Basin in Goiás;Agriculture is one of the main economic activities in Brazil. The intensive use of water for irrigated agriculture leads to water rise demand contributing to increase water stress. Agent-based models help assess this problem with promising applications entailing an organizing principle to inform us of how to view a real-world system and effectively build a model. In this work, agent-based modeling is applied to simulate water usage for irrigation in agricultural production in the Samambaia river basin in the municipality of Cristalina in the Goias state of Brazil. The use of real data enables analysis of resource availability in a scenario with high demand irrigation, allowing a greater understanding of the needs of the parties involved.;agent-based modeling, agent-based simulation, irrigated agriculture, water resources;Programa de Pós-graduação em Tecnologia Ambiental e Recursos HídricosDepartamento de Engenharia Civil e Ambiental;en_US;Published;16;2020;2020-08-30 22:22:28
107041;Celia Ghedini Ralha;An Agent-Based Model for Simulating Irrigated Agriculture in the Samambaia Basin in Goiás;Agriculture is one of the main economic activities in Brazil. The intensive use of water for irrigated agriculture leads to water rise demand contributing to increase water stress. Agent-based models help assess this problem with promising applications entailing an organizing principle to inform us of how to view a real-world system and effectively build a model. In this work, agent-based modeling is applied to simulate water usage for irrigation in agricultural production in the Samambaia river basin in the municipality of Cristalina in the Goias state of Brazil. The use of real data enables analysis of resource availability in a scenario with high demand irrigation, allowing a greater understanding of the needs of the parties involved.;agent-based modeling, agent-based simulation, irrigated agriculture, water resources;Computer Science DepartmentInstitute of Exact ScienceUniversity of Brasília (UnB), Brazil;en_US;Published;16;2020;2020-08-30 22:22:28
107128;Fábio Carlos Moreno;Hash Tables for a Digital Lexicon;This paper deals with the construction of digital lexicons within the scope of Natural Language Processing. Data Structures called Hash Tables have demonstrated to generate good results for Natural Language Interface for Databases and have data dispersion, response speed and programming simplicity as main features. The storage of the desired information is done by associating a key through the hashing functions that is responsible for distributing the information in this table. The objective of this paper is to present the tool called Visual TaHs that uses a sparse table to a real lexicon (Lexicon of Herbs), improving performance results of several implemented hash functions. Such structure has achieved satisfactory results in terms of speed and storage when compared to conventional databases and can work in various media, such as desktop, Web and mobile.;Hash, Lexicon, Natural Language Processing;Universidade Estadual do Norte do Paraná (UENP);en_US;Published;13;2020;2020-08-31 21:23:11
107128;Cinthyan Sachs C. de Barbosa;Hash Tables for a Digital Lexicon;This paper deals with the construction of digital lexicons within the scope of Natural Language Processing. Data Structures called Hash Tables have demonstrated to generate good results for Natural Language Interface for Databases and have data dispersion, response speed and programming simplicity as main features. The storage of the desired information is done by associating a key through the hashing functions that is responsible for distributing the information in this table. The objective of this paper is to present the tool called Visual TaHs that uses a sparse table to a real lexicon (Lexicon of Herbs), improving performance results of several implemented hash functions. Such structure has achieved satisfactory results in terms of speed and storage when compared to conventional databases and can work in various media, such as desktop, Web and mobile.;Hash, Lexicon, Natural Language Processing;Universidade Estadual de Londrina (UEL);en_US;Published;13;2020;2020-08-31 21:23:11
107128;Edio Roberto Manfio;Hash Tables for a Digital Lexicon;This paper deals with the construction of digital lexicons within the scope of Natural Language Processing. Data Structures called Hash Tables have demonstrated to generate good results for Natural Language Interface for Databases and have data dispersion, response speed and programming simplicity as main features. The storage of the desired information is done by associating a key through the hashing functions that is responsible for distributing the information in this table. The objective of this paper is to present the tool called Visual TaHs that uses a sparse table to a real lexicon (Lexicon of Herbs), improving performance results of several implemented hash functions. Such structure has achieved satisfactory results in terms of speed and storage when compared to conventional databases and can work in various media, such as desktop, Web and mobile.;Hash, Lexicon, Natural Language Processing;Faculdade de Tecnologia de Garça (FATEC-Garça-SP);en_US;Published;13;2020;2020-08-31 21:23:11
107149;Carolinne Roque e Faria;System for Identifying Pests and Diseases in Soybean Crop through Natural Language Processing;The presence of technologies in the agronomic field has the purpose of proposing the best solutions to the challenges found in agriculture, especially to the problems that affect cultivars. One of the obstacles found is to apply the use of your own language in applications that interact with the user in Brazilian Agribusiness. Therefore, this work uses Natural Language Processing techniques for the development of an automatic and effective computer system to interact with the user and assist in the identification of pests and diseases in soybean crop, stored in a non-relational database repository to provide accurate diagnostics to simplify the work of the farmer and the agricultural stakeholders who deal with a lot of information. In order to build dialogues and provide rich consultations, from agriculture manuals, a data structure with 108 pests and diseases with their information on the soybean cultivar and through the spaCy tool, it was possible to pre-process the texts, recognize the entities and support the requirements for the development of the conversacional system.;Digital Agriculture, Intelligent systems, Natural Language Processing;Universidade Estadual de Londrina;en_US;Published;13;2020;2020-09-01 1:42:52
107149;Cinthyan S. C. Barbosa;System for Identifying Pests and Diseases in Soybean Crop through Natural Language Processing;The presence of technologies in the agronomic field has the purpose of proposing the best solutions to the challenges found in agriculture, especially to the problems that affect cultivars. One of the obstacles found is to apply the use of your own language in applications that interact with the user in Brazilian Agribusiness. Therefore, this work uses Natural Language Processing techniques for the development of an automatic and effective computer system to interact with the user and assist in the identification of pests and diseases in soybean crop, stored in a non-relational database repository to provide accurate diagnostics to simplify the work of the farmer and the agricultural stakeholders who deal with a lot of information. In order to build dialogues and provide rich consultations, from agriculture manuals, a data structure with 108 pests and diseases with their information on the soybean cultivar and through the spaCy tool, it was possible to pre-process the texts, recognize the entities and support the requirements for the development of the conversacional system.;Digital Agriculture, Intelligent systems, Natural Language Processing;State University of Londrina, Computings Science Department;en_US;Published;13;2020;2020-09-01 1:42:52
107154;João Pedro Bernardino Andrade;Centralized Algorithms Based on Clustering with Self-tuning of Parameters for Cooperative Target Observation;Clustering on target positions is a class of centralized algorithms used to calculate the surveillance robots' displacements in the Cooperative Target Observation (CTO) problem. This work proposes and evaluates Fuzzy C-means (FCM) and Density-Based Spatial Clustering of Applications with Noise (DBSCAN) with K-means (DBSk) based self-tuning clustering centralized algorithms for the CTO problem and compares its performances with that of K-means. Two random motion patterns are adopted for the targets: in free space or on a grid. As a contribution, the work allows identifying ranges of problem configuration parameters in which each algorithm shows the highest average performance. As a first conclusion, in the challenging situation in which the relative speed of the targets is high, and the relative sensor range of the surveillance is low, for which the existing algorithms present a substantial drop in performance, the FCM algorithm proposed outperforms the others. Finally, the DBSk algorithm adapts very well in low execution frequency, showing promising results in this challenging situation.;Multi-Agent Systems, Agent-Based Simulation, Clustering Methods, Intelligent Robots;-;en_US;Published;10;2020;2020-08-31 21:32:09
107154;Jose Everardo B. Maia;Centralized Algorithms Based on Clustering with Self-tuning of Parameters for Cooperative Target Observation;Clustering on target positions is a class of centralized algorithms used to calculate the surveillance robots' displacements in the Cooperative Target Observation (CTO) problem. This work proposes and evaluates Fuzzy C-means (FCM) and Density-Based Spatial Clustering of Applications with Noise (DBSCAN) with K-means (DBSk) based self-tuning clustering centralized algorithms for the CTO problem and compares its performances with that of K-means. Two random motion patterns are adopted for the targets: in free space or on a grid. As a contribution, the work allows identifying ranges of problem configuration parameters in which each algorithm shows the highest average performance. As a first conclusion, in the challenging situation in which the relative speed of the targets is high, and the relative sensor range of the surveillance is low, for which the existing algorithms present a substantial drop in performance, the FCM algorithm proposed outperforms the others. Finally, the DBSk algorithm adapts very well in low execution frequency, showing promising results in this challenging situation.;Multi-Agent Systems, Agent-Based Simulation, Clustering Methods, Intelligent Robots;-;en_US;Published;10;2020;2020-08-31 21:32:09
107154;Gustavo Augusto L. de Campos;Centralized Algorithms Based on Clustering with Self-tuning of Parameters for Cooperative Target Observation;Clustering on target positions is a class of centralized algorithms used to calculate the surveillance robots' displacements in the Cooperative Target Observation (CTO) problem. This work proposes and evaluates Fuzzy C-means (FCM) and Density-Based Spatial Clustering of Applications with Noise (DBSCAN) with K-means (DBSk) based self-tuning clustering centralized algorithms for the CTO problem and compares its performances with that of K-means. Two random motion patterns are adopted for the targets: in free space or on a grid. As a contribution, the work allows identifying ranges of problem configuration parameters in which each algorithm shows the highest average performance. As a first conclusion, in the challenging situation in which the relative speed of the targets is high, and the relative sensor range of the surveillance is low, for which the existing algorithms present a substantial drop in performance, the FCM algorithm proposed outperforms the others. Finally, the DBSk algorithm adapts very well in low execution frequency, showing promising results in this challenging situation.;Multi-Agent Systems, Agent-Based Simulation, Clustering Methods, Intelligent Robots;-;en_US;Published;10;2020;2020-08-31 21:32:09
107176;Thiago Valentim Bezerra;Dimensioning the relationship between availability and data center energy flow metrics;The advancement of technology and the growing number of applications available to network users have increased the demand for services hosted in cloud environments. In 2020, more than 4 billion of people access these services through the Internet, a value 7% higher in comparison to the same period in 2019. To support the demand for such services, an environment that provides such conditions for applications available whenever needed has grown in importance. These environments are generally available from large data centers, which consume large amounts of electricity to provide such demand service capacity. In this context, this work proposes an integrated and dynamic strategy that demonstrates the impact of the availability on the energyconsumption of the devices that compose the data center system architecture. In order to accomplish this, colored Petri net models were proposed for quantifying the cost, environmental impact and availability of the electric energy infrastructure ofdata centers. The models presented in this work are supported by the developed prototype. Two case studies illustrate the applicability of the proposed models and strategy. Significant results were obtained, showing an increase close to 100% in the system availability, with practically the same operational cost and environmental impact.;Availability --- Data centers --- Energy flow model --- colored petri nets;Universidade Federal Rural de Pernambuco;en_US;Published;15;2020;2020-09-01 0:25:08
107176;Wenderson de Souza Leonardo;Dimensioning the relationship between availability and data center energy flow metrics;The advancement of technology and the growing number of applications available to network users have increased the demand for services hosted in cloud environments. In 2020, more than 4 billion of people access these services through the Internet, a value 7% higher in comparison to the same period in 2019. To support the demand for such services, an environment that provides such conditions for applications available whenever needed has grown in importance. These environments are generally available from large data centers, which consume large amounts of electricity to provide such demand service capacity. In this context, this work proposes an integrated and dynamic strategy that demonstrates the impact of the availability on the energyconsumption of the devices that compose the data center system architecture. In order to accomplish this, colored Petri net models were proposed for quantifying the cost, environmental impact and availability of the electric energy infrastructure ofdata centers. The models presented in this work are supported by the developed prototype. Two case studies illustrate the applicability of the proposed models and strategy. Significant results were obtained, showing an increase close to 100% in the system availability, with practically the same operational cost and environmental impact.;Availability --- Data centers --- Energy flow model --- colored petri nets;Universidade Federal Rural de Pernambuco;en_US;Published;15;2020;2020-09-01 0:25:08
107176;Gabriel Alves de Albuquerque Júnior;Dimensioning the relationship between availability and data center energy flow metrics;The advancement of technology and the growing number of applications available to network users have increased the demand for services hosted in cloud environments. In 2020, more than 4 billion of people access these services through the Internet, a value 7% higher in comparison to the same period in 2019. To support the demand for such services, an environment that provides such conditions for applications available whenever needed has grown in importance. These environments are generally available from large data centers, which consume large amounts of electricity to provide such demand service capacity. In this context, this work proposes an integrated and dynamic strategy that demonstrates the impact of the availability on the energyconsumption of the devices that compose the data center system architecture. In order to accomplish this, colored Petri net models were proposed for quantifying the cost, environmental impact and availability of the electric energy infrastructure ofdata centers. The models presented in this work are supported by the developed prototype. Two case studies illustrate the applicability of the proposed models and strategy. Significant results were obtained, showing an increase close to 100% in the system availability, with practically the same operational cost and environmental impact.;Availability --- Data centers --- Energy flow model --- colored petri nets;Universidade Federal Rural de Pernambuco;en_US;Published;15;2020;2020-09-01 0:25:08
107176;Gustavo Rau de Almeida Callou;Dimensioning the relationship between availability and data center energy flow metrics;The advancement of technology and the growing number of applications available to network users have increased the demand for services hosted in cloud environments. In 2020, more than 4 billion of people access these services through the Internet, a value 7% higher in comparison to the same period in 2019. To support the demand for such services, an environment that provides such conditions for applications available whenever needed has grown in importance. These environments are generally available from large data centers, which consume large amounts of electricity to provide such demand service capacity. In this context, this work proposes an integrated and dynamic strategy that demonstrates the impact of the availability on the energyconsumption of the devices that compose the data center system architecture. In order to accomplish this, colored Petri net models were proposed for quantifying the cost, environmental impact and availability of the electric energy infrastructure ofdata centers. The models presented in this work are supported by the developed prototype. Two case studies illustrate the applicability of the proposed models and strategy. Significant results were obtained, showing an increase close to 100% in the system availability, with practically the same operational cost and environmental impact.;Availability --- Data centers --- Energy flow model --- colored petri nets;Universidade Federal Rural de Pernambuco;en_US;Published;15;2020;2020-09-01 0:25:08
107202;Tairone Nunes Magalhaes;Iracema: a Python library for audio content analysis;Iracema is a Python library that aims to provide models for the extraction of meaningful informationfrom recordings of monophonic pieces of music, for purposes of research in music performance. With this objective in mind, we propose an architecture that will provide to users an abstraction level that simplifies the manipulation of different kinds of time series, as well as the extraction of segments from them. In this paper we: (1) introduce some key concepts at the core of the proposed  architecture (2) describe the current functionalities of the package (3) give some examples of the application programming interface and (4) give some brief examples of audio analysis using the system.;Music Expressiveness, Music Information Retrieval, Software Systems and Languages for Sound and Music;Center for Research on Musical Gesture & Expression;en_US;Published;11;2020;2020-09-02 10:18:08
107202;Felippe Brandão Barros;Iracema: a Python library for audio content analysis;Iracema is a Python library that aims to provide models for the extraction of meaningful informationfrom recordings of monophonic pieces of music, for purposes of research in music performance. With this objective in mind, we propose an architecture that will provide to users an abstraction level that simplifies the manipulation of different kinds of time series, as well as the extraction of segments from them. In this paper we: (1) introduce some key concepts at the core of the proposed  architecture (2) describe the current functionalities of the package (3) give some examples of the application programming interface and (4) give some brief examples of audio analysis using the system.;Music Expressiveness, Music Information Retrieval, Software Systems and Languages for Sound and Music;Center for Research on Musical Gesture & Expression;en_US;Published;11;2020;2020-09-02 10:18:08
107202;Mauricio Alves Loureiro;Iracema: a Python library for audio content analysis;Iracema is a Python library that aims to provide models for the extraction of meaningful informationfrom recordings of monophonic pieces of music, for purposes of research in music performance. With this objective in mind, we propose an architecture that will provide to users an abstraction level that simplifies the manipulation of different kinds of time series, as well as the extraction of segments from them. In this paper we: (1) introduce some key concepts at the core of the proposed  architecture (2) describe the current functionalities of the package (3) give some examples of the application programming interface and (4) give some brief examples of audio analysis using the system.;Music Expressiveness, Music Information Retrieval, Software Systems and Languages for Sound and Music;Center for Research on Musical Gesture & Expression;en_US;Published;11;2020;2020-09-02 10:18:08
107292;Leonardo Araujo Zoehler Brum;Challenges and Perspectives on Real-time Singing Voice Synthesis;This paper describes the state of art of real-time singing voice synthesis and presents its concept, applications and technical aspects. A technological mapping and a literature review are made in order to indicate the latest developments in this area. We made a brief comparative analysis among the selected works. Finally, we have discussed challenges and future research problems.;Real-time singing voice synthesis, Sound Synthesis, TTS, MIDI, Computer Music;Universidade Federal de Sergipe;en_US;Published;8;2020;2020-09-05 11:23:49
107292;Edward David Moreno;Challenges and Perspectives on Real-time Singing Voice Synthesis;This paper describes the state of art of real-time singing voice synthesis and presents its concept, applications and technical aspects. A technological mapping and a literature review are made in order to indicate the latest developments in this area. We made a brief comparative analysis among the selected works. Finally, we have discussed challenges and future research problems.;Real-time singing voice synthesis, Sound Synthesis, TTS, MIDI, Computer Music;Universidade Federal de Sergipe;en_US;Published;8;2020;2020-09-05 11:23:49
107478;Alfredo Silveira Araújo Neto;Optimization Inspired on Herd Immunity Applied to Non-Hierarchical Grouping of Objects;Characterized as one of the most important operations related to data analysis, one non-hierarchical grouping consists of, even without having any information about the elements to be classified, establish upon a finite collection of objects, the partitioning of the items that constitute it into subsets or groups without intersecting, so that the elements that are part of a certain group are more similar to each other than the items that belong to distinct group. In this context, this study proposes the application of a meta-heuristic inspired by herd immunity to the determination of the non-hierarchical grouping of objects, and compares the results obtained by this method with the answers provided by four other grouping strategies, described in the literature. In particular, the resulting arrangements of the classification of 33 benchmark collections, performed by the suggested algorithm, by the metaheuristic inspired by the particle swarm, by the genetic algorithm, by the K-means algorithm and by the meta-heuristic inspired by the thermal annealing process, were compared under the perspective of 10 different evaluation measures, indicating that the partitions established by the meta-heuristic inspired by the herd immunity may, in certain respects, be more favorable than the classifications obtained by the other clustering methods.;Data mining, heuristic, combinatorial optimization, bio-inspired computing;Techway Informática Ltda.;en_US;Published;15;2020;2020-09-12 17:06:25
107748;Egon Lüftenegger;Co-creating Service-Dominant Business Artifacts with Action Design Research: Towards Ambidextrous Business Process Management;Due to the rise of digital innovations, business process management research requires not only to focus on internal organizational improvement aspects. It should also adopt an explorative focus to include completely new business processes driven by digital innovations. Our research project began as an innovation initiative of an international Dutch conglomerate in the financial services sector for exploring new business models. This effort took the form of collaboration among academics in business informatics and practitioners. We formalized this collaboration by adopting action design research (ADR) for reaching impact within the company while contributing new knowledge. The use of ADR resulted in the artifacts' co-creation that led to shared benefits, resulting in a win-win situation for the academics and practitioners.On the one hand, academics built a framework and its underlying artifacts for service-dominant business design and engineering. On the other hand, the framework supported the organizational transformation driven by digital innovation. This framework helps explore new strategic approaches that influence the design of new business models enabled by new business processes due to combining new and current capabilities known as business services.;business model, service-dominant logic, Business Engineering;BusinessModelRadar.com;en_US;Published;14;2020;2020-09-22 12:47:34
107964;Riham Alhomsi;The Explainable Business Process (XBP) - An Exploratory Research;Providing explanations to the business process, its decisions and its activities, is an important key factor for the process in order to achieve the business objectives of the business process, and to minimize and deal with the ambiguity of the business process that causes multiple interpretations, as well as to engender the appropriate trust of the users in the process. As a first step towards adding explanations to business process, we present an exploratory study to bring in the concept of explainability into business process, where we propose a conceptual framework to use the explainability with business process in a model that we called the Explainable Business Process XBP, furthermore we propose the XBP lifecycle based on the Model-based and Incremental Knowledge Engineering (MIKE) approach, in order to show in details the phase where explainability can take a place in business process lifecycle, noting that we focus on explaining the decisions and activities of the process in its as-is model without transforming it into a to-be model.;business process, explainability, explainable business process;-;en_US;Published;18;2020;2020-09-30 3:45:10
107964;Adriana Santarosa Vivacqua;The Explainable Business Process (XBP) - An Exploratory Research;Providing explanations to the business process, its decisions and its activities, is an important key factor for the process in order to achieve the business objectives of the business process, and to minimize and deal with the ambiguity of the business process that causes multiple interpretations, as well as to engender the appropriate trust of the users in the process. As a first step towards adding explanations to business process, we present an exploratory study to bring in the concept of explainability into business process, where we propose a conceptual framework to use the explainability with business process in a model that we called the Explainable Business Process XBP, furthermore we propose the XBP lifecycle based on the Model-based and Incremental Knowledge Engineering (MIKE) approach, in order to show in details the phase where explainability can take a place in business process lifecycle, noting that we focus on explaining the decisions and activities of the process in its as-is model without transforming it into a to-be model.;business process, explainability, explainable business process;-;en_US;Published;18;2020;2020-09-30 3:45:10
108126;Alexandra Virgínia Valente da Silva;Classification Based on Rules for the Study of Cotton Productivity in the State of Mato Grosso;The advance of cotton farming in the Brazilian savannah boosted and made possible a highly technified, efficient and profitable production, elevating the country from the condition of cotton fiber importer in the 70s to one of the main exporters so far. Despite the increasing contribution of technologies such as transgenic cultivars, machines, inputs and more efficient data management, in recent years there has been a stagnation of cotton productivity in the State of Mato Grosso (MT). Data Mining (MD) techniques offer an excellent opportunity to assess this problem. Through the rules-based classification applied to a real database (BD) of cotton production in MT, factors were identified that were affecting and consequently limiting the increase in productivity. In the pre-processing of the data, we perform the attributes, selection, transformation and identification of outliers. Numerical attributes were discretized using automatic techniques: Kononenko (KO), Better Encoding (BE) and combination: KO + BE. In modeling the rule algorithms used were PART and JRip, both implemented in the WEKA tool. Performance was assessed using statistical metrics: accuracy, recall, cost and their combination using the I_FC index (created by the authors). Results showed better performance for the PART classifier, with discretization by the KO + BE technique, followed by binary conversion. The analysis of the rules made it possible to identify the attributes that most impact productivity. This article is an excerpt from an ICMC/USP Professional Master's Dissertation in Science carried out in São Carlos-SP/BR.;Cotton productivity, Data mining, Classification based on rules, Machine learning;Instituto de Ciências Matemáticas e Computação da Universidade de São Paulo (ICMC – USP), São Carlos-SP, Brasil.;en_US;Published;23;2020;2020-10-04 23:10:13
108126;Carlos Manoel Pedro Vaz;Classification Based on Rules for the Study of Cotton Productivity in the State of Mato Grosso;The advance of cotton farming in the Brazilian savannah boosted and made possible a highly technified, efficient and profitable production, elevating the country from the condition of cotton fiber importer in the 70s to one of the main exporters so far. Despite the increasing contribution of technologies such as transgenic cultivars, machines, inputs and more efficient data management, in recent years there has been a stagnation of cotton productivity in the State of Mato Grosso (MT). Data Mining (MD) techniques offer an excellent opportunity to assess this problem. Through the rules-based classification applied to a real database (BD) of cotton production in MT, factors were identified that were affecting and consequently limiting the increase in productivity. In the pre-processing of the data, we perform the attributes, selection, transformation and identification of outliers. Numerical attributes were discretized using automatic techniques: Kononenko (KO), Better Encoding (BE) and combination: KO + BE. In modeling the rule algorithms used were PART and JRip, both implemented in the WEKA tool. Performance was assessed using statistical metrics: accuracy, recall, cost and their combination using the I_FC index (created by the authors). Results showed better performance for the PART classifier, with discretization by the KO + BE technique, followed by binary conversion. The analysis of the rules made it possible to identify the attributes that most impact productivity. This article is an excerpt from an ICMC/USP Professional Master's Dissertation in Science carried out in São Carlos-SP/BR.;Cotton productivity, Data mining, Classification based on rules, Machine learning;Empresa Brasileira de Pesquisa Agropecuária - Embrapa (Embrapa Instrumentação) : São Carlos, São Paulo , Brasil;en_US;Published;23;2020;2020-10-04 23:10:13
108126;Ednaldo José Ferreira;Classification Based on Rules for the Study of Cotton Productivity in the State of Mato Grosso;The advance of cotton farming in the Brazilian savannah boosted and made possible a highly technified, efficient and profitable production, elevating the country from the condition of cotton fiber importer in the 70s to one of the main exporters so far. Despite the increasing contribution of technologies such as transgenic cultivars, machines, inputs and more efficient data management, in recent years there has been a stagnation of cotton productivity in the State of Mato Grosso (MT). Data Mining (MD) techniques offer an excellent opportunity to assess this problem. Through the rules-based classification applied to a real database (BD) of cotton production in MT, factors were identified that were affecting and consequently limiting the increase in productivity. In the pre-processing of the data, we perform the attributes, selection, transformation and identification of outliers. Numerical attributes were discretized using automatic techniques: Kononenko (KO), Better Encoding (BE) and combination: KO + BE. In modeling the rule algorithms used were PART and JRip, both implemented in the WEKA tool. Performance was assessed using statistical metrics: accuracy, recall, cost and their combination using the I_FC index (created by the authors). Results showed better performance for the PART classifier, with discretization by the KO + BE technique, followed by binary conversion. The analysis of the rules made it possible to identify the attributes that most impact productivity. This article is an excerpt from an ICMC/USP Professional Master's Dissertation in Science carried out in São Carlos-SP/BR.;Cotton productivity, Data mining, Classification based on rules, Machine learning;Empresa Brasileira de Pesquisa Agropecuária - Embrapa (Embrapa Instrumentação) : São Carlos, São Paulo , Brasil;en_US;Published;23;2020;2020-10-04 23:10:13
108126;Rafael Galbieri;Classification Based on Rules for the Study of Cotton Productivity in the State of Mato Grosso;The advance of cotton farming in the Brazilian savannah boosted and made possible a highly technified, efficient and profitable production, elevating the country from the condition of cotton fiber importer in the 70s to one of the main exporters so far. Despite the increasing contribution of technologies such as transgenic cultivars, machines, inputs and more efficient data management, in recent years there has been a stagnation of cotton productivity in the State of Mato Grosso (MT). Data Mining (MD) techniques offer an excellent opportunity to assess this problem. Through the rules-based classification applied to a real database (BD) of cotton production in MT, factors were identified that were affecting and consequently limiting the increase in productivity. In the pre-processing of the data, we perform the attributes, selection, transformation and identification of outliers. Numerical attributes were discretized using automatic techniques: Kononenko (KO), Better Encoding (BE) and combination: KO + BE. In modeling the rule algorithms used were PART and JRip, both implemented in the WEKA tool. Performance was assessed using statistical metrics: accuracy, recall, cost and their combination using the I_FC index (created by the authors). Results showed better performance for the PART classifier, with discretization by the KO + BE technique, followed by binary conversion. The analysis of the rules made it possible to identify the attributes that most impact productivity. This article is an excerpt from an ICMC/USP Professional Master's Dissertation in Science carried out in São Carlos-SP/BR.;Cotton productivity, Data mining, Classification based on rules, Machine learning;Instituto Mato-grossense do Algodão, Primavera do Leste-MT, Brasil.;en_US;Published;23;2020;2020-10-04 23:10:13
110015;Luiz Fernando Braz;Using the Myers-Briggs Type Indicator (MBTI) for Modeling Multiagent Systems;The formation of high-performance teams has been a constant challenge for organizations, which despite considering human capital as one of the most important resources, it still lacks the means to allow them to have a better understanding of several factors that influence the formation of these teams. In this sense, studies also demonstrate that teamwork has a significant impact on the results presented by organizations, in which human behavior is highlighted as one of the main aspects to be considered in the building of work teams. The Myers-Briggs Type Indicator seeks to classify the behavioral preferences of individuals around eight characteristics, which grouped as dichotomies, describe different psychological types. With it, researchers have sought to expand the ability to understand the human factor, using strategies with multiagent systems that, through experiments and simulations, using computer resources, enable the development of artificial agents that simulate human actions. In this work, we present an overview of the research approaches that use MBTI to model agents, aiming at providing a better knowledge of human behavior. Additionally, we make a preliminary discussion of how these results could be explored in order to advance the studies of psychological factors' influence in organizations' work teams formation.;Multiagent systems, MAS, High-Performance Teams, MBTI;-;en_US;Published;11;2020;2020-12-18 13:03:31
110015;Jaime Simão Sichman;Using the Myers-Briggs Type Indicator (MBTI) for Modeling Multiagent Systems;The formation of high-performance teams has been a constant challenge for organizations, which despite considering human capital as one of the most important resources, it still lacks the means to allow them to have a better understanding of several factors that influence the formation of these teams. In this sense, studies also demonstrate that teamwork has a significant impact on the results presented by organizations, in which human behavior is highlighted as one of the main aspects to be considered in the building of work teams. The Myers-Briggs Type Indicator seeks to classify the behavioral preferences of individuals around eight characteristics, which grouped as dichotomies, describe different psychological types. With it, researchers have sought to expand the ability to understand the human factor, using strategies with multiagent systems that, through experiments and simulations, using computer resources, enable the development of artificial agents that simulate human actions. In this work, we present an overview of the research approaches that use MBTI to model agents, aiming at providing a better knowledge of human behavior. Additionally, we make a preliminary discussion of how these results could be explored in order to advance the studies of psychological factors' influence in organizations' work teams formation.;Multiagent systems, MAS, High-Performance Teams, MBTI;-;en_US;Published;11;2020;2020-12-18 13:03:31
110111;Alan Keller Gomes;A Novel Approach to the Measurement of Bourdieusian Social Capital within Institutional Pages and Profiles;We present in this paper a novel approach for measuring Bourdieusian Social Capital (BSC) within  Institutional Pages and Profiles. We analyse Facebook's Institutional Pages and Twitter's Institutional Profiles. Supported by Pierre Bourdie's theory, we search for directions to identify and capture data related to sociability practices, i. e. actions performed such as Like, Comment and Share. The system of symbolic exchanges and mutual recognition treated by Pierre Bourdieu is represented and extracted automatically from these data in the form of generalized sequential patterns. In this format, the social interactions captured from each page are represented as sequences of actions. Next, we also use such data to measure the frequency of occurrence of each sequence. From such frequencies, we compute the effective mobilization capacity. Finally, the volume of BSC is computed based on the capacity of effective mobilization, the number of social interactions captured and the number of followers on each page. The results are aligned with Bourdieu's theory. The approach can be generalized to institutional pages or profiles in Online Social Networks.;Online Social Networks, Bourdieusian Social Capital, Sequential Patterns;Instituto Federal de Goiás;en_US;Published;16;2020;2020-12-22 0:46:11
110111;Kaique Matheus Rodrigues Cunha;A Novel Approach to the Measurement of Bourdieusian Social Capital within Institutional Pages and Profiles;We present in this paper a novel approach for measuring Bourdieusian Social Capital (BSC) within  Institutional Pages and Profiles. We analyse Facebook's Institutional Pages and Twitter's Institutional Profiles. Supported by Pierre Bourdie's theory, we search for directions to identify and capture data related to sociability practices, i. e. actions performed such as Like, Comment and Share. The system of symbolic exchanges and mutual recognition treated by Pierre Bourdieu is represented and extracted automatically from these data in the form of generalized sequential patterns. In this format, the social interactions captured from each page are represented as sequences of actions. Next, we also use such data to measure the frequency of occurrence of each sequence. From such frequencies, we compute the effective mobilization capacity. Finally, the volume of BSC is computed based on the capacity of effective mobilization, the number of social interactions captured and the number of followers on each page. The results are aligned with Bourdieu's theory. The approach can be generalized to institutional pages or profiles in Online Social Networks.;Online Social Networks, Bourdieusian Social Capital, Sequential Patterns;-;en_US;Published;16;2020;2020-12-22 0:46:11
110111;Guilherme Augusto da Silva Ferreira;A Novel Approach to the Measurement of Bourdieusian Social Capital within Institutional Pages and Profiles;We present in this paper a novel approach for measuring Bourdieusian Social Capital (BSC) within  Institutional Pages and Profiles. We analyse Facebook's Institutional Pages and Twitter's Institutional Profiles. Supported by Pierre Bourdie's theory, we search for directions to identify and capture data related to sociability practices, i. e. actions performed such as Like, Comment and Share. The system of symbolic exchanges and mutual recognition treated by Pierre Bourdieu is represented and extracted automatically from these data in the form of generalized sequential patterns. In this format, the social interactions captured from each page are represented as sequences of actions. Next, we also use such data to measure the frequency of occurrence of each sequence. From such frequencies, we compute the effective mobilization capacity. Finally, the volume of BSC is computed based on the capacity of effective mobilization, the number of social interactions captured and the number of followers on each page. The results are aligned with Bourdieu's theory. The approach can be generalized to institutional pages or profiles in Online Social Networks.;Online Social Networks, Bourdieusian Social Capital, Sequential Patterns;-;en_US;Published;16;2020;2020-12-22 0:46:11
110818;Jeferson José Baqueta;An Adaptive Trust Model Based on Fuzzy Logic;In cooperative environments is common that agents delegate tasks to each other to achieve their goals since an agent may not have the capabilities or resources to achieve its objectives alone. However, to select good partners, the agent needs to deal with information about the abilities, experience, and goals of their partners. In this situation, the lack or inaccuracy of information may affect the agent's judgment about a given partner and hence, increases the risk to rely on an untrustworthy agent. Therefore, in this work, we present a trust model that combines different pieces of information, such as social image, reputation, and references to produce more precise information about the characteristics and abilities of agents. An important aspect of our trust model is that it can be easily configured to deal with different evaluation criteria. For instance, as presented in our experiments, the agents are able to select their partners by availability instead of the expertise level. Besides, the model allows the agents to decide when their own opinions about a partner are more relevant than the opinions received from third parties, and vice-versa. Such flexibility can be explored in dynamic scenarios, where the environment and the behavior of the agents might change constantly.;Trust Model, Reputation, Social image, Partner Selection;Universidade Tecnológica Federal do Paraná;en_US;Published;13;2021;2021-01-20 17:50:49
110818;Miriam Mariela Mercedes Morveli-Espinoza;An Adaptive Trust Model Based on Fuzzy Logic;In cooperative environments is common that agents delegate tasks to each other to achieve their goals since an agent may not have the capabilities or resources to achieve its objectives alone. However, to select good partners, the agent needs to deal with information about the abilities, experience, and goals of their partners. In this situation, the lack or inaccuracy of information may affect the agent's judgment about a given partner and hence, increases the risk to rely on an untrustworthy agent. Therefore, in this work, we present a trust model that combines different pieces of information, such as social image, reputation, and references to produce more precise information about the characteristics and abilities of agents. An important aspect of our trust model is that it can be easily configured to deal with different evaluation criteria. For instance, as presented in our experiments, the agents are able to select their partners by availability instead of the expertise level. Besides, the model allows the agents to decide when their own opinions about a partner are more relevant than the opinions received from third parties, and vice-versa. Such flexibility can be explored in dynamic scenarios, where the environment and the behavior of the agents might change constantly.;Trust Model, Reputation, Social image, Partner Selection;Universidade Tecnológica Federal do Paraná;en_US;Published;13;2021;2021-01-20 17:50:49
110818;Gustavo Alberto Giménez Lugo;An Adaptive Trust Model Based on Fuzzy Logic;In cooperative environments is common that agents delegate tasks to each other to achieve their goals since an agent may not have the capabilities or resources to achieve its objectives alone. However, to select good partners, the agent needs to deal with information about the abilities, experience, and goals of their partners. In this situation, the lack or inaccuracy of information may affect the agent's judgment about a given partner and hence, increases the risk to rely on an untrustworthy agent. Therefore, in this work, we present a trust model that combines different pieces of information, such as social image, reputation, and references to produce more precise information about the characteristics and abilities of agents. An important aspect of our trust model is that it can be easily configured to deal with different evaluation criteria. For instance, as presented in our experiments, the agents are able to select their partners by availability instead of the expertise level. Besides, the model allows the agents to decide when their own opinions about a partner are more relevant than the opinions received from third parties, and vice-versa. Such flexibility can be explored in dynamic scenarios, where the environment and the behavior of the agents might change constantly.;Trust Model, Reputation, Social image, Partner Selection;Universidade Tecnológica Federal do Paraná;en_US;Published;13;2021;2021-01-20 17:50:49
110818;Cesar Augusto Tacla;An Adaptive Trust Model Based on Fuzzy Logic;In cooperative environments is common that agents delegate tasks to each other to achieve their goals since an agent may not have the capabilities or resources to achieve its objectives alone. However, to select good partners, the agent needs to deal with information about the abilities, experience, and goals of their partners. In this situation, the lack or inaccuracy of information may affect the agent's judgment about a given partner and hence, increases the risk to rely on an untrustworthy agent. Therefore, in this work, we present a trust model that combines different pieces of information, such as social image, reputation, and references to produce more precise information about the characteristics and abilities of agents. An important aspect of our trust model is that it can be easily configured to deal with different evaluation criteria. For instance, as presented in our experiments, the agents are able to select their partners by availability instead of the expertise level. Besides, the model allows the agents to decide when their own opinions about a partner are more relevant than the opinions received from third parties, and vice-versa. Such flexibility can be explored in dynamic scenarios, where the environment and the behavior of the agents might change constantly.;Trust Model, Reputation, Social image, Partner Selection;Universidade Tecnológica Federal do Paraná;en_US;Published;13;2021;2021-01-20 17:50:49
110830;Rafhael R. Cunha;A Conceptual Model for Situating Purposes in Artificial Institutions;{In multi-agent systems, artificial institutions connect institutional concepts, belonging to the institutional reality, to the concrete elements that compose the system. The institutional reality is composed of a set of institutional concepts, called Status-Functions. Current works on artificial institutions focus on identifying the status-functions and connecting them to the concrete elements. However, the functions associated with the status-functions are implicit. As a consequence, the agents cannot reason about the functions provided by the elements that carry the status-functions and, thus, cannot exploit these functions to satisfy their goals. Considering this problem, this paper proposes a model to express the functions -- or the purposes -- associated with the status-functions. Examples illustrate the application of the model in a practical scenario, showing how the agents can use purposes to reason about the satisfaction of their goals in institutional contexts.;purposes, status-functions, artificial institutions, multi-agent systems;Federal University of the Santa Catarina;en_US;Published;12;2021;2021-01-20 19:42:43
110830;Jomi Fred Hübner;A Conceptual Model for Situating Purposes in Artificial Institutions;{In multi-agent systems, artificial institutions connect institutional concepts, belonging to the institutional reality, to the concrete elements that compose the system. The institutional reality is composed of a set of institutional concepts, called Status-Functions. Current works on artificial institutions focus on identifying the status-functions and connecting them to the concrete elements. However, the functions associated with the status-functions are implicit. As a consequence, the agents cannot reason about the functions provided by the elements that carry the status-functions and, thus, cannot exploit these functions to satisfy their goals. Considering this problem, this paper proposes a model to express the functions -- or the purposes -- associated with the status-functions. Examples illustrate the application of the model in a practical scenario, showing how the agents can use purposes to reason about the satisfaction of their goals in institutional contexts.;purposes, status-functions, artificial institutions, multi-agent systems;-;en_US;Published;12;2021;2021-01-20 19:42:43
110830;Maiquel de Brito;A Conceptual Model for Situating Purposes in Artificial Institutions;{In multi-agent systems, artificial institutions connect institutional concepts, belonging to the institutional reality, to the concrete elements that compose the system. The institutional reality is composed of a set of institutional concepts, called Status-Functions. Current works on artificial institutions focus on identifying the status-functions and connecting them to the concrete elements. However, the functions associated with the status-functions are implicit. As a consequence, the agents cannot reason about the functions provided by the elements that carry the status-functions and, thus, cannot exploit these functions to satisfy their goals. Considering this problem, this paper proposes a model to express the functions -- or the purposes -- associated with the status-functions. Examples illustrate the application of the model in a practical scenario, showing how the agents can use purposes to reason about the satisfaction of their goals in institutional contexts.;purposes, status-functions, artificial institutions, multi-agent systems;-;en_US;Published;12;2021;2021-01-20 19:42:43
110837;Lucas Fernando Souza de Castro;Integrating Embedded Multiagent Systems with Urban Simulation Tools and IoT Applications;The smart city systems development connected to the Internet of Things (IoT) has been the goal of several works in the multi-agent system field. Nevertheless, just a few projects demonstrate how to deploy and make the connection among the employed systems. This paper proposes an approach towards the integration of a MAS through the JaCaMo framework plus an Urban Simulation Tool (SUMO), IoT applications (Node-RED, InfluxDB, and Grafana), and an IoT platform (Konker). The integration presented in this paper applies in a Smart Parking scenario with real features, where is shown the integration and the connection through all layers, from agent level to artifacts, including real environment and simulation, as well as IoT applications. In future works, we intend to establish a methodology that shows how to properly integrate these different applications regardless of the scenario and the used tools.;Smart City, Multi-Agents, Urban Simulation, IoT;University of Campinas;en_US;Published;9;2021;2021-01-20 23:57:48
110837;Fabian Cesar Pereira Brandão Manoel;Integrating Embedded Multiagent Systems with Urban Simulation Tools and IoT Applications;The smart city systems development connected to the Internet of Things (IoT) has been the goal of several works in the multi-agent system field. Nevertheless, just a few projects demonstrate how to deploy and make the connection among the employed systems. This paper proposes an approach towards the integration of a MAS through the JaCaMo framework plus an Urban Simulation Tool (SUMO), IoT applications (Node-RED, InfluxDB, and Grafana), and an IoT platform (Konker). The integration presented in this paper applies in a Smart Parking scenario with real features, where is shown the integration and the connection through all layers, from agent level to artifacts, including real environment and simulation, as well as IoT applications. In future works, we intend to establish a methodology that shows how to properly integrate these different applications regardless of the scenario and the used tools.;Smart City, Multi-Agents, Urban Simulation, IoT;-;en_US;Published;9;2021;2021-01-20 23:57:48
110837;Vinicius Souza de Jesus;Integrating Embedded Multiagent Systems with Urban Simulation Tools and IoT Applications;The smart city systems development connected to the Internet of Things (IoT) has been the goal of several works in the multi-agent system field. Nevertheless, just a few projects demonstrate how to deploy and make the connection among the employed systems. This paper proposes an approach towards the integration of a MAS through the JaCaMo framework plus an Urban Simulation Tool (SUMO), IoT applications (Node-RED, InfluxDB, and Grafana), and an IoT platform (Konker). The integration presented in this paper applies in a Smart Parking scenario with real features, where is shown the integration and the connection through all layers, from agent level to artifacts, including real environment and simulation, as well as IoT applications. In future works, we intend to establish a methodology that shows how to properly integrate these different applications regardless of the scenario and the used tools.;Smart City, Multi-Agents, Urban Simulation, IoT;-;en_US;Published;9;2021;2021-01-20 23:57:48
110837;Carlos Eduardo Pantoja;Integrating Embedded Multiagent Systems with Urban Simulation Tools and IoT Applications;The smart city systems development connected to the Internet of Things (IoT) has been the goal of several works in the multi-agent system field. Nevertheless, just a few projects demonstrate how to deploy and make the connection among the employed systems. This paper proposes an approach towards the integration of a MAS through the JaCaMo framework plus an Urban Simulation Tool (SUMO), IoT applications (Node-RED, InfluxDB, and Grafana), and an IoT platform (Konker). The integration presented in this paper applies in a Smart Parking scenario with real features, where is shown the integration and the connection through all layers, from agent level to artifacts, including real environment and simulation, as well as IoT applications. In future works, we intend to establish a methodology that shows how to properly integrate these different applications regardless of the scenario and the used tools.;Smart City, Multi-Agents, Urban Simulation, IoT;-;en_US;Published;9;2021;2021-01-20 23:57:48
110837;Andre Pinz Borges;Integrating Embedded Multiagent Systems with Urban Simulation Tools and IoT Applications;The smart city systems development connected to the Internet of Things (IoT) has been the goal of several works in the multi-agent system field. Nevertheless, just a few projects demonstrate how to deploy and make the connection among the employed systems. This paper proposes an approach towards the integration of a MAS through the JaCaMo framework plus an Urban Simulation Tool (SUMO), IoT applications (Node-RED, InfluxDB, and Grafana), and an IoT platform (Konker). The integration presented in this paper applies in a Smart Parking scenario with real features, where is shown the integration and the connection through all layers, from agent level to artifacts, including real environment and simulation, as well as IoT applications. In future works, we intend to establish a methodology that shows how to properly integrate these different applications regardless of the scenario and the used tools.;Smart City, Multi-Agents, Urban Simulation, IoT;-;en_US;Published;9;2021;2021-01-20 23:57:48
110837;Gleifer Vaz Alves;Integrating Embedded Multiagent Systems with Urban Simulation Tools and IoT Applications;The smart city systems development connected to the Internet of Things (IoT) has been the goal of several works in the multi-agent system field. Nevertheless, just a few projects demonstrate how to deploy and make the connection among the employed systems. This paper proposes an approach towards the integration of a MAS through the JaCaMo framework plus an Urban Simulation Tool (SUMO), IoT applications (Node-RED, InfluxDB, and Grafana), and an IoT platform (Konker). The integration presented in this paper applies in a Smart Parking scenario with real features, where is shown the integration and the connection through all layers, from agent level to artifacts, including real environment and simulation, as well as IoT applications. In future works, we intend to establish a methodology that shows how to properly integrate these different applications regardless of the scenario and the used tools.;Smart City, Multi-Agents, Urban Simulation, IoT;-;en_US;Published;9;2021;2021-01-20 23:57:48
111817;Eanes Torres Pereira;Towards Causal Effect Estimation of Emotional Labeling of Watched Videos;Emotions play a crucial role in human life, they are measured using many approaches. There are also many methodologies for emotion elicitation. Emotion elicitation through video watching is one important approach used to create emotion datasets. However, the causation link between video content and elicited emotions was not well explained by scientific research. In this article, we present an approach for computing the causal effect of video content on elicited emotion. The Do-Calculus theory was employed for computing causal inference, and a SCM (Structured Causal Model) was proposed considering the following variables: EEG signal, age, gender, video content, like/dislike, and emotional quadrant. To evaluate the approach, EEG data were collected from volunteers watching a sample of videos from the LIRIS-ACCEDE dataset. A total of 48 causal effects was statistically evaluated in order to check the causal impact of age, gender, and video content on liking and emotion. The results show that the approach can be generalized for any dataset that contains the variables of the proposed SCM. Furthermore, the proposed approach can be applied to any other similar dataset if an appropriate SCM is provided.;affective computing, causal inference,  pattern recognition, multimedia;Unidade Acadêmica de Sistemas e Computação, Centro de Engenharia Elétrica e Informática, Universidade Federal de Campina Grande;en_US;Published;14;2021;2021-03-01 10:08:06
111817;Geovane do Nascimento Silva;Towards Causal Effect Estimation of Emotional Labeling of Watched Videos;Emotions play a crucial role in human life, they are measured using many approaches. There are also many methodologies for emotion elicitation. Emotion elicitation through video watching is one important approach used to create emotion datasets. However, the causation link between video content and elicited emotions was not well explained by scientific research. In this article, we present an approach for computing the causal effect of video content on elicited emotion. The Do-Calculus theory was employed for computing causal inference, and a SCM (Structured Causal Model) was proposed considering the following variables: EEG signal, age, gender, video content, like/dislike, and emotional quadrant. To evaluate the approach, EEG data were collected from volunteers watching a sample of videos from the LIRIS-ACCEDE dataset. A total of 48 causal effects was statistically evaluated in order to check the causal impact of age, gender, and video content on liking and emotion. The results show that the approach can be generalized for any dataset that contains the variables of the proposed SCM. Furthermore, the proposed approach can be applied to any other similar dataset if an appropriate SCM is provided.;affective computing, causal inference,  pattern recognition, multimedia;Unidade Acadêmica de Sistemas e Computação, Centro de Engenharia Elétrica e Informática, Universidade Federal de Campina Grande;en_US;Published;14;2021;2021-03-01 10:08:06
111993;Lucas Siqueira;Ab Initio Protein Structure Prediction Using Evolutionary Approach: A Survey;Protein Structure Prediction (PSP) problem is to determine the three-dimensional structure of a protein only from its primary structure. Misfolding of a protein causes human diseases. Thus, the knowledge of the structure and functionality of proteins, combined with the prediction of their structure is a complex problem and a challenge for the area of computational biology. The metaheuristic optimization algorithms are naturally applicable to support in solving NP-hard problems.These algorithms are bio-inspired, since they were designed based on procedures found in nature, such as the successful evolutionary behavior of natural systems. In this paper, we present a survey on methods to approach the \textit{ab initio} protein structure prediction based on evolutionary computing algorithms, considering both single and multi-objective optimization. An overview of the works is presented, with some details about which characteristics of the problem are considered, as well as specific points of the algorithms used. A comparison between the approaches is presented and some directions of the research field are pointed out.;Protein Structure Prediction, Evolutionary Algorithms, Ab Initio, Bioinformatics;-;en_US;Published;13;2021;2021-03-09 18:20:09
111993;Sandra Venske;Ab Initio Protein Structure Prediction Using Evolutionary Approach: A Survey;Protein Structure Prediction (PSP) problem is to determine the three-dimensional structure of a protein only from its primary structure. Misfolding of a protein causes human diseases. Thus, the knowledge of the structure and functionality of proteins, combined with the prediction of their structure is a complex problem and a challenge for the area of computational biology. The metaheuristic optimization algorithms are naturally applicable to support in solving NP-hard problems.These algorithms are bio-inspired, since they were designed based on procedures found in nature, such as the successful evolutionary behavior of natural systems. In this paper, we present a survey on methods to approach the \textit{ab initio} protein structure prediction based on evolutionary computing algorithms, considering both single and multi-objective optimization. An overview of the works is presented, with some details about which characteristics of the problem are considered, as well as specific points of the algorithms used. A comparison between the approaches is presented and some directions of the research field are pointed out.;Protein Structure Prediction, Evolutionary Algorithms, Ab Initio, Bioinformatics;-;en_US;Published;13;2021;2021-03-09 18:20:09
112006;Ricardo Kerschbaumer;The Notification Oriented Paradigm Language to Digital Hardware as an Intuitive High-level Synthesis Tool;The parallelism allowed by FPGAs has attracted attention for knowing applications that need processing power. However, the need for specific and very technical development language has not stimulate its broad use. As an alternative, there are High-level Synthesis Languages (HSL), which allow less complicated FPGA use. However, they do not tend to take full advantage of the FPGA technology. Therefore, another alternative was developed, based on the Notification Oriented Paradigm (NOP), called NOP for Digital Hardware (NOP-DH). NOP allows development in high level with its rule-oriented language called NOPL. Its entity decoupling, parallelism, and redundancy avoidance are useful for best performance. In turn, the NOP-DH brings NOP for the FPGA context with the benefits observed in software but enhanced by hardware nature. This paper reviews the NOPL for NOP-DH (NOPL-DH) that aims high level programming for FPGA. The paper proposes the NOPL-DH test by independent developers, by developing a monitoring device for a box transporting bidirectional conveyer. As a result, NOPL-DH allowed high-level development under the NOP-DH structure in an FPGA, without the need for technical knowledge and, still, maintaining and exploring the NOP properties in FPGA;FPGA, Notification Oriented Paradigm (NOP), Notification Oriented Paradigm to Digital Hardware (NOP-DH), NOP Language (NOPL), NOPL-DH, VHDL;Instituto Federal Catarinense - IFC eUniversidade Tecnológica Federal do Paraná - UTFPR;en_US;Published;16;2021;2021-03-09 10:11:26
112006;André Augusto Kaviatkovski;The Notification Oriented Paradigm Language to Digital Hardware as an Intuitive High-level Synthesis Tool;The parallelism allowed by FPGAs has attracted attention for knowing applications that need processing power. However, the need for specific and very technical development language has not stimulate its broad use. As an alternative, there are High-level Synthesis Languages (HSL), which allow less complicated FPGA use. However, they do not tend to take full advantage of the FPGA technology. Therefore, another alternative was developed, based on the Notification Oriented Paradigm (NOP), called NOP for Digital Hardware (NOP-DH). NOP allows development in high level with its rule-oriented language called NOPL. Its entity decoupling, parallelism, and redundancy avoidance are useful for best performance. In turn, the NOP-DH brings NOP for the FPGA context with the benefits observed in software but enhanced by hardware nature. This paper reviews the NOPL for NOP-DH (NOPL-DH) that aims high level programming for FPGA. The paper proposes the NOPL-DH test by independent developers, by developing a monitoring device for a box transporting bidirectional conveyer. As a result, NOPL-DH allowed high-level development under the NOP-DH structure in an FPGA, without the need for technical knowledge and, still, maintaining and exploring the NOP properties in FPGA;FPGA, Notification Oriented Paradigm (NOP), Notification Oriented Paradigm to Digital Hardware (NOP-DH), NOP Language (NOPL), NOPL-DH, VHDL;Universidade Tecnológica Federal do Paraná - UTFPR;en_US;Published;16;2021;2021-03-09 10:11:26
112006;Gabriel Rodrigues Garcia;The Notification Oriented Paradigm Language to Digital Hardware as an Intuitive High-level Synthesis Tool;The parallelism allowed by FPGAs has attracted attention for knowing applications that need processing power. However, the need for specific and very technical development language has not stimulate its broad use. As an alternative, there are High-level Synthesis Languages (HSL), which allow less complicated FPGA use. However, they do not tend to take full advantage of the FPGA technology. Therefore, another alternative was developed, based on the Notification Oriented Paradigm (NOP), called NOP for Digital Hardware (NOP-DH). NOP allows development in high level with its rule-oriented language called NOPL. Its entity decoupling, parallelism, and redundancy avoidance are useful for best performance. In turn, the NOP-DH brings NOP for the FPGA context with the benefits observed in software but enhanced by hardware nature. This paper reviews the NOPL for NOP-DH (NOPL-DH) that aims high level programming for FPGA. The paper proposes the NOPL-DH test by independent developers, by developing a monitoring device for a box transporting bidirectional conveyer. As a result, NOPL-DH allowed high-level development under the NOP-DH structure in an FPGA, without the need for technical knowledge and, still, maintaining and exploring the NOP properties in FPGA;FPGA, Notification Oriented Paradigm (NOP), Notification Oriented Paradigm to Digital Hardware (NOP-DH), NOP Language (NOPL), NOPL-DH, VHDL;Universidade Tecnológica Federal do Paraná - UTFPR;en_US;Published;16;2021;2021-03-09 10:11:26
112006;Carlos Raimundo Erig Lima;The Notification Oriented Paradigm Language to Digital Hardware as an Intuitive High-level Synthesis Tool;The parallelism allowed by FPGAs has attracted attention for knowing applications that need processing power. However, the need for specific and very technical development language has not stimulate its broad use. As an alternative, there are High-level Synthesis Languages (HSL), which allow less complicated FPGA use. However, they do not tend to take full advantage of the FPGA technology. Therefore, another alternative was developed, based on the Notification Oriented Paradigm (NOP), called NOP for Digital Hardware (NOP-DH). NOP allows development in high level with its rule-oriented language called NOPL. Its entity decoupling, parallelism, and redundancy avoidance are useful for best performance. In turn, the NOP-DH brings NOP for the FPGA context with the benefits observed in software but enhanced by hardware nature. This paper reviews the NOPL for NOP-DH (NOPL-DH) that aims high level programming for FPGA. The paper proposes the NOPL-DH test by independent developers, by developing a monitoring device for a box transporting bidirectional conveyer. As a result, NOPL-DH allowed high-level development under the NOP-DH structure in an FPGA, without the need for technical knowledge and, still, maintaining and exploring the NOP properties in FPGA;FPGA, Notification Oriented Paradigm (NOP), Notification Oriented Paradigm to Digital Hardware (NOP-DH), NOP Language (NOPL), NOPL-DH, VHDL;Universidade Tecnológica Federal do Paraná - UTFPR;en_US;Published;16;2021;2021-03-09 10:11:26
112006;Jean Marcelo Simão;The Notification Oriented Paradigm Language to Digital Hardware as an Intuitive High-level Synthesis Tool;The parallelism allowed by FPGAs has attracted attention for knowing applications that need processing power. However, the need for specific and very technical development language has not stimulate its broad use. As an alternative, there are High-level Synthesis Languages (HSL), which allow less complicated FPGA use. However, they do not tend to take full advantage of the FPGA technology. Therefore, another alternative was developed, based on the Notification Oriented Paradigm (NOP), called NOP for Digital Hardware (NOP-DH). NOP allows development in high level with its rule-oriented language called NOPL. Its entity decoupling, parallelism, and redundancy avoidance are useful for best performance. In turn, the NOP-DH brings NOP for the FPGA context with the benefits observed in software but enhanced by hardware nature. This paper reviews the NOPL for NOP-DH (NOPL-DH) that aims high level programming for FPGA. The paper proposes the NOPL-DH test by independent developers, by developing a monitoring device for a box transporting bidirectional conveyer. As a result, NOPL-DH allowed high-level development under the NOP-DH structure in an FPGA, without the need for technical knowledge and, still, maintaining and exploring the NOP properties in FPGA;FPGA, Notification Oriented Paradigm (NOP), Notification Oriented Paradigm to Digital Hardware (NOP-DH), NOP Language (NOPL), NOPL-DH, VHDL;Universidade Tecnológica Federal do Paraná - UTFPR;en_US;Published;16;2021;2021-03-09 10:11:26
112813;Otávio Augusto Maciel Camargo;A Review of Testbeds on SCADA Systems with Malware Analysis;Supervisory control and data acquisition (SCADA) systems are among the major types of Industrial Control Systems (ICS) and are responsible for monitoring and controlling essential infrastructures such as power generation, water treatment, and transportation. Very common and with high added-value, these systems have malware as one of their main threats, and due to their characteristics, it is practically impossible to test the security of a system without compromising it, requiring simulated test platforms to verify their cyber resilience. This review will discuss the most recent studies on ICS testbeds with a focus on cybersecurity and malware impact analysis.;Malware, Industrial Control Systems, SCADA, Testbed, Industry;Systems Development Center (CDS), Brasília, DF, Brazil;en_US;Published;10;2021;2021-04-07 9:44:44
112813;Julio Cesar Duarte;A Review of Testbeds on SCADA Systems with Malware Analysis;Supervisory control and data acquisition (SCADA) systems are among the major types of Industrial Control Systems (ICS) and are responsible for monitoring and controlling essential infrastructures such as power generation, water treatment, and transportation. Very common and with high added-value, these systems have malware as one of their main threats, and due to their characteristics, it is practically impossible to test the security of a system without compromising it, requiring simulated test platforms to verify their cyber resilience. This review will discuss the most recent studies on ICS testbeds with a focus on cybersecurity and malware impact analysis.;Malware, Industrial Control Systems, SCADA, Testbed, Industry;Military Institute of Engineering (IME), Rio de Janeiro, RJ, Brazil;en_US;Published;10;2021;2021-04-07 9:44:44
112813;Anderson Fernandes Pereira dos Santos;A Review of Testbeds on SCADA Systems with Malware Analysis;Supervisory control and data acquisition (SCADA) systems are among the major types of Industrial Control Systems (ICS) and are responsible for monitoring and controlling essential infrastructures such as power generation, water treatment, and transportation. Very common and with high added-value, these systems have malware as one of their main threats, and due to their characteristics, it is practically impossible to test the security of a system without compromising it, requiring simulated test platforms to verify their cyber resilience. This review will discuss the most recent studies on ICS testbeds with a focus on cybersecurity and malware impact analysis.;Malware, Industrial Control Systems, SCADA, Testbed, Industry;Military Institute of Engineering (IME), Rio de Janeiro, RJ, Brazil;en_US;Published;10;2021;2021-04-07 9:44:44
112813;Cesar Augusto Borges;A Review of Testbeds on SCADA Systems with Malware Analysis;Supervisory control and data acquisition (SCADA) systems are among the major types of Industrial Control Systems (ICS) and are responsible for monitoring and controlling essential infrastructures such as power generation, water treatment, and transportation. Very common and with high added-value, these systems have malware as one of their main threats, and due to their characteristics, it is practically impossible to test the security of a system without compromising it, requiring simulated test platforms to verify their cyber resilience. This review will discuss the most recent studies on ICS testbeds with a focus on cybersecurity and malware impact analysis.;Malware, Industrial Control Systems, SCADA, Testbed, Industry;Systems Development Center (CDS), Brasília, DF, Brazil;en_US;Published;10;2021;2021-04-07 9:44:44
116017;Rafaela de Amorim Barbosa Silva;Evaluating the Causal Effect of Multimedia and Affective Temperament in Felt Emotion and Liking;In this paper, we propose an approach to evaluate the causal effect of videos on subjects who watched movies from the LIRIS-ACCEDE dataset and from whom the following information was collected: affective temperaments, gender, and electroencephalography (EEG) signals. The affective temperament was obtained by analyzing their answers to the AFECT questionnaire. Evidence was collected from specialized literature to design a Structural Causal Model to be subjected to Do-Calculus Causal Inference. Video concepts were extracted to characterize the major video content after k-means clustering. Information from 15 volunteers was analyzed and the effects of video content, affective temperament, and gender on emotion response and liking were computed. Higher Order Crossings (HOC) were extracted from EEG signals and the features were clustered and used as intermediate evidence of affective influence. This research provides answers for the following questions about the specific watched videos: (i) How does gender affect the felt emotion and liking? (ii) How does the affective temperament of a person affect felt emotion and liking? and (iii) How does the content of a video affect felt emotion and liking? The main contribution of this paper is in the proposed methodology which can be applied to any similar dataset to investigate the causal relationships of video content and affective temperament on the emotion of the audience.;Affective Computing --- Causality --- Multimedia --- Affective Temperament;Unidade Acadêmica de Sistemas e Computação, Centro de Engenharia Elétrica e Informática, Universidade Federal de Campina Grande;en_US;Published;12;2021;2021-06-16 17:14:34
116017;Eanes Torres Pereira;Evaluating the Causal Effect of Multimedia and Affective Temperament in Felt Emotion and Liking;In this paper, we propose an approach to evaluate the causal effect of videos on subjects who watched movies from the LIRIS-ACCEDE dataset and from whom the following information was collected: affective temperaments, gender, and electroencephalography (EEG) signals. The affective temperament was obtained by analyzing their answers to the AFECT questionnaire. Evidence was collected from specialized literature to design a Structural Causal Model to be subjected to Do-Calculus Causal Inference. Video concepts were extracted to characterize the major video content after k-means clustering. Information from 15 volunteers was analyzed and the effects of video content, affective temperament, and gender on emotion response and liking were computed. Higher Order Crossings (HOC) were extracted from EEG signals and the features were clustered and used as intermediate evidence of affective influence. This research provides answers for the following questions about the specific watched videos: (i) How does gender affect the felt emotion and liking? (ii) How does the affective temperament of a person affect felt emotion and liking? and (iii) How does the content of a video affect felt emotion and liking? The main contribution of this paper is in the proposed methodology which can be applied to any similar dataset to investigate the causal relationships of video content and affective temperament on the emotion of the audience.;Affective Computing --- Causality --- Multimedia --- Affective Temperament;Unidade Acadêmica de Sistemas e Computação, Centro de Engenharia Elétrica e Informática, Universidade Federal de Campina Grande;en_US;Published;12;2021;2021-06-16 17:14:34
116606;Tiago Fernandes Tavares;Contributions on Computer Music from the SBCM 2019;The Brazilian Symposia on Computer Music are events that foster a rich environment for exciting interdisciplinary discussion. In its 17th edition, in 2019, the event was held in São João Del Rei, MG. This special issue presents 5 selected papers from the conference's technical program covering different research fields like sound synthesis, music information retrieval, sound systems, and digital musical instruments.;Brazilian Symposia on Computer Music, Computer Music, Special Issue;UNICAMPAv. Albert Einstein, 400, CEP 13083-852, Campinas, SP, Brasil, Sala 311 | +55 19 3521-3773;en_US;Published;3;2021;2021-07-07 19:40:02
116606;Flávio Luiz Schiavoni;Contributions on Computer Music from the SBCM 2019;The Brazilian Symposia on Computer Music are events that foster a rich environment for exciting interdisciplinary discussion. In its 17th edition, in 2019, the event was held in São João Del Rei, MG. This special issue presents 5 selected papers from the conference's technical program covering different research fields like sound synthesis, music information retrieval, sound systems, and digital musical instruments.;Brazilian Symposia on Computer Music, Computer Music, Special Issue;Universidade Federal de São João Del-Rei.Av. Visconde do Rio Preto, s/nº, Colônia do BengoVila São Paulo (Fábricas)36301360 - São João del Rei, MG - BrasilTelefone: (32) 33728192;en_US;Published;3;2021;2021-07-07 19:40:02
117481;Daniel Felix Brito;Exploring Supervised Techniques for Automated Recognition of Intention Classes from Portuguese Free Texts on Agriculture;Technical and scientific knowledge is vast and complex, particularly in interdisciplinary fields such as sustainable agriculture, which is available in several interrelated, geographically dispersed and interdisciplinary online textual information sources. In this context, it is essential to support people with computational mechanisms that allow them to retrieve and interpret information in an appropriate way, as communication in these software systems is typically asynchronous and textual. User’s intention recognition and analysis in textual documents results in benefits for better information retrieval. However, intentions are expressed implicitly in texts in natural language and the specificities of the domain and cultural aspects of language make it difficult to process and analyze the text by computer systems. This requires the study of methods for the automatic recognition of intention classes in text. In this article, we conduct extensive experimental analyses on techniques based on language models and machine learning to detect instances of intention classes in texts about sustainable agriculture written in Portuguese. In our methodology, we perform a morphological analysis of the sentences and evaluate four Word Embeddings techniques (Word2Vec, Wang2Vec, FastText and Glove) combined with four machine learning techniques (Support Vector Machine, Artificial Neural Network, Random Forest and Transfer Learning). The results obtained by applying the techniques proposed in a database with textual information on sustainable agriculture indicate promising possibilities in the recognition of intentions in free texts  in  Portuguese language on sustainable agriculture.;Intention Detection, Sustainable Agriculture, Word Embeddings, Illocution Class, Intentions Recognition, Machine Learning;CTI Renato Archer, Campinas, São Paulo, Brasil;en_US;Published;25;2021;2021-08-06 12:24:09
117481;Jarbas Lopes Cardoso Júnior;Exploring Supervised Techniques for Automated Recognition of Intention Classes from Portuguese Free Texts on Agriculture;Technical and scientific knowledge is vast and complex, particularly in interdisciplinary fields such as sustainable agriculture, which is available in several interrelated, geographically dispersed and interdisciplinary online textual information sources. In this context, it is essential to support people with computational mechanisms that allow them to retrieve and interpret information in an appropriate way, as communication in these software systems is typically asynchronous and textual. User’s intention recognition and analysis in textual documents results in benefits for better information retrieval. However, intentions are expressed implicitly in texts in natural language and the specificities of the domain and cultural aspects of language make it difficult to process and analyze the text by computer systems. This requires the study of methods for the automatic recognition of intention classes in text. In this article, we conduct extensive experimental analyses on techniques based on language models and machine learning to detect instances of intention classes in texts about sustainable agriculture written in Portuguese. In our methodology, we perform a morphological analysis of the sentences and evaluate four Word Embeddings techniques (Word2Vec, Wang2Vec, FastText and Glove) combined with four machine learning techniques (Support Vector Machine, Artificial Neural Network, Random Forest and Transfer Learning). The results obtained by applying the techniques proposed in a database with textual information on sustainable agriculture indicate promising possibilities in the recognition of intentions in free texts  in  Portuguese language on sustainable agriculture.;Intention Detection, Sustainable Agriculture, Word Embeddings, Illocution Class, Intentions Recognition, Machine Learning;CTI Renato Archer, Campinas, São Paulo, Brasil;en_US;Published;25;2021;2021-08-06 12:24:09
117481;Júlio Cesar dos Reis;Exploring Supervised Techniques for Automated Recognition of Intention Classes from Portuguese Free Texts on Agriculture;Technical and scientific knowledge is vast and complex, particularly in interdisciplinary fields such as sustainable agriculture, which is available in several interrelated, geographically dispersed and interdisciplinary online textual information sources. In this context, it is essential to support people with computational mechanisms that allow them to retrieve and interpret information in an appropriate way, as communication in these software systems is typically asynchronous and textual. User’s intention recognition and analysis in textual documents results in benefits for better information retrieval. However, intentions are expressed implicitly in texts in natural language and the specificities of the domain and cultural aspects of language make it difficult to process and analyze the text by computer systems. This requires the study of methods for the automatic recognition of intention classes in text. In this article, we conduct extensive experimental analyses on techniques based on language models and machine learning to detect instances of intention classes in texts about sustainable agriculture written in Portuguese. In our methodology, we perform a morphological analysis of the sentences and evaluate four Word Embeddings techniques (Word2Vec, Wang2Vec, FastText and Glove) combined with four machine learning techniques (Support Vector Machine, Artificial Neural Network, Random Forest and Transfer Learning). The results obtained by applying the techniques proposed in a database with textual information on sustainable agriculture indicate promising possibilities in the recognition of intentions in free texts  in  Portuguese language on sustainable agriculture.;Intention Detection, Sustainable Agriculture, Word Embeddings, Illocution Class, Intentions Recognition, Machine Learning;Universidade Estadual de Campinas, Campinas, São Paulo, Brasil;en_US;Published;25;2021;2021-08-06 12:24:09
117481;Guilherme Ruppert;Exploring Supervised Techniques for Automated Recognition of Intention Classes from Portuguese Free Texts on Agriculture;Technical and scientific knowledge is vast and complex, particularly in interdisciplinary fields such as sustainable agriculture, which is available in several interrelated, geographically dispersed and interdisciplinary online textual information sources. In this context, it is essential to support people with computational mechanisms that allow them to retrieve and interpret information in an appropriate way, as communication in these software systems is typically asynchronous and textual. User’s intention recognition and analysis in textual documents results in benefits for better information retrieval. However, intentions are expressed implicitly in texts in natural language and the specificities of the domain and cultural aspects of language make it difficult to process and analyze the text by computer systems. This requires the study of methods for the automatic recognition of intention classes in text. In this article, we conduct extensive experimental analyses on techniques based on language models and machine learning to detect instances of intention classes in texts about sustainable agriculture written in Portuguese. In our methodology, we perform a morphological analysis of the sentences and evaluate four Word Embeddings techniques (Word2Vec, Wang2Vec, FastText and Glove) combined with four machine learning techniques (Support Vector Machine, Artificial Neural Network, Random Forest and Transfer Learning). The results obtained by applying the techniques proposed in a database with textual information on sustainable agriculture indicate promising possibilities in the recognition of intentions in free texts  in  Portuguese language on sustainable agriculture.;Intention Detection, Sustainable Agriculture, Word Embeddings, Illocution Class, Intentions Recognition, Machine Learning;CTI Renato Archer, Campinas, São Paulo, Brasil;en_US;Published;25;2021;2021-08-06 12:24:09
117481;Rodrigo Bonacin Bonacin;Exploring Supervised Techniques for Automated Recognition of Intention Classes from Portuguese Free Texts on Agriculture;Technical and scientific knowledge is vast and complex, particularly in interdisciplinary fields such as sustainable agriculture, which is available in several interrelated, geographically dispersed and interdisciplinary online textual information sources. In this context, it is essential to support people with computational mechanisms that allow them to retrieve and interpret information in an appropriate way, as communication in these software systems is typically asynchronous and textual. User’s intention recognition and analysis in textual documents results in benefits for better information retrieval. However, intentions are expressed implicitly in texts in natural language and the specificities of the domain and cultural aspects of language make it difficult to process and analyze the text by computer systems. This requires the study of methods for the automatic recognition of intention classes in text. In this article, we conduct extensive experimental analyses on techniques based on language models and machine learning to detect instances of intention classes in texts about sustainable agriculture written in Portuguese. In our methodology, we perform a morphological analysis of the sentences and evaluate four Word Embeddings techniques (Word2Vec, Wang2Vec, FastText and Glove) combined with four machine learning techniques (Support Vector Machine, Artificial Neural Network, Random Forest and Transfer Learning). The results obtained by applying the techniques proposed in a database with textual information on sustainable agriculture indicate promising possibilities in the recognition of intentions in free texts  in  Portuguese language on sustainable agriculture.;Intention Detection, Sustainable Agriculture, Word Embeddings, Illocution Class, Intentions Recognition, Machine Learning;Unifaccamp and CTI Renato Archer, Campinas, São Paulo, Brasil;en_US;Published;25;2021;2021-08-06 12:24:09
118372;Raphael Rocha da Silva;Building Contrastive Summaries of Subjective Text Via Opinion Ranking;This article investigates methods to automatically compare entities from opinionated text to help users to obtain important information from a large amount of data, a task known as “contrastive opinion summarization”. The task aims at generating contrastive summaries that highlight differences between entities given opinionated text (written about each entity individually) where opinions have been previously identified. These summaries are made by selecting sentences from the input data. The core of the problem is to find out how to choose these more relevant sentences in an appropriate manner. The proposed method uses a heuristic that makesdecisions according to the opinions found in the input text and to traits that a summary is expected to present. The evaluation is made by measuring three characteristics that contrastive summaries are expected to have: representativity (presence of opinions that are frequent in the input), contrastivity (presence of opinions that highlight differences between entities) and diversity (presence of different opinions to avoid redundancy). The novel method is compared to methods previously published and performs significantly better than them according to the measures used. The main contributions of this work are: a comparative analysis of methods of contrastive opinion summarization, the proposal of a systematic way to evaluate summaries, the development of a new method that performs better than others previously known and the creation of a dataset for the task.;opinion mining, evaluation, summarization;Interinstitutional Center for Computational Linguistics (NILC), Institute of Mathematical and Computer Sciences, University of São Paulo. São Carlos/SP, Brazil;en_US;Published;23;2021;2021-09-14 10:41:15
118372;Thiago Alexandre Salgueiro Pardo;Building Contrastive Summaries of Subjective Text Via Opinion Ranking;This article investigates methods to automatically compare entities from opinionated text to help users to obtain important information from a large amount of data, a task known as “contrastive opinion summarization”. The task aims at generating contrastive summaries that highlight differences between entities given opinionated text (written about each entity individually) where opinions have been previously identified. These summaries are made by selecting sentences from the input data. The core of the problem is to find out how to choose these more relevant sentences in an appropriate manner. The proposed method uses a heuristic that makesdecisions according to the opinions found in the input text and to traits that a summary is expected to present. The evaluation is made by measuring three characteristics that contrastive summaries are expected to have: representativity (presence of opinions that are frequent in the input), contrastivity (presence of opinions that highlight differences between entities) and diversity (presence of different opinions to avoid redundancy). The novel method is compared to methods previously published and performs significantly better than them according to the measures used. The main contributions of this work are: a comparative analysis of methods of contrastive opinion summarization, the proposal of a systematic way to evaluate summaries, the development of a new method that performs better than others previously known and the creation of a dataset for the task.;opinion mining, evaluation, summarization;Interinstitutional Center for Computational Linguistics (NILC), Institute of Mathematical and Computer Sciences, University of São Paulo. São Carlos/SP, Brazil;en_US;Published;23;2021;2021-09-14 10:41:15
119164;Gustavo Caetano Borges;SSM: A Semantic Metasearch Platform for Scientific Data retrieval;Scientific research in all fields has advanced in complexity and in the amount of data generated. The heterogeneity of data repositories, data meaning and their metadata standards makes this problem even more significant. In spite of several proposals to find and retrieve research data from public repositories, there is still need for more comprehensive retrieval solutions. In this article, we specify and develop a mechanism to search for scientific data that takes advantage of metadata records and semantic methods. We present the conception of our architecture and how we have implemented it in a use case in the agriculture domain.;Metadata, Query expansion, ontology, agriculture;Universidade Estadual de Campinas;en_US;Published;10;2021;2021-10-07 17:09:00
119164;Julio Cesar dos Reis;SSM: A Semantic Metasearch Platform for Scientific Data retrieval;Scientific research in all fields has advanced in complexity and in the amount of data generated. The heterogeneity of data repositories, data meaning and their metadata standards makes this problem even more significant. In spite of several proposals to find and retrieve research data from public repositories, there is still need for more comprehensive retrieval solutions. In this article, we specify and develop a mechanism to search for scientific data that takes advantage of metadata records and semantic methods. We present the conception of our architecture and how we have implemented it in a use case in the agriculture domain.;Metadata, Query expansion, ontology, agriculture;Universidade Estadual de Campinas;en_US;Published;10;2021;2021-10-07 17:09:00
119164;Claudia Bauzer Medeiros;SSM: A Semantic Metasearch Platform for Scientific Data retrieval;Scientific research in all fields has advanced in complexity and in the amount of data generated. The heterogeneity of data repositories, data meaning and their metadata standards makes this problem even more significant. In spite of several proposals to find and retrieve research data from public repositories, there is still need for more comprehensive retrieval solutions. In this article, we specify and develop a mechanism to search for scientific data that takes advantage of metadata records and semantic methods. We present the conception of our architecture and how we have implemented it in a use case in the agriculture domain.;Metadata, Query expansion, ontology, agriculture;Universidade Estadual de Campinas;en_US;Published;10;2021;2021-10-07 17:09:00
119196;Cleyton Ferreira Gonçalves;Stochastic Models for Planning VLE Moodle Environments based on Containers and Virtual Machines;Moodle Virtual Learning Environments (VLEs) represent tools of a pedagogical dimension where the teacher uses various resources to stimulate student learning. Content presented in hypertext, audio or vídeo formats can be adopted as a means to facilitate the learning. These platforms tend to produce high processing rates on servers, large volumes of data on the network and, consequently, degrade performance, increase energy consumption and costs. However, to provide eficiente sharing of computing resources and at the same time minimize financial costs, these VLE platforms typically run on virtualized infrastructures such as Virtual Machines (VM) or containers, which have advantages and disadvantages. Stochastic models, such as stochastic Petri nets (SPNs), can be used in the modeling and evaluation of such environments. Therefore, this work aims to use analytical modeling through SPNs to assess the performance, energy consumption and cost of environments based on containers and VMs. Metrics such as throughput, response time, energy consumption and cost are collected and analyzed. The results revealed that, for example, a cluster with 10 replicas, occupied at their maximum capacity, can generate a 46.54% reduction in energy consumption if containers are used. Additionally, we validate the accuracy of the analytical models by comparing their results with the results obtained in a real infrastructure.;container, virtual machine, performance evaluation, energy consumption, cost evaluation, stochastic petri nets;Departamento de Computação, Universidade Federal Rural de Pernambuco, Recife, Pernambuco, Brasil;en_US;Published;20;2021;2021-10-09 2:41:58
119196;Ermeson Carneiro Andrade;Stochastic Models for Planning VLE Moodle Environments based on Containers and Virtual Machines;Moodle Virtual Learning Environments (VLEs) represent tools of a pedagogical dimension where the teacher uses various resources to stimulate student learning. Content presented in hypertext, audio or vídeo formats can be adopted as a means to facilitate the learning. These platforms tend to produce high processing rates on servers, large volumes of data on the network and, consequently, degrade performance, increase energy consumption and costs. However, to provide eficiente sharing of computing resources and at the same time minimize financial costs, these VLE platforms typically run on virtualized infrastructures such as Virtual Machines (VM) or containers, which have advantages and disadvantages. Stochastic models, such as stochastic Petri nets (SPNs), can be used in the modeling and evaluation of such environments. Therefore, this work aims to use analytical modeling through SPNs to assess the performance, energy consumption and cost of environments based on containers and VMs. Metrics such as throughput, response time, energy consumption and cost are collected and analyzed. The results revealed that, for example, a cluster with 10 replicas, occupied at their maximum capacity, can generate a 46.54% reduction in energy consumption if containers are used. Additionally, we validate the accuracy of the analytical models by comparing their results with the results obtained in a real infrastructure.;container, virtual machine, performance evaluation, energy consumption, cost evaluation, stochastic petri nets;Departamento de Computação, Universidade Federal Rural de Pernambuco, Recife, Pernambuco, Brasil;en_US;Published;20;2021;2021-10-09 2:41:58
119196;Júlio Rodrigues de Mendonça Neto;Stochastic Models for Planning VLE Moodle Environments based on Containers and Virtual Machines;Moodle Virtual Learning Environments (VLEs) represent tools of a pedagogical dimension where the teacher uses various resources to stimulate student learning. Content presented in hypertext, audio or vídeo formats can be adopted as a means to facilitate the learning. These platforms tend to produce high processing rates on servers, large volumes of data on the network and, consequently, degrade performance, increase energy consumption and costs. However, to provide eficiente sharing of computing resources and at the same time minimize financial costs, these VLE platforms typically run on virtualized infrastructures such as Virtual Machines (VM) or containers, which have advantages and disadvantages. Stochastic models, such as stochastic Petri nets (SPNs), can be used in the modeling and evaluation of such environments. Therefore, this work aims to use analytical modeling through SPNs to assess the performance, energy consumption and cost of environments based on containers and VMs. Metrics such as throughput, response time, energy consumption and cost are collected and analyzed. The results revealed that, for example, a cluster with 10 replicas, occupied at their maximum capacity, can generate a 46.54% reduction in energy consumption if containers are used. Additionally, we validate the accuracy of the analytical models by comparing their results with the results obtained in a real infrastructure.;container, virtual machine, performance evaluation, energy consumption, cost evaluation, stochastic petri nets; Coordenação de Informática, Instituto Federal de Alagoas, Maceió, Alagoas, Brasil;en_US;Published;20;2021;2021-10-09 2:41:58
119196;Gustavo Rau de Almeida Callou;Stochastic Models for Planning VLE Moodle Environments based on Containers and Virtual Machines;Moodle Virtual Learning Environments (VLEs) represent tools of a pedagogical dimension where the teacher uses various resources to stimulate student learning. Content presented in hypertext, audio or vídeo formats can be adopted as a means to facilitate the learning. These platforms tend to produce high processing rates on servers, large volumes of data on the network and, consequently, degrade performance, increase energy consumption and costs. However, to provide eficiente sharing of computing resources and at the same time minimize financial costs, these VLE platforms typically run on virtualized infrastructures such as Virtual Machines (VM) or containers, which have advantages and disadvantages. Stochastic models, such as stochastic Petri nets (SPNs), can be used in the modeling and evaluation of such environments. Therefore, this work aims to use analytical modeling through SPNs to assess the performance, energy consumption and cost of environments based on containers and VMs. Metrics such as throughput, response time, energy consumption and cost are collected and analyzed. The results revealed that, for example, a cluster with 10 replicas, occupied at their maximum capacity, can generate a 46.54% reduction in energy consumption if containers are used. Additionally, we validate the accuracy of the analytical models by comparing their results with the results obtained in a real infrastructure.;container, virtual machine, performance evaluation, energy consumption, cost evaluation, stochastic petri nets;Departamento de Computação, Universidade Federal Rural de Pernambuco, Recife, Pernambuco, Brasil;en_US;Published;20;2021;2021-10-09 2:41:58
119207;Marcelo Luis Rodrigues Filho;Efficient Breast Cancer Classification Using Histopathological Images and a Simple VGG;Breast cancer is the second most deadly disease worldwide. This severe condition led to 627,000 people dying in 2018. Thus, early detection is critical for improving the patients' lifetime or even curing them. In this context, we can appeal to Medicine 4.0, which exploits machine learning capabilities to obtain a faster and more efficient diagnosis. Therefore, this work aims to apply a simpler convolutional neural network, called VGG-7, for classifying breast cancer in histopathological images. Results have shown that VGG-7 overcomes the performance of VGG-16 and VGG-19, showing an accuracy of 98%, a precision of 99%, a recall of 98%, and an F1 score of 98%.;breast cancer, machine learning, histopathological images, convolutional neural network;Instituto Federal do Maranhão;en_US;Published;12;2021;2021-10-10 21:52:51
119207;Omar Andres Carmona Cortes;Efficient Breast Cancer Classification Using Histopathological Images and a Simple VGG;Breast cancer is the second most deadly disease worldwide. This severe condition led to 627,000 people dying in 2018. Thus, early detection is critical for improving the patients' lifetime or even curing them. In this context, we can appeal to Medicine 4.0, which exploits machine learning capabilities to obtain a faster and more efficient diagnosis. Therefore, this work aims to apply a simpler convolutional neural network, called VGG-7, for classifying breast cancer in histopathological images. Results have shown that VGG-7 overcomes the performance of VGG-16 and VGG-19, showing an accuracy of 98%, a precision of 99%, a recall of 98%, and an F1 score of 98%.;breast cancer, machine learning, histopathological images, convolutional neural network;Instituto Federal do Maranhão;en_US;Published;12;2021;2021-10-10 21:52:51
120399;Allan Matheus Marques dos Santos;Application of Profile Prediction for Proactive Scheduling;Today, cloud environments are widely used as execution platforms for most applications. In these environments, virtualized applications often share computing resources. Although this increases hardware utilization, resources competition can cause performance degradation, and knowing which applications can run on the same host without causing too much interference is key to a better scheduling and performance. Therefore, it is important to predict the resource consumption profile of applications in their subsequent iterations. This work evaluates the use of machine learning techniques to predict the increase or decrease in computational resources consumption. The prediction models are evaluated through experiments using real and benchmark applications. Finally, we conclude that some models offer significantly better performance when compared to the current trend of resource usage. These models averaged up to 94% on the F1 metric for this task.;application profile, scheduling, cloud computing, intelligent agents;Instituto Militar de Engenharia;en_US;Published;10;2021;2021-11-29 15:37:34
120399;Raquel Coelho Gomes  Pinto;Application of Profile Prediction for Proactive Scheduling;Today, cloud environments are widely used as execution platforms for most applications. In these environments, virtualized applications often share computing resources. Although this increases hardware utilization, resources competition can cause performance degradation, and knowing which applications can run on the same host without causing too much interference is key to a better scheduling and performance. Therefore, it is important to predict the resource consumption profile of applications in their subsequent iterations. This work evaluates the use of machine learning techniques to predict the increase or decrease in computational resources consumption. The prediction models are evaluated through experiments using real and benchmark applications. Finally, we conclude that some models offer significantly better performance when compared to the current trend of resource usage. These models averaged up to 94% on the F1 metric for this task.;application profile, scheduling, cloud computing, intelligent agents;Instituto Militar de Engenharia;en_US;Published;10;2021;2021-11-29 15:37:34
120399;Julio Cesar  Duarte;Application of Profile Prediction for Proactive Scheduling;Today, cloud environments are widely used as execution platforms for most applications. In these environments, virtualized applications often share computing resources. Although this increases hardware utilization, resources competition can cause performance degradation, and knowing which applications can run on the same host without causing too much interference is key to a better scheduling and performance. Therefore, it is important to predict the resource consumption profile of applications in their subsequent iterations. This work evaluates the use of machine learning techniques to predict the increase or decrease in computational resources consumption. The prediction models are evaluated through experiments using real and benchmark applications. Finally, we conclude that some models offer significantly better performance when compared to the current trend of resource usage. These models averaged up to 94% on the F1 metric for this task.;application profile, scheduling, cloud computing, intelligent agents;Instituto Militar de Engenharia;en_US;Published;10;2021;2021-11-29 15:37:34
120399;Bruno Richard  Schulze;Application of Profile Prediction for Proactive Scheduling;Today, cloud environments are widely used as execution platforms for most applications. In these environments, virtualized applications often share computing resources. Although this increases hardware utilization, resources competition can cause performance degradation, and knowing which applications can run on the same host without causing too much interference is key to a better scheduling and performance. Therefore, it is important to predict the resource consumption profile of applications in their subsequent iterations. This work evaluates the use of machine learning techniques to predict the increase or decrease in computational resources consumption. The prediction models are evaluated through experiments using real and benchmark applications. Finally, we conclude that some models offer significantly better performance when compared to the current trend of resource usage. These models averaged up to 94% on the F1 metric for this task.;application profile, scheduling, cloud computing, intelligent agents;National Laboratory of Scientific Computing;en_US;Published;10;2021;2021-11-29 15:37:34
121388;Joelson Miller Bezerra de Souza;Optimal Control and Adaptive Learning for Stabilization of a Quadrotor-type Unmanned Aerial Vehicle via Approximate Dynamic Programming;The development of an optimal controller for stabilization of a quadrotor system using an adaptive critic structure based on policy iteration schemes is proposed in this paper. This approach is inserted in the context of Approximate Dynamic Programming and it is used to solve optimal decision problems on-line, without requiring complete knowledge of the system dynamics model to be controlled. The main feature of the adaptive critic design method that allows for on-line implementation is that it solves the Bellman optimality equation in a forward-in-time fashion, whereas traditional dynamic programming requires a backward-in-time procedure. This feedback control design technique is able to tune the controller parameters on-line in the presence of variations in plant dynamics and external disturbances using data measured along the system trajectories. Computational simulation results based on a quadrotor model demonstrate the effectiveness of the proposed control scheme.;Optimal Control, Quadrotor, Policy Iteration, Adaptive Critic Design, Approximate Dynamic Programming;Universidade Estadual do Maranhão;en_US;Published;14;2022;2022-01-05 23:57:10
121388;Patrícia Helena Moraes Rêgo;Optimal Control and Adaptive Learning for Stabilization of a Quadrotor-type Unmanned Aerial Vehicle via Approximate Dynamic Programming;The development of an optimal controller for stabilization of a quadrotor system using an adaptive critic structure based on policy iteration schemes is proposed in this paper. This approach is inserted in the context of Approximate Dynamic Programming and it is used to solve optimal decision problems on-line, without requiring complete knowledge of the system dynamics model to be controlled. The main feature of the adaptive critic design method that allows for on-line implementation is that it solves the Bellman optimality equation in a forward-in-time fashion, whereas traditional dynamic programming requires a backward-in-time procedure. This feedback control design technique is able to tune the controller parameters on-line in the presence of variations in plant dynamics and external disturbances using data measured along the system trajectories. Computational simulation results based on a quadrotor model demonstrate the effectiveness of the proposed control scheme.;Optimal Control, Quadrotor, Policy Iteration, Adaptive Critic Design, Approximate Dynamic Programming;Universidade Estadual do Maranhão;en_US;Published;14;2022;2022-01-05 23:57:10
121388;Guilherme Bonfim Sousa;Optimal Control and Adaptive Learning for Stabilization of a Quadrotor-type Unmanned Aerial Vehicle via Approximate Dynamic Programming;The development of an optimal controller for stabilization of a quadrotor system using an adaptive critic structure based on policy iteration schemes is proposed in this paper. This approach is inserted in the context of Approximate Dynamic Programming and it is used to solve optimal decision problems on-line, without requiring complete knowledge of the system dynamics model to be controlled. The main feature of the adaptive critic design method that allows for on-line implementation is that it solves the Bellman optimality equation in a forward-in-time fashion, whereas traditional dynamic programming requires a backward-in-time procedure. This feedback control design technique is able to tune the controller parameters on-line in the presence of variations in plant dynamics and external disturbances using data measured along the system trajectories. Computational simulation results based on a quadrotor model demonstrate the effectiveness of the proposed control scheme.;Optimal Control, Quadrotor, Policy Iteration, Adaptive Critic Design, Approximate Dynamic Programming;Universidade Estadual do Maranhão;en_US;Published;14;2022;2022-01-05 23:57:10
121388;Janes Valdo Rodrigues Lima;Optimal Control and Adaptive Learning for Stabilization of a Quadrotor-type Unmanned Aerial Vehicle via Approximate Dynamic Programming;The development of an optimal controller for stabilization of a quadrotor system using an adaptive critic structure based on policy iteration schemes is proposed in this paper. This approach is inserted in the context of Approximate Dynamic Programming and it is used to solve optimal decision problems on-line, without requiring complete knowledge of the system dynamics model to be controlled. The main feature of the adaptive critic design method that allows for on-line implementation is that it solves the Bellman optimality equation in a forward-in-time fashion, whereas traditional dynamic programming requires a backward-in-time procedure. This feedback control design technique is able to tune the controller parameters on-line in the presence of variations in plant dynamics and external disturbances using data measured along the system trajectories. Computational simulation results based on a quadrotor model demonstrate the effectiveness of the proposed control scheme.;Optimal Control, Quadrotor, Policy Iteration, Adaptive Critic Design, Approximate Dynamic Programming;Universidade Estadual do Maranhão;en_US;Published;14;2022;2022-01-05 23:57:10
121425;Daniel W. S. Rocha;SEGBEE: Mobile Application for Honey Segmentation in Apiary Boards;Beekeeping is one of the most important activities for humans. Since ancient times, honey has been used in the treatment of several diseases and is an extremely powerful antioxidant. The process of visual analysis of the apiary requires trained specialists who try to obtain relevant information to make a decision about what to do with the honeycomb. Since the process is performed manually, given the complexity of the task, opportunities arise for the application of automated systems that can assist the beekeeper's decision making. Thus, this paper presents the development of the application \textit{SegBee}, a computational tool that performs the segmentation in the apiary plates, where there is the presence of honey, in an accessible, fast and practical way. To do this, the OpenCV library was used for the digital image processing part, and the Kivy library was used to develop the interface of the mobile application. The tests performed showed that the images were adequately segmented by \textit{SegBee}, indicating where the honey is located on each analyzed plate. A visual comparison was made between results obtained by \textit{SegBee} and another commercial application, demonstrating the effectiveness of the developed tool. The proposed solution contributes to the improvement of the beekeeping professionals' work, once the application is simple to use and fast to process, being able to help in the honey identification task in apiaries plates.;Beekeeping, Digital Image Processing, Mobile Application, Segmentation;Instituto Federal de Minas Gerais;en_US;Published;10;2022;2022-01-07 19:47:24
121425;Eduardo Cardoso Melo;SEGBEE: Mobile Application for Honey Segmentation in Apiary Boards;Beekeeping is one of the most important activities for humans. Since ancient times, honey has been used in the treatment of several diseases and is an extremely powerful antioxidant. The process of visual analysis of the apiary requires trained specialists who try to obtain relevant information to make a decision about what to do with the honeycomb. Since the process is performed manually, given the complexity of the task, opportunities arise for the application of automated systems that can assist the beekeeper's decision making. Thus, this paper presents the development of the application \textit{SegBee}, a computational tool that performs the segmentation in the apiary plates, where there is the presence of honey, in an accessible, fast and practical way. To do this, the OpenCV library was used for the digital image processing part, and the Kivy library was used to develop the interface of the mobile application. The tests performed showed that the images were adequately segmented by \textit{SegBee}, indicating where the honey is located on each analyzed plate. A visual comparison was made between results obtained by \textit{SegBee} and another commercial application, demonstrating the effectiveness of the developed tool. The proposed solution contributes to the improvement of the beekeeping professionals' work, once the application is simple to use and fast to process, being able to help in the honey identification task in apiaries plates.;Beekeeping, Digital Image Processing, Mobile Application, Segmentation;Instituto Federal de Minas Gerais;en_US;Published;10;2022;2022-01-07 19:47:24
121425;Bruno Alberto Soares Oliveira;SEGBEE: Mobile Application for Honey Segmentation in Apiary Boards;Beekeeping is one of the most important activities for humans. Since ancient times, honey has been used in the treatment of several diseases and is an extremely powerful antioxidant. The process of visual analysis of the apiary requires trained specialists who try to obtain relevant information to make a decision about what to do with the honeycomb. Since the process is performed manually, given the complexity of the task, opportunities arise for the application of automated systems that can assist the beekeeper's decision making. Thus, this paper presents the development of the application \textit{SegBee}, a computational tool that performs the segmentation in the apiary plates, where there is the presence of honey, in an accessible, fast and practical way. To do this, the OpenCV library was used for the digital image processing part, and the Kivy library was used to develop the interface of the mobile application. The tests performed showed that the images were adequately segmented by \textit{SegBee}, indicating where the honey is located on each analyzed plate. A visual comparison was made between results obtained by \textit{SegBee} and another commercial application, demonstrating the effectiveness of the developed tool. The proposed solution contributes to the improvement of the beekeeping professionals' work, once the application is simple to use and fast to process, being able to help in the honey identification task in apiaries plates.;Beekeeping, Digital Image Processing, Mobile Application, Segmentation;Universidade Federal de Minas Gerais;en_US;Published;10;2022;2022-01-07 19:47:24
122929;Candy A.  Huanca-Anquise;Multi-Objective, Multi-Armed Bandits: Algorithms for Repeated Games and Application to Route Choice;Multi-objective decision-making in multi-agent scenarios poses multiple challenges. Dealing with multiple objectives and non-stationarity caused by simultaneous learning are only two of them, which have been addressed separately. In this work, reinforcement learning algorithms that tackle both issues together are proposed and applied to a route choice problem, where drivers must select an action in a single-state formulation, while aiming to minimize both their travel time and toll. Hence, we deal with repeated games, now with a multi-objective approach. Advantages, limitations and differences of these algorithms are discussed. Our results show that the proposed algorithms for action selection using reinforcement learning deal with non-stationarity and multiple objectives, while providing alternative solutions to those of centralized methods.;Multi-objective decision-making, Multi-objective route choice, Reinforcement learning, Repeated games, Multiagent systems, Multi-armed Bandit algorithms;Federal University of Rio Grande do Sul;en_US;Published;12;2022;2022-03-18 12:46:32
122929;Ana Lúcia Cetertich Bazzan;Multi-Objective, Multi-Armed Bandits: Algorithms for Repeated Games and Application to Route Choice;Multi-objective decision-making in multi-agent scenarios poses multiple challenges. Dealing with multiple objectives and non-stationarity caused by simultaneous learning are only two of them, which have been addressed separately. In this work, reinforcement learning algorithms that tackle both issues together are proposed and applied to a route choice problem, where drivers must select an action in a single-state formulation, while aiming to minimize both their travel time and toll. Hence, we deal with repeated games, now with a multi-objective approach. Advantages, limitations and differences of these algorithms are discussed. Our results show that the proposed algorithms for action selection using reinforcement learning deal with non-stationarity and multiple objectives, while providing alternative solutions to those of centralized methods.;Multi-objective decision-making, Multi-objective route choice, Reinforcement learning, Repeated games, Multiagent systems, Multi-armed Bandit algorithms;Federal University of Rio Grande do Sul;en_US;Published;12;2022;2022-03-18 12:46:32
122929;Anderson  R. Tavares;Multi-Objective, Multi-Armed Bandits: Algorithms for Repeated Games and Application to Route Choice;Multi-objective decision-making in multi-agent scenarios poses multiple challenges. Dealing with multiple objectives and non-stationarity caused by simultaneous learning are only two of them, which have been addressed separately. In this work, reinforcement learning algorithms that tackle both issues together are proposed and applied to a route choice problem, where drivers must select an action in a single-state formulation, while aiming to minimize both their travel time and toll. Hence, we deal with repeated games, now with a multi-objective approach. Advantages, limitations and differences of these algorithms are discussed. Our results show that the proposed algorithms for action selection using reinforcement learning deal with non-stationarity and multiple objectives, while providing alternative solutions to those of centralized methods.;Multi-objective decision-making, Multi-objective route choice, Reinforcement learning, Repeated games, Multiagent systems, Multi-armed Bandit algorithms;Federal University of Rio Grande do Sul;en_US;Published;12;2022;2022-03-18 12:46:32
124564;Rodrigo Henrique Cunha Palácios;Neural Classification of Rotor Faults in Three-Phase Induction Motors using Electric Current Signals in the Frequency Domain;Three-phase induction motors are widely used in different applications in the industry due to their robustness, low cost, and reliability. Untimely identification and correct diagnosis of incipient faults reduce cost and improve the maintenance management of these machines. This paper explores a new method for robust classification of rotor failures in three-phase induction motors (MITs) connected directly to the electrical network, operating in a steady-state, under unbalanced voltages and load conditions. Through an innovative methodology, an analysis of the electrical current signals from 1 hp and 2 hp motors in the frequency domain was performed. Such analysis was applied in constructing input matrices for a Multilayer Perceptron Neural Network (MLPNN) to detect faults. Furthermore, this methodology proved to be robust because the samples of the failing and healthy motors include voltage unbalance conditions in the electrical supply and a significant variation in the load applied to the motor shaft. Such load variation was used for the detection of failures of 1, 2, and 4 broken bars consecutively on the rotor and in the condition of 2 broken bars and 2 other broken bars diametrically opposite. The results were promising and were obtained using 847 real samples from an experimental bench used to construct the neural model and its respective validation.;Motor Faults, FFT, Multilayer Perceptron, Artificial Neural Network;Universidade Tecnológica Federal do Paraná;en_US;Published;7;2022;2022-05-16 20:23:46
124564;Ivan Nunes da Silva;Neural Classification of Rotor Faults in Three-Phase Induction Motors using Electric Current Signals in the Frequency Domain;Three-phase induction motors are widely used in different applications in the industry due to their robustness, low cost, and reliability. Untimely identification and correct diagnosis of incipient faults reduce cost and improve the maintenance management of these machines. This paper explores a new method for robust classification of rotor failures in three-phase induction motors (MITs) connected directly to the electrical network, operating in a steady-state, under unbalanced voltages and load conditions. Through an innovative methodology, an analysis of the electrical current signals from 1 hp and 2 hp motors in the frequency domain was performed. Such analysis was applied in constructing input matrices for a Multilayer Perceptron Neural Network (MLPNN) to detect faults. Furthermore, this methodology proved to be robust because the samples of the failing and healthy motors include voltage unbalance conditions in the electrical supply and a significant variation in the load applied to the motor shaft. Such load variation was used for the detection of failures of 1, 2, and 4 broken bars consecutively on the rotor and in the condition of 2 broken bars and 2 other broken bars diametrically opposite. The results were promising and were obtained using 847 real samples from an experimental bench used to construct the neural model and its respective validation.;Motor Faults, FFT, Multilayer Perceptron, Artificial Neural Network;Universidade de São Paulo;en_US;Published;7;2022;2022-05-16 20:23:46
124564;Wagner  Fontes Godoy;Neural Classification of Rotor Faults in Three-Phase Induction Motors using Electric Current Signals in the Frequency Domain;Three-phase induction motors are widely used in different applications in the industry due to their robustness, low cost, and reliability. Untimely identification and correct diagnosis of incipient faults reduce cost and improve the maintenance management of these machines. This paper explores a new method for robust classification of rotor failures in three-phase induction motors (MITs) connected directly to the electrical network, operating in a steady-state, under unbalanced voltages and load conditions. Through an innovative methodology, an analysis of the electrical current signals from 1 hp and 2 hp motors in the frequency domain was performed. Such analysis was applied in constructing input matrices for a Multilayer Perceptron Neural Network (MLPNN) to detect faults. Furthermore, this methodology proved to be robust because the samples of the failing and healthy motors include voltage unbalance conditions in the electrical supply and a significant variation in the load applied to the motor shaft. Such load variation was used for the detection of failures of 1, 2, and 4 broken bars consecutively on the rotor and in the condition of 2 broken bars and 2 other broken bars diametrically opposite. The results were promising and were obtained using 847 real samples from an experimental bench used to construct the neural model and its respective validation.;Motor Faults, FFT, Multilayer Perceptron, Artificial Neural Network;Universidade Tecnológica Federal do Paraná;en_US;Published;7;2022;2022-05-16 20:23:46
125288;Alfredo Silveira Araújo Neto;Optimization Inspired by the Organization of Interiors Applied to Cluster Analysis;Cluster analysis consists of a procedure capable of establishing, based on a similarity measure, the classification of a collection of objects into disjoint subsets of elements, so that the items included in a group or subset are more similar to each other than objects added to a distinct group. For represent an operation capable of being interpreted as a combinatorial optimization problem, which has as a criterion function to minimize the squared error associated with the established subdivision of items, it observe that the application of approximation algorithms to its resolution has been frequently performed. In this circumstance, this study proposes the  use of a metaheuristic inspired by the composition and decoration of internal environments to cluster analysis, and compares the results obtained by this method with the responses achieved by five other classification techniques. Particulary, a non-parametric statistical evaluation, which compared the subdivisions determined by the method suggested with the subsets established by the K-means clustering algorithm and by meta-heuristics inspired by biological behaviors and physical phenomena, indicated that the proposed strategy it obtned groupings equivalent or more congruent than those determined by the other partitioning methods.;Evolutive Computing, Metaheuristics, Combinatorial Optimization, Pattern Recognition;Techway Informática Ltda.;en_US;Published;15;2022;2022-06-16 22:23:14
125396;Carlo Kleber da Silva		 Rodrigues;Comparative Analysis of Blockchain-Based Platforms for Managing Electronic Health Records in the Public Healthcare System of Brazil;This article comparatively analyzes two platforms based on Blockchain, aiming at the management of electronic health records in the public healthcare system of Brazil. The difference between the platforms primarily lies in the deployed consensus algorithm. Efficiency, availability, integrity, and confidentiality requirements are evaluated through analytical models and theoretical discussions. Among the obtained results, we highlight the following: (i) the platform with a voting-based consensus algorithm yields a more efficient system, but is more prone to service unavailability, than that of the platform deploying an intensive-compute consensus algorithm (ii) integrity and confidentiality requirements may be satisfactorily met regardless of the consensus type. As the main contribution, this article provides valuable experimental results and theoretical subsidies, which together complement previous research and help to lay the groundwork for the fruitful development of real projects. Finally, conclusions and future work conclude this article.;Blockchain, Platform, Healthcare, Efficiency, Security;Universidade Federal da Paraíba;en_US;Published;9;2022;2022-06-22 8:31:05
125567;Tiago  Costa;Performance Assessment of a Wireless Mesh Network for Post-harvest Food Quality Traceability of Fruit Products: A Case Study;This paper presents the performance evaluation of a wireless sensor mesh network, for monitoring temperature and humidity in horticultural products when transported in truck galleys. For this purpose a software solution was proposed using ESP8266 devices powered by batteries. The mesh network was managed by the painlessMesh library. The proposed solution aims to minimize the energy consumption of the sensor nodes. The validation of the solution was performed in an area simulating a galley of a truck, where five sensor nodes and a root node were distributed. The tests were developed considering four different models involving variations in messages delivery confirmation, number of attempts until successful delivery and duty cycle duration of the nodes. The performance evaluation of the solution aimed to determine, connectivity rate, sending rate after connection and delivery rates of the first and second attempts. The results obtained show that the message delivery confirmation does not bring added value to the solution, contributing only to increase energy consumption. The use of synchronous duty cycles also showed worse results than the asynchronous use. These results allow the creation of a knowledge base for the use of this solution in a real context.;Wireless mesh networks, ESP8266, PainlessMesh, Performance assessment; Instituto Politécnico de Castelo Branco;en_US;Published;11;2022;2022-06-30 9:17:31
125567;Luís  Santos;Performance Assessment of a Wireless Mesh Network for Post-harvest Food Quality Traceability of Fruit Products: A Case Study;This paper presents the performance evaluation of a wireless sensor mesh network, for monitoring temperature and humidity in horticultural products when transported in truck galleys. For this purpose a software solution was proposed using ESP8266 devices powered by batteries. The mesh network was managed by the painlessMesh library. The proposed solution aims to minimize the energy consumption of the sensor nodes. The validation of the solution was performed in an area simulating a galley of a truck, where five sensor nodes and a root node were distributed. The tests were developed considering four different models involving variations in messages delivery confirmation, number of attempts until successful delivery and duty cycle duration of the nodes. The performance evaluation of the solution aimed to determine, connectivity rate, sending rate after connection and delivery rates of the first and second attempts. The results obtained show that the message delivery confirmation does not bring added value to the solution, contributing only to increase energy consumption. The use of synchronous duty cycles also showed worse results than the asynchronous use. These results allow the creation of a knowledge base for the use of this solution in a real context.;Wireless mesh networks, ESP8266, PainlessMesh, Performance assessment;Instituto Politécnico de Castelo Branco;en_US;Published;11;2022;2022-06-30 9:17:31
125567;João M. L. P. Caldeira;Performance Assessment of a Wireless Mesh Network for Post-harvest Food Quality Traceability of Fruit Products: A Case Study;This paper presents the performance evaluation of a wireless sensor mesh network, for monitoring temperature and humidity in horticultural products when transported in truck galleys. For this purpose a software solution was proposed using ESP8266 devices powered by batteries. The mesh network was managed by the painlessMesh library. The proposed solution aims to minimize the energy consumption of the sensor nodes. The validation of the solution was performed in an area simulating a galley of a truck, where five sensor nodes and a root node were distributed. The tests were developed considering four different models involving variations in messages delivery confirmation, number of attempts until successful delivery and duty cycle duration of the nodes. The performance evaluation of the solution aimed to determine, connectivity rate, sending rate after connection and delivery rates of the first and second attempts. The results obtained show that the message delivery confirmation does not bring added value to the solution, contributing only to increase energy consumption. The use of synchronous duty cycles also showed worse results than the asynchronous use. These results allow the creation of a knowledge base for the use of this solution in a real context.;Wireless mesh networks, ESP8266, PainlessMesh, Performance assessment;Instituto Politécnico de Castelo Branco;en_US;Published;11;2022;2022-06-30 9:17:31
125567;Vasco N. G. J. Soares;Performance Assessment of a Wireless Mesh Network for Post-harvest Food Quality Traceability of Fruit Products: A Case Study;This paper presents the performance evaluation of a wireless sensor mesh network, for monitoring temperature and humidity in horticultural products when transported in truck galleys. For this purpose a software solution was proposed using ESP8266 devices powered by batteries. The mesh network was managed by the painlessMesh library. The proposed solution aims to minimize the energy consumption of the sensor nodes. The validation of the solution was performed in an area simulating a galley of a truck, where five sensor nodes and a root node were distributed. The tests were developed considering four different models involving variations in messages delivery confirmation, number of attempts until successful delivery and duty cycle duration of the nodes. The performance evaluation of the solution aimed to determine, connectivity rate, sending rate after connection and delivery rates of the first and second attempts. The results obtained show that the message delivery confirmation does not bring added value to the solution, contributing only to increase energy consumption. The use of synchronous duty cycles also showed worse results than the asynchronous use. These results allow the creation of a knowledge base for the use of this solution in a real context.;Wireless mesh networks, ESP8266, PainlessMesh, Performance assessment;Instituto Politécnico de Castelo Branco;en_US;Published;11;2022;2022-06-30 9:17:31
125567; Pedro D.  Gaspar;Performance Assessment of a Wireless Mesh Network for Post-harvest Food Quality Traceability of Fruit Products: A Case Study;This paper presents the performance evaluation of a wireless sensor mesh network, for monitoring temperature and humidity in horticultural products when transported in truck galleys. For this purpose a software solution was proposed using ESP8266 devices powered by batteries. The mesh network was managed by the painlessMesh library. The proposed solution aims to minimize the energy consumption of the sensor nodes. The validation of the solution was performed in an area simulating a galley of a truck, where five sensor nodes and a root node were distributed. The tests were developed considering four different models involving variations in messages delivery confirmation, number of attempts until successful delivery and duty cycle duration of the nodes. The performance evaluation of the solution aimed to determine, connectivity rate, sending rate after connection and delivery rates of the first and second attempts. The results obtained show that the message delivery confirmation does not bring added value to the solution, contributing only to increase energy consumption. The use of synchronous duty cycles also showed worse results than the asynchronous use. These results allow the creation of a knowledge base for the use of this solution in a real context.;Wireless mesh networks, ESP8266, PainlessMesh, Performance assessment;Universidade da Beira Interior;en_US;Published;11;2022;2022-06-30 9:17:31
125825;Wagner da Silva Maciel  Sodré;Chatbot Optimization using Sentiment Analysis and Timeline Navigation;A chatbot or conversational agent is a software that can interact or ``chat'' with a human user using a natural language, like English, for instance. Since the first chatbot developed, many have been created but most of their problems still persist, like providing the right answer to the user and user acceptance itself. Considering such facts, in this work, we present a chatbot-building framework that considers the use of sentiment analysis and tree timelines to provide a better chatbot answer. For instance, as presented in our experiments, the user can be addressed to a human attendant when its sentiment is very negative, or even try another branch of the tree timeline, as an alternative answer, whenever the user sentiment is less negative.;Chatbot, Framework, Sentiment Analysis, Timeline Tree;Instituto Militar de Engenharia;en_US;Published;11;2022;2022-07-11 19:24:39
125825;Julio Cesar  Duarte;Chatbot Optimization using Sentiment Analysis and Timeline Navigation;A chatbot or conversational agent is a software that can interact or ``chat'' with a human user using a natural language, like English, for instance. Since the first chatbot developed, many have been created but most of their problems still persist, like providing the right answer to the user and user acceptance itself. Considering such facts, in this work, we present a chatbot-building framework that considers the use of sentiment analysis and tree timelines to provide a better chatbot answer. For instance, as presented in our experiments, the user can be addressed to a human attendant when its sentiment is very negative, or even try another branch of the tree timeline, as an alternative answer, whenever the user sentiment is less negative.;Chatbot, Framework, Sentiment Analysis, Timeline Tree;Instituto Militar de Engenharia;en_US;Published;11;2022;2022-07-11 19:24:39
126016;Lucas de Souza;COVID-19 Detection Using Forced Cough Sounds and Medical Information;The World Health Organization (WHO) has declared the novel coronavirus (COVID-19) outbreak a global pandemic in March 2020. Through a lot of cooperation and the effort of scientists, several vaccines have been created. However, there is no guarantee that the virus will shortly disappear, even if a large part of the population is vaccinated. Therefore, non-invasive methods, with low cost and real-time results, are important to detect infected individuals and enable earlier adequate treatment, in addition to preventing the spread of the virus. An alternative is using forced cough sounds and medical information to distinguish a healthy person from those infected with COVID-19 via artificial intelligence. An additional challenge is the unbalancing of these data, as there are more samples of healthy individuals than contaminated ones. We propose here a Deep Neural Network model to classify people as healthy or sick concerning COVID-19. We used here a model composed by an Convolutional Neural Network and two other Neural Networks with two full-connected layers, each one trained with different data from the same individual. To evaluate the performance of the proposed method, we combined two datasets from the literature: COUGHVID and Coswara. That dataset contains clinical information regarding previous respiratory conditions, symptoms (fever or muscle pain), and a cough record. The results show that our model is simpler (with fewer parameters) than those from the literature and generalizes better the prediction of infected individuals. The proposal presents an average Area Under the ROC Curve (AUC) equal to 0.885 with a confidence interval (0.881 - 0.888), while the literature reports 0.771 with (0.752 - 0.783).;COVID-19 detection, Cough sounds, Deep Neural Networks;Universidade Federal de Juiz de Fora;en_US;Published;8;2022;2022-07-20 22:19:37
126016;Heder Bernardino;COVID-19 Detection Using Forced Cough Sounds and Medical Information;The World Health Organization (WHO) has declared the novel coronavirus (COVID-19) outbreak a global pandemic in March 2020. Through a lot of cooperation and the effort of scientists, several vaccines have been created. However, there is no guarantee that the virus will shortly disappear, even if a large part of the population is vaccinated. Therefore, non-invasive methods, with low cost and real-time results, are important to detect infected individuals and enable earlier adequate treatment, in addition to preventing the spread of the virus. An alternative is using forced cough sounds and medical information to distinguish a healthy person from those infected with COVID-19 via artificial intelligence. An additional challenge is the unbalancing of these data, as there are more samples of healthy individuals than contaminated ones. We propose here a Deep Neural Network model to classify people as healthy or sick concerning COVID-19. We used here a model composed by an Convolutional Neural Network and two other Neural Networks with two full-connected layers, each one trained with different data from the same individual. To evaluate the performance of the proposed method, we combined two datasets from the literature: COUGHVID and Coswara. That dataset contains clinical information regarding previous respiratory conditions, symptoms (fever or muscle pain), and a cough record. The results show that our model is simpler (with fewer parameters) than those from the literature and generalizes better the prediction of infected individuals. The proposal presents an average Area Under the ROC Curve (AUC) equal to 0.885 with a confidence interval (0.881 - 0.888), while the literature reports 0.771 with (0.752 - 0.783).;COVID-19 detection, Cough sounds, Deep Neural Networks;Universidade Federal de Juiz de Fora;en_US;Published;8;2022;2022-07-20 22:19:37
126016;Jairo de Souza;COVID-19 Detection Using Forced Cough Sounds and Medical Information;The World Health Organization (WHO) has declared the novel coronavirus (COVID-19) outbreak a global pandemic in March 2020. Through a lot of cooperation and the effort of scientists, several vaccines have been created. However, there is no guarantee that the virus will shortly disappear, even if a large part of the population is vaccinated. Therefore, non-invasive methods, with low cost and real-time results, are important to detect infected individuals and enable earlier adequate treatment, in addition to preventing the spread of the virus. An alternative is using forced cough sounds and medical information to distinguish a healthy person from those infected with COVID-19 via artificial intelligence. An additional challenge is the unbalancing of these data, as there are more samples of healthy individuals than contaminated ones. We propose here a Deep Neural Network model to classify people as healthy or sick concerning COVID-19. We used here a model composed by an Convolutional Neural Network and two other Neural Networks with two full-connected layers, each one trained with different data from the same individual. To evaluate the performance of the proposed method, we combined two datasets from the literature: COUGHVID and Coswara. That dataset contains clinical information regarding previous respiratory conditions, symptoms (fever or muscle pain), and a cough record. The results show that our model is simpler (with fewer parameters) than those from the literature and generalizes better the prediction of infected individuals. The proposal presents an average Area Under the ROC Curve (AUC) equal to 0.885 with a confidence interval (0.881 - 0.888), while the literature reports 0.771 with (0.752 - 0.783).;COVID-19 detection, Cough sounds, Deep Neural Networks;Universidade Federal de Juiz de Fora;en_US;Published;8;2022;2022-07-20 22:19:37
126016;Alex Vieira;COVID-19 Detection Using Forced Cough Sounds and Medical Information;The World Health Organization (WHO) has declared the novel coronavirus (COVID-19) outbreak a global pandemic in March 2020. Through a lot of cooperation and the effort of scientists, several vaccines have been created. However, there is no guarantee that the virus will shortly disappear, even if a large part of the population is vaccinated. Therefore, non-invasive methods, with low cost and real-time results, are important to detect infected individuals and enable earlier adequate treatment, in addition to preventing the spread of the virus. An alternative is using forced cough sounds and medical information to distinguish a healthy person from those infected with COVID-19 via artificial intelligence. An additional challenge is the unbalancing of these data, as there are more samples of healthy individuals than contaminated ones. We propose here a Deep Neural Network model to classify people as healthy or sick concerning COVID-19. We used here a model composed by an Convolutional Neural Network and two other Neural Networks with two full-connected layers, each one trained with different data from the same individual. To evaluate the performance of the proposed method, we combined two datasets from the literature: COUGHVID and Coswara. That dataset contains clinical information regarding previous respiratory conditions, symptoms (fever or muscle pain), and a cough record. The results show that our model is simpler (with fewer parameters) than those from the literature and generalizes better the prediction of infected individuals. The proposal presents an average Area Under the ROC Curve (AUC) equal to 0.885 with a confidence interval (0.881 - 0.888), while the literature reports 0.771 with (0.752 - 0.783).;COVID-19 detection, Cough sounds, Deep Neural Networks;Universidade Federal de Juiz de Fora;en_US;Published;8;2022;2022-07-20 22:19:37
126309;Arthur Alexandre Artoni;Autism Spectrum Disorder Diagnosis Assistance using Machine Learning;Autism Spectrum Disorder (ASD) is a common but complex disorder to diagnose since there are no imaging or blood tests that can detect ASD. Several techniques can be used, such as diagnostic scales that contain specific questionnaires formulated by specialists that serve as a guide in the diagnostic process. In this paper, Machine Learning (ML) was applied on three public databases containing AQ-10 test results for adults, adolescents, and children as well as other characteristics that could influence the diagnosis of ASD. Experiments were carried out on the databases to list which attributes would be truly relevant for the diagnosis of ASD using ML, which could be of great value for medical students or residents, and for physicians who are not specialists in ASD. The experiments have shown that it is possible to reduce the number of attributes to only 5 while maintaining an Accuracy above 0.9. In the other Database to maintain the same level of Accuracy, the fewer attribute numbers were 7. The Support Vector Machine stood out from the others algorithms used in this paper, obtaining superior results in all scenarios.;Austim, Machine Learning, Diagnosi;Universidade Estadual de Londrina;en_US;Published;17;2022;2022-07-31 15:02:02
126309;Cinthyan Barbosa;Autism Spectrum Disorder Diagnosis Assistance using Machine Learning;Autism Spectrum Disorder (ASD) is a common but complex disorder to diagnose since there are no imaging or blood tests that can detect ASD. Several techniques can be used, such as diagnostic scales that contain specific questionnaires formulated by specialists that serve as a guide in the diagnostic process. In this paper, Machine Learning (ML) was applied on three public databases containing AQ-10 test results for adults, adolescents, and children as well as other characteristics that could influence the diagnosis of ASD. Experiments were carried out on the databases to list which attributes would be truly relevant for the diagnosis of ASD using ML, which could be of great value for medical students or residents, and for physicians who are not specialists in ASD. The experiments have shown that it is possible to reduce the number of attributes to only 5 while maintaining an Accuracy above 0.9. In the other Database to maintain the same level of Accuracy, the fewer attribute numbers were 7. The Support Vector Machine stood out from the others algorithms used in this paper, obtaining superior results in all scenarios.;Austim, Machine Learning, Diagnosi;Universidade Estadual de Londrina;en_US;Published;17;2022;2022-07-31 15:02:02
126309; Marcelo Morandini;Autism Spectrum Disorder Diagnosis Assistance using Machine Learning;Autism Spectrum Disorder (ASD) is a common but complex disorder to diagnose since there are no imaging or blood tests that can detect ASD. Several techniques can be used, such as diagnostic scales that contain specific questionnaires formulated by specialists that serve as a guide in the diagnostic process. In this paper, Machine Learning (ML) was applied on three public databases containing AQ-10 test results for adults, adolescents, and children as well as other characteristics that could influence the diagnosis of ASD. Experiments were carried out on the databases to list which attributes would be truly relevant for the diagnosis of ASD using ML, which could be of great value for medical students or residents, and for physicians who are not specialists in ASD. The experiments have shown that it is possible to reduce the number of attributes to only 5 while maintaining an Accuracy above 0.9. In the other Database to maintain the same level of Accuracy, the fewer attribute numbers were 7. The Support Vector Machine stood out from the others algorithms used in this paper, obtaining superior results in all scenarios.;Austim, Machine Learning, Diagnosi;Universidade de São Paulo;en_US;Published;17;2022;2022-07-31 15:02:02
126544;Lilandra Maria de Oliveira;Provision and Collection of Safety Evidence: A Systematic Literature Review;Safety-Critical Systems (SCS) are becoming more and more present in modern societies’ daily lives, increasing people’s dependence on them. Current SCS are firmly based on computational technology possible failures in the operation of these systems can lead to accidents and endanger human life, as well as damage the environment and property. SCS are present in many areas such as avionics, automotive systems, industrial plants (chemical, oil & gas, and nuclear), medical devices, railroad control, defense, and aerospace systems. Companies that develop SCS must present evidence of their safety to obtain certification and authorization. This paper presents a Systematic Literature Review (SLR) to investigate processes, tools, and techniques for collecting and managing safety evidence in SCS. The authors conducted this SLR according to the guidelines proposed by Kitchenham and Charters. The SLR comprises seven (7) research questions that investigate essential aspects of collecting and managing safety evidence. The primary studies analyzed in this SLR were selected based on a search string applied into four data sources: ACM, IEEE Xplore, SpringerLink, and ScienceDirect. Data extraction considered (fifty-one) 51 primary studies. The authors identified eleven (11) different approaches covering processes, tools, and techniques for collecting and managing safety evidence. Despite other SLR works conducted about safety evidence, none of them focused on the details related to safety evidence collection. We found that very few approaches focused specifically on the process of collecting safety evidence.;safety evidence collection, safety evidence model,  safety-critical systems certification, systematic literature review;Federal University of Sao Paulo;en_US;Published;13;2022;2022-08-14 15:57:54
126544;Luiz Eduardo Galvão Martins;Provision and Collection of Safety Evidence: A Systematic Literature Review;Safety-Critical Systems (SCS) are becoming more and more present in modern societies’ daily lives, increasing people’s dependence on them. Current SCS are firmly based on computational technology possible failures in the operation of these systems can lead to accidents and endanger human life, as well as damage the environment and property. SCS are present in many areas such as avionics, automotive systems, industrial plants (chemical, oil & gas, and nuclear), medical devices, railroad control, defense, and aerospace systems. Companies that develop SCS must present evidence of their safety to obtain certification and authorization. This paper presents a Systematic Literature Review (SLR) to investigate processes, tools, and techniques for collecting and managing safety evidence in SCS. The authors conducted this SLR according to the guidelines proposed by Kitchenham and Charters. The SLR comprises seven (7) research questions that investigate essential aspects of collecting and managing safety evidence. The primary studies analyzed in this SLR were selected based on a search string applied into four data sources: ACM, IEEE Xplore, SpringerLink, and ScienceDirect. Data extraction considered (fifty-one) 51 primary studies. The authors identified eleven (11) different approaches covering processes, tools, and techniques for collecting and managing safety evidence. Despite other SLR works conducted about safety evidence, none of them focused on the details related to safety evidence collection. We found that very few approaches focused specifically on the process of collecting safety evidence.;safety evidence collection, safety evidence model,  safety-critical systems certification, systematic literature review;Federal University of Sao Paulo;en_US;Published;13;2022;2022-08-14 15:57:54
126544;Johnny Cardoso Marques;Provision and Collection of Safety Evidence: A Systematic Literature Review;Safety-Critical Systems (SCS) are becoming more and more present in modern societies’ daily lives, increasing people’s dependence on them. Current SCS are firmly based on computational technology possible failures in the operation of these systems can lead to accidents and endanger human life, as well as damage the environment and property. SCS are present in many areas such as avionics, automotive systems, industrial plants (chemical, oil & gas, and nuclear), medical devices, railroad control, defense, and aerospace systems. Companies that develop SCS must present evidence of their safety to obtain certification and authorization. This paper presents a Systematic Literature Review (SLR) to investigate processes, tools, and techniques for collecting and managing safety evidence in SCS. The authors conducted this SLR according to the guidelines proposed by Kitchenham and Charters. The SLR comprises seven (7) research questions that investigate essential aspects of collecting and managing safety evidence. The primary studies analyzed in this SLR were selected based on a search string applied into four data sources: ACM, IEEE Xplore, SpringerLink, and ScienceDirect. Data extraction considered (fifty-one) 51 primary studies. The authors identified eleven (11) different approaches covering processes, tools, and techniques for collecting and managing safety evidence. Despite other SLR works conducted about safety evidence, none of them focused on the details related to safety evidence collection. We found that very few approaches focused specifically on the process of collecting safety evidence.;safety evidence collection, safety evidence model,  safety-critical systems certification, systematic literature review;Technological Institute of Aeronautics;en_US;Published;13;2022;2022-08-14 15:57:54
127207;Vitor M. T.  Aleluia;Livestock Monitoring Prototype Implementation and Validation;This paper presents the proposal, implementation and validation of a low cost fault-tolerant functional prototype for livestock monitoring. This prototype uses IoT devices, ESP8266 and ESP32, creating a mesh network, managed by the painlessMesh library, with WiFi and LoRa technologies. It allows, for instance, the collection of vital signs from animals. In comparison with the traditional method of livestock examination, this cost-efficient approach reduces manual labor and saves working time. It also improves animal health, increases profits and decreases the environmental footprint.;Livestock Monitoring, Prototype, Wireless Mesh Networks, PainlessMesh; Instituto Politécnico de Castelo Branco;en_US;Published;12;2022;2022-09-16 7:56:54
127207;Vasco N. G.  J. Soares;Livestock Monitoring Prototype Implementation and Validation;This paper presents the proposal, implementation and validation of a low cost fault-tolerant functional prototype for livestock monitoring. This prototype uses IoT devices, ESP8266 and ESP32, creating a mesh network, managed by the painlessMesh library, with WiFi and LoRa technologies. It allows, for instance, the collection of vital signs from animals. In comparison with the traditional method of livestock examination, this cost-efficient approach reduces manual labor and saves working time. It also improves animal health, increases profits and decreases the environmental footprint.;Livestock Monitoring, Prototype, Wireless Mesh Networks, PainlessMesh;Instituto Politécnico de Castelo Branco;en_US;Published;12;2022;2022-09-16 7:56:54
127207;João M. L. P.  Caldeira;Livestock Monitoring Prototype Implementation and Validation;This paper presents the proposal, implementation and validation of a low cost fault-tolerant functional prototype for livestock monitoring. This prototype uses IoT devices, ESP8266 and ESP32, creating a mesh network, managed by the painlessMesh library, with WiFi and LoRa technologies. It allows, for instance, the collection of vital signs from animals. In comparison with the traditional method of livestock examination, this cost-efficient approach reduces manual labor and saves working time. It also improves animal health, increases profits and decreases the environmental footprint.;Livestock Monitoring, Prototype, Wireless Mesh Networks, PainlessMesh;Instituto Politécnico de Castelo Branco;en_US;Published;12;2022;2022-09-16 7:56:54
127207;Pedro D.  Gaspar;Livestock Monitoring Prototype Implementation and Validation;This paper presents the proposal, implementation and validation of a low cost fault-tolerant functional prototype for livestock monitoring. This prototype uses IoT devices, ESP8266 and ESP32, creating a mesh network, managed by the painlessMesh library, with WiFi and LoRa technologies. It allows, for instance, the collection of vital signs from animals. In comparison with the traditional method of livestock examination, this cost-efficient approach reduces manual labor and saves working time. It also improves animal health, increases profits and decreases the environmental footprint.;Livestock Monitoring, Prototype, Wireless Mesh Networks, PainlessMesh;Universidade da Beira Interior ;en_US;Published;12;2022;2022-09-16 7:56:54
128836;Lucas José Gonçalves Freitas;Catboost Algorithm Application in Legal Texts and UN 2030 Agenda;This article evaluates the application of the Catboost algorithm for automatic classification of legal texts in The United Nations (UN) 2030 Agenda for Sustainable Development Goals (SDGs). The task consists of labeling texts from initial petitions and rulings based on identifying topics related to the objectives of the 2030 Agenda, which include sustainable development, quality education, gender equality, preservation of the environment, among other topics of interest to UN member countries. This work aims to help Judicial System employees in case management task, an activity that is manual and repetitive. Since the Catboost algorithm allows joining textual, numerical and categorical features in the same classification model. The proposed approach adds to the classification algorithm traditional metadata about legal processes, such as the Supreme Court Class and Field of Law. The main contributions of this work are: analysis of metadata in machine learning flows and evaluation of the Catboost algorithm for textual classification in legal contexts.;Natural Language Processing, Legal Text Classification, Machine Learning, UN 2030 Agenda;Universidade de Brasília;en_US;Published;7;2022;2022-12-12 17:41:58
128836;Pamella Sada Dias Edokawa;Catboost Algorithm Application in Legal Texts and UN 2030 Agenda;This article evaluates the application of the Catboost algorithm for automatic classification of legal texts in The United Nations (UN) 2030 Agenda for Sustainable Development Goals (SDGs). The task consists of labeling texts from initial petitions and rulings based on identifying topics related to the objectives of the 2030 Agenda, which include sustainable development, quality education, gender equality, preservation of the environment, among other topics of interest to UN member countries. This work aims to help Judicial System employees in case management task, an activity that is manual and repetitive. Since the Catboost algorithm allows joining textual, numerical and categorical features in the same classification model. The proposed approach adds to the classification algorithm traditional metadata about legal processes, such as the Supreme Court Class and Field of Law. The main contributions of this work are: analysis of metadata in machine learning flows and evaluation of the Catboost algorithm for textual classification in legal contexts.;Natural Language Processing, Legal Text Classification, Machine Learning, UN 2030 Agenda;Secretaria de Gestao Estratégica, Supremo Tribunal Federal;en_US;Published;7;2022;2022-12-12 17:41:58
128836;Thaís Carvalho Valadares Rodrigues;Catboost Algorithm Application in Legal Texts and UN 2030 Agenda;This article evaluates the application of the Catboost algorithm for automatic classification of legal texts in The United Nations (UN) 2030 Agenda for Sustainable Development Goals (SDGs). The task consists of labeling texts from initial petitions and rulings based on identifying topics related to the objectives of the 2030 Agenda, which include sustainable development, quality education, gender equality, preservation of the environment, among other topics of interest to UN member countries. This work aims to help Judicial System employees in case management task, an activity that is manual and repetitive. Since the Catboost algorithm allows joining textual, numerical and categorical features in the same classification model. The proposed approach adds to the classification algorithm traditional metadata about legal processes, such as the Supreme Court Class and Field of Law. The main contributions of this work are: analysis of metadata in machine learning flows and evaluation of the Catboost algorithm for textual classification in legal contexts.;Natural Language Processing, Legal Text Classification, Machine Learning, UN 2030 Agenda; Universidade de Bras´ılia;en_US;Published;7;2022;2022-12-12 17:41:58
128836;Ariane Hayana Thomé de Farias;Catboost Algorithm Application in Legal Texts and UN 2030 Agenda;This article evaluates the application of the Catboost algorithm for automatic classification of legal texts in The United Nations (UN) 2030 Agenda for Sustainable Development Goals (SDGs). The task consists of labeling texts from initial petitions and rulings based on identifying topics related to the objectives of the 2030 Agenda, which include sustainable development, quality education, gender equality, preservation of the environment, among other topics of interest to UN member countries. This work aims to help Judicial System employees in case management task, an activity that is manual and repetitive. Since the Catboost algorithm allows joining textual, numerical and categorical features in the same classification model. The proposed approach adds to the classification algorithm traditional metadata about legal processes, such as the Supreme Court Class and Field of Law. The main contributions of this work are: analysis of metadata in machine learning flows and evaluation of the Catboost algorithm for textual classification in legal contexts.;Natural Language Processing, Legal Text Classification, Machine Learning, UN 2030 Agenda;Corregedoria Geral de Justic¸a, Tribunal de Justic¸a de Roraima;en_US;Published;7;2022;2022-12-12 17:41:58
128836;Euler Rodrigues de Alencar;Catboost Algorithm Application in Legal Texts and UN 2030 Agenda;This article evaluates the application of the Catboost algorithm for automatic classification of legal texts in The United Nations (UN) 2030 Agenda for Sustainable Development Goals (SDGs). The task consists of labeling texts from initial petitions and rulings based on identifying topics related to the objectives of the 2030 Agenda, which include sustainable development, quality education, gender equality, preservation of the environment, among other topics of interest to UN member countries. This work aims to help Judicial System employees in case management task, an activity that is manual and repetitive. Since the Catboost algorithm allows joining textual, numerical and categorical features in the same classification model. The proposed approach adds to the classification algorithm traditional metadata about legal processes, such as the Supreme Court Class and Field of Law. The main contributions of this work are: analysis of metadata in machine learning flows and evaluation of the Catboost algorithm for textual classification in legal contexts.;Natural Language Processing, Legal Text Classification, Machine Learning, UN 2030 Agenda;Secretaria de Gestao Estratégica, Supremo Tribunal Federal ;en_US;Published;7;2022;2022-12-12 17:41:58
129783;João Fernandes;The Use of Technology in the Diagnosis and Treatment of Epilepsy: Trends and Open Issues;Epilepsy is one of the most common neurological diseases in the world, affecting millions of people. The impact of this disease goes beyond seizures. It has repercussions on the individual’s health and quality of life (i.e., neurological, psychological, and physical consequences) and social inclusion. This article presents a review of the literature and discusses the technological and scientific advances in the diagnosis and treatment of epilepsy. It begins by introducing the related concepts, then analyzes the different technological approaches, exposing their strengths and limitations, and concludes by identifying challenges and open problems for future research.;Epilepsy, sensors, implants, algorithms, survey;Instituto Politécnico de Castelo Branco;en_US;Published;14;2023;2023-01-31 7:02:22
129783;Pedro D. Gaspar;The Use of Technology in the Diagnosis and Treatment of Epilepsy: Trends and Open Issues;Epilepsy is one of the most common neurological diseases in the world, affecting millions of people. The impact of this disease goes beyond seizures. It has repercussions on the individual’s health and quality of life (i.e., neurological, psychological, and physical consequences) and social inclusion. This article presents a review of the literature and discusses the technological and scientific advances in the diagnosis and treatment of epilepsy. It begins by introducing the related concepts, then analyzes the different technological approaches, exposing their strengths and limitations, and concludes by identifying challenges and open problems for future research.;Epilepsy, sensors, implants, algorithms, survey;Universidade da Beira Interior;en_US;Published;14;2023;2023-01-31 7:02:22
129783;Eva Menino;The Use of Technology in the Diagnosis and Treatment of Epilepsy: Trends and Open Issues;Epilepsy is one of the most common neurological diseases in the world, affecting millions of people. The impact of this disease goes beyond seizures. It has repercussions on the individual’s health and quality of life (i.e., neurological, psychological, and physical consequences) and social inclusion. This article presents a review of the literature and discusses the technological and scientific advances in the diagnosis and treatment of epilepsy. It begins by introducing the related concepts, then analyzes the different technological approaches, exposing their strengths and limitations, and concludes by identifying challenges and open problems for future research.;Epilepsy, sensors, implants, algorithms, survey;Escola Superior de Saúde do Instituto Politécnico de Leiria;en_US;Published;14;2023;2023-01-31 7:02:22
129783;João M. L. P.  Caldeira;The Use of Technology in the Diagnosis and Treatment of Epilepsy: Trends and Open Issues;Epilepsy is one of the most common neurological diseases in the world, affecting millions of people. The impact of this disease goes beyond seizures. It has repercussions on the individual’s health and quality of life (i.e., neurological, psychological, and physical consequences) and social inclusion. This article presents a review of the literature and discusses the technological and scientific advances in the diagnosis and treatment of epilepsy. It begins by introducing the related concepts, then analyzes the different technological approaches, exposing their strengths and limitations, and concludes by identifying challenges and open problems for future research.;Epilepsy, sensors, implants, algorithms, survey;Instituto Politécnico de Castelo Branco;en_US;Published;14;2023;2023-01-31 7:02:22
129783;Vasco N. G. J. Soares;The Use of Technology in the Diagnosis and Treatment of Epilepsy: Trends and Open Issues;Epilepsy is one of the most common neurological diseases in the world, affecting millions of people. The impact of this disease goes beyond seizures. It has repercussions on the individual’s health and quality of life (i.e., neurological, psychological, and physical consequences) and social inclusion. This article presents a review of the literature and discusses the technological and scientific advances in the diagnosis and treatment of epilepsy. It begins by introducing the related concepts, then analyzes the different technological approaches, exposing their strengths and limitations, and concludes by identifying challenges and open problems for future research.;Epilepsy, sensors, implants, algorithms, survey;Instituto Politécnico de Castelo Branco;en_US;Published;14;2023;2023-01-31 7:02:22
129787;Miguel Gonçalves;Road Pavement Damage Detection using Computer Vision Techniques: Approaches, Challenges and Opportunities;The work presented in this paper is the result of a preliminary research aimed at using computer vision techniques for road pavement damage detection in the context of a smart city. It first introduces the related concepts. Then, it surveys the state of the art and existing solutions, presenting their main features, strengths and limitations. The most promising solutions are identified. Finally, it discusses open challenges and research directions in this area.;Smart Cities, Road Pavement Damage Detection, Computer Vision, Convolutional Neural Networks, Object Detection, Survey;Instituto Politécnico de Castelo Branco;en_US;Published;13;2023;2023-01-31 8:56:45
129787;Tomás Marques;Road Pavement Damage Detection using Computer Vision Techniques: Approaches, Challenges and Opportunities;The work presented in this paper is the result of a preliminary research aimed at using computer vision techniques for road pavement damage detection in the context of a smart city. It first introduces the related concepts. Then, it surveys the state of the art and existing solutions, presenting their main features, strengths and limitations. The most promising solutions are identified. Finally, it discusses open challenges and research directions in this area.;Smart Cities, Road Pavement Damage Detection, Computer Vision, Convolutional Neural Networks, Object Detection, Survey;Instituto Politécnico de Castelo Branco;en_US;Published;13;2023;2023-01-31 8:56:45
129787;Pedro D.  Gaspar;Road Pavement Damage Detection using Computer Vision Techniques: Approaches, Challenges and Opportunities;The work presented in this paper is the result of a preliminary research aimed at using computer vision techniques for road pavement damage detection in the context of a smart city. It first introduces the related concepts. Then, it surveys the state of the art and existing solutions, presenting their main features, strengths and limitations. The most promising solutions are identified. Finally, it discusses open challenges and research directions in this area.;Smart Cities, Road Pavement Damage Detection, Computer Vision, Convolutional Neural Networks, Object Detection, Survey;Universidade da Beira Interior;en_US;Published;13;2023;2023-01-31 8:56:45
129787;Vasco N. G. J. Soares;Road Pavement Damage Detection using Computer Vision Techniques: Approaches, Challenges and Opportunities;The work presented in this paper is the result of a preliminary research aimed at using computer vision techniques for road pavement damage detection in the context of a smart city. It first introduces the related concepts. Then, it surveys the state of the art and existing solutions, presenting their main features, strengths and limitations. The most promising solutions are identified. Finally, it discusses open challenges and research directions in this area.;Smart Cities, Road Pavement Damage Detection, Computer Vision, Convolutional Neural Networks, Object Detection, Survey;Instituto Politécnico de Castelo Branco;en_US;Published;13;2023;2023-01-31 8:56:45
129787;João M. L. P. Caldeira;Road Pavement Damage Detection using Computer Vision Techniques: Approaches, Challenges and Opportunities;The work presented in this paper is the result of a preliminary research aimed at using computer vision techniques for road pavement damage detection in the context of a smart city. It first introduces the related concepts. Then, it surveys the state of the art and existing solutions, presenting their main features, strengths and limitations. The most promising solutions are identified. Finally, it discusses open challenges and research directions in this area.;Smart Cities, Road Pavement Damage Detection, Computer Vision, Convolutional Neural Networks, Object Detection, Survey;Instituto Politécnico de Castelo Branco;en_US;Published;13;2023;2023-01-31 8:56:45
129990;Matheus Raffael Simon;Evaluation of the Relationship Between the Age of the Patient and the Mandibular Trabecular Bone Structure Through Dental Cone Beam Tomographic Images by Means of a Convolutional Neural Network; Osteoporosis is a systemic condition that affects bone mineral density in individuals, affecting mainly women and is diagnosed by DXA examination. The research used 137 cone beam tomography scans, from which 1389 mandibles samples from female patients and 633 from male patients were selected for CNN training, which is composed of 3 dense layers with 100 neurons each and Relu activation function, update of weights by the Adam algorithm, using MaxPooling in each convolution the dense layer uses 100 neurons, with the activation function. The neural network training accuracy was 98% for males and 94% for females, with area under the ROC curve (AUC) equal to 0.94 and 0.81 for the respective genders. The accuracy obtained in validating the CNN was 98% for males and 89% for females. In the supervised test, using 5 cuts from 10 exams of the test set of each sex approximately 100% accuracy was obtained for both sexes. Thus, it is concluded that the proposed model is capable of classifying the samples in the proposed age groups and proved to be robust and solid in the tests.;Imaging processing, convolutional neural network, trabecular bone structure, mandible, cone beam tomography, osteoporosis;State University of Western Parana;en_US;Published;10;2023;2023-02-05 20:39:34
129990;Adair Santa Catarina;Evaluation of the Relationship Between the Age of the Patient and the Mandibular Trabecular Bone Structure Through Dental Cone Beam Tomographic Images by Means of a Convolutional Neural Network; Osteoporosis is a systemic condition that affects bone mineral density in individuals, affecting mainly women and is diagnosed by DXA examination. The research used 137 cone beam tomography scans, from which 1389 mandibles samples from female patients and 633 from male patients were selected for CNN training, which is composed of 3 dense layers with 100 neurons each and Relu activation function, update of weights by the Adam algorithm, using MaxPooling in each convolution the dense layer uses 100 neurons, with the activation function. The neural network training accuracy was 98% for males and 94% for females, with area under the ROC curve (AUC) equal to 0.94 and 0.81 for the respective genders. The accuracy obtained in validating the CNN was 98% for males and 89% for females. In the supervised test, using 5 cuts from 10 exams of the test set of each sex approximately 100% accuracy was obtained for both sexes. Thus, it is concluded that the proposed model is capable of classifying the samples in the proposed age groups and proved to be robust and solid in the tests.;Imaging processing, convolutional neural network, trabecular bone structure, mandible, cone beam tomography, osteoporosis;State University of Western Parana;en_US;Published;10;2023;2023-02-05 20:39:34
129990;Adriane Yaeko Togashi;Evaluation of the Relationship Between the Age of the Patient and the Mandibular Trabecular Bone Structure Through Dental Cone Beam Tomographic Images by Means of a Convolutional Neural Network; Osteoporosis is a systemic condition that affects bone mineral density in individuals, affecting mainly women and is diagnosed by DXA examination. The research used 137 cone beam tomography scans, from which 1389 mandibles samples from female patients and 633 from male patients were selected for CNN training, which is composed of 3 dense layers with 100 neurons each and Relu activation function, update of weights by the Adam algorithm, using MaxPooling in each convolution the dense layer uses 100 neurons, with the activation function. The neural network training accuracy was 98% for males and 94% for females, with area under the ROC curve (AUC) equal to 0.94 and 0.81 for the respective genders. The accuracy obtained in validating the CNN was 98% for males and 89% for females. In the supervised test, using 5 cuts from 10 exams of the test set of each sex approximately 100% accuracy was obtained for both sexes. Thus, it is concluded that the proposed model is capable of classifying the samples in the proposed age groups and proved to be robust and solid in the tests.;Imaging processing, convolutional neural network, trabecular bone structure, mandible, cone beam tomography, osteoporosis;State University of Western Parana;en_US;Published;10;2023;2023-02-05 20:39:34
130669;Mariana Dourado X. S. Santos;Machine Learning Algorithms for Peripheral Blood Cell Classification - A Hemovision Project Experience;This research explores the use of machine learning algorithms to classify nucleated peripheral blood cells. The ResNet18 convolutional neural network was used to pre-process the images and replace the dense layers and for the output, the Support Vector Machine (SVM) classifier was chosen. Images from different datasets were used for training and testing the model. Thus, the developed model achieved an accuracy and F1-Score of 99.96%. In face of the obtained results, it was found that machine learning algorithms can be satisfactorily integrated into educational and diagnostic support processes.;Support Vector Machine, Convolutional Neural Networks, White Blood Cells,  Machine Learning;Universidade Federal de Goiás;en_US;Published;11;2023;2023-03-06 23:58:15
130669;William Laus Bertemes;Machine Learning Algorithms for Peripheral Blood Cell Classification - A Hemovision Project Experience;This research explores the use of machine learning algorithms to classify nucleated peripheral blood cells. The ResNet18 convolutional neural network was used to pre-process the images and replace the dense layers and for the output, the Support Vector Machine (SVM) classifier was chosen. Images from different datasets were used for training and testing the model. Thus, the developed model achieved an accuracy and F1-Score of 99.96%. In face of the obtained results, it was found that machine learning algorithms can be satisfactorily integrated into educational and diagnostic support processes.;Support Vector Machine, Convolutional Neural Networks, White Blood Cells,  Machine Learning;Universidade Federal de Goias;en_US;Published;11;2023;2023-03-06 23:58:15
130669;Iaan Mesquita de Souza;Machine Learning Algorithms for Peripheral Blood Cell Classification - A Hemovision Project Experience;This research explores the use of machine learning algorithms to classify nucleated peripheral blood cells. The ResNet18 convolutional neural network was used to pre-process the images and replace the dense layers and for the output, the Support Vector Machine (SVM) classifier was chosen. Images from different datasets were used for training and testing the model. Thus, the developed model achieved an accuracy and F1-Score of 99.96%. In face of the obtained results, it was found that machine learning algorithms can be satisfactorily integrated into educational and diagnostic support processes.;Support Vector Machine, Convolutional Neural Networks, White Blood Cells,  Machine Learning;Universidade Federal de Goiás;en_US;Published;11;2023;2023-03-06 23:58:15
130669;Mateus Henrique B. Andrades;Machine Learning Algorithms for Peripheral Blood Cell Classification - A Hemovision Project Experience;This research explores the use of machine learning algorithms to classify nucleated peripheral blood cells. The ResNet18 convolutional neural network was used to pre-process the images and replace the dense layers and for the output, the Support Vector Machine (SVM) classifier was chosen. Images from different datasets were used for training and testing the model. Thus, the developed model achieved an accuracy and F1-Score of 99.96%. In face of the obtained results, it was found that machine learning algorithms can be satisfactorily integrated into educational and diagnostic support processes.;Support Vector Machine, Convolutional Neural Networks, White Blood Cells,  Machine Learning;Universidade Federal de Goiás;en_US;Published;11;2023;2023-03-06 23:58:15
130669;Vinicius Sebba Patto;Machine Learning Algorithms for Peripheral Blood Cell Classification - A Hemovision Project Experience;This research explores the use of machine learning algorithms to classify nucleated peripheral blood cells. The ResNet18 convolutional neural network was used to pre-process the images and replace the dense layers and for the output, the Support Vector Machine (SVM) classifier was chosen. Images from different datasets were used for training and testing the model. Thus, the developed model achieved an accuracy and F1-Score of 99.96%. In face of the obtained results, it was found that machine learning algorithms can be satisfactorily integrated into educational and diagnostic support processes.;Support Vector Machine, Convolutional Neural Networks, White Blood Cells,  Machine Learning;Universidade Federal de Goiás;en_US;Published;11;2023;2023-03-06 23:58:15
